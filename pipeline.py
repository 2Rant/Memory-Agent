import os
import time
import uuid
import json
import numpy as np
from typing import List, Dict, Optional, Any, Union
from dataclasses import dataclass
from dotenv import load_dotenv
from openai import OpenAI
from utils import (MEMREADER_PROMPT, 
                   get_embedding, parse_messages, LME_JUDGE_MODEL_TEMPLATE, 
                   LME_ANSWER_PROMPT, remove_code_blocks, extract_json)
from lme_eval import lme_grader
from datetime import datetime, timezone
import pytz
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from vector_db import VectorDBConfig, VectorDBFactory
# ==========================================
# 0. Setup & Prompts
# ==========================================
load_dotenv()

# âš ï¸ è¯·ç¡®ä¿ç¯å¢ƒå˜é‡ä¸­æœ‰ OPENAI_API_KEY å’Œ MILVUS_URI
# å¦‚æœæ˜¯æœ¬åœ°æµ‹è¯•ï¼Œç¡®ä¿ Docker ä¸­ Milvus å·²å¯åŠ¨

llm_client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"), 
    base_url=os.getenv("OPENAI_BASE_URL")
)


MEMORY_MANAGER_PROMPT = """You are a specialized Memory Manager Agent.
Your role is to maintain the consistency and growth of a memory graph using the provided tools.

[INPUTS]
You will receive:
1. "New Facts": A list of atomic facts extracted from the latest user input.
2. "Existing Memories": A list of retrieved memory items, each with a simplified Integer ID (e.g., "0", "1", "2").
   - These memories include those directly related to the new facts, as well as other related facts connected with these memories.
   - They form a connected graph of information relevant to the new facts.

[MANDATORY OUTPUT FORMAT]
For every new fact you process, you MUST:
1. First generate a detailed thinking process
2. Then call the appropriate tool

[THINKING PROCESS REQUIREMENTS]
Your thinking process MUST include:
- The specific new fact you're analyzing
- Which existing memories are relevant (with their IDs)
- How memories are connected through related facts
- Your comparison and reasoning
- Which operation you've decided to perform and why

[OPERATIONS & GUIDELINES]
Compare New Facts with Existing Memories and perform the following operations using the available tools. 
DO NOT output raw JSON text. You MUST use the provided function tools.

1. **ADD (create_memory)**
   - **Condition**: If a fact contains completely NEW information not present in Existing Memories.
   - **Action**: Call `create_memory` with a concise summary of the facts, not just a simple concatenation.
   - **Important**: Memory content should be a meaningful and concise summary.
  
2. **UPDATE (update_memory)**
   - **Condition**: If a fact adds detail, corrects, or updates a specific Existing Memory.
   - **Constraint**: You MUST use the Integer ID (e.g., "0") provided in the input as the `target_memory_id`.
   - **Logic**: Merge the old content and new fact into a comprehensive statement, not just a simple concatenation.
   - **Example**:
     - Old (ID="0"): "User likes generic pizza."
     - New Fact: "User loves pepperoni pizza."
     - Action: `update_memory(target_memory_id="0", new_content="User loves pepperoni pizza", ...)`

3. **DELETE (delete_memory)**
   - **Condition**: If a fact explicitly contradicts an Existing Memory (and the new fact is trusted), or if the memory is no longer valid.
   - **Constraint**: Use the Integer ID (e.g., "1") as `target_memory_id`.

4. **INFER (infer_memory) [CRITICAL]**
   - **Condition**: Look for higher-level insights. If combining "Memory A" and "Memory B" reveals a hidden connection or causality.
   - **Action**: Call `infer_memory`.
   - **Example**:
     - Memory A (ID="2"): "User moved to Singapore."
     - Memory B (ID="3"): "User bought a Type G power adapter."
     - Inference: "User is preparing electronics for Singapore power standards."
     - Action: `infer_memory(source_memory_ids=["2", "3"], inference_content="...")`

5. **NOOP (no_operation)**
   - **Condition**: If the fact is redundant (already exactly covered by memory), similar to existing facts associated with the retrieved memories, or trivial.

[STRICT ID RULES]
- When calling `update_memory` or `delete_memory`, **ONLY** use the string integer IDs (e.g., "0", "1", "2") found in the [EXISTING MEMORIES] list.
- **NEVER** invent a UUID or use an ID that is not in the provided list.
"""

# --- TOOLS ---
MEMORY_TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "create_memory",
            "description": "Create a NEW independent memory node with a concise summary of the facts.",
            "parameters": {
                "type": "object",
                "properties": {
                    "content": {"type": "string", "description": "The concise summary content of the new memory, not just a list of facts."},
                    "evidence_facts": {"type": "array", "items": {"type": "string"}, "description": "Facts supporting this memory."}
                },
                "required": ["content", "evidence_facts"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "update_memory",
            "description": "Update an existing memory by merging the old content and new fact into a comprehensive, concise statement.",
            "parameters": {
                "type": "object",
                "properties": {
                    "target_memory_id": {"type": "string", "description": "The simplified Integer ID (e.g., '0') of the memory to update, found in the [EXISTING MEMORIES] list."},
                    "new_content": {"type": "string", "description": "The merged/updated comprehensive statement."},
                    "evidence_facts": {"type": "array", "items": {"type": "string"}, "description": "Facts supporting this update."}
                },
                "required": ["target_memory_id", "new_content", "evidence_facts"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "infer_memory",
            "description": "Look for higher-level insights. If combining multiple existing memories reveals a hidden connection or causality, create an inferred memory.",
            "parameters": {
                "type": "object",
                "properties": {
                    "source_memory_ids": {"type": "array", "items": {"type": "string"}, "description": "List of simplified Integer IDs (e.g., ['0', '1']) acting as premises, found in the [EXISTING MEMORIES] list."},
                    "inference_content": {"type": "string", "description": "The higher-level insight or inference derived from combining the source memories."},
                    "evidence_facts": {"type": "array", "items": {"type": "string"}, "description": "Facts supporting this inference."}
                },
                "required": ["source_memory_ids", "inference_content", "evidence_facts"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "delete_memory",
            "description": "Archive/Soft-delete a memory if it explicitly contradicts a new fact (and the new fact is trusted), or if the memory is no longer valid.",
            "parameters": {
                "type": "object",
                "properties": {
                    "target_memory_id": {"type": "string", "description": "The simplified Integer ID (e.g., '1') of the memory to delete, found in the [EXISTING MEMORIES] list."},
                    "evidence_facts": {"type": "array", "items": {"type": "string"}, "description": "Facts supporting this deletion."}
                },
                "required": ["target_memory_id", "evidence_facts"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "no_operation",
            "description": "No action needed if the fact is redundant (already exactly covered by memory or its associated facts).",
            "parameters": {
                "type": "object",
                "properties": {"reason": {"type": "string", "description": "The reason for no operation."}},
                "required": ["reason"]
            }
        }
    }
]

# --- UTILS ---
def get_embedding(text: str) -> List[float]:
    text = text.replace("\n", " ")
    return llm_client.embeddings.create(input=[text], model="text-embedding-3-small").data[0].embedding

@dataclass
class MilvusConfig:
    """Milvusé…ç½®ç±»ï¼ˆå…¼å®¹æ—§ä»£ç ï¼‰"""
    uri: str = os.getenv("MILVUS_URI")
    user_name: str = os.getenv("MILVUS_USER_NAME")
    # password: str = os.getenv("MILVUS_PASSWORD")
    db_name: str = os.getenv("MILVUS_DB_NAME", "default")
    dimension: int = 1536
    
    def to_vector_db_config(self, vector_db_type: str = "milvus") -> VectorDBConfig:
        """è½¬æ¢ä¸ºVectorDBConfig"""
        # ç¡®ä¿vector_db_typeæ˜¯å­—ç¬¦ä¸²ç±»å‹
        if not isinstance(vector_db_type, str):
            vector_db_type = "milvus"  # é»˜è®¤ä½¿ç”¨milvus
        
        # æ ¹æ®vector_db_typeé€‰æ‹©ä¸åŒçš„URL
        if vector_db_type == "qdrant":
            uri = os.getenv("QDRANT_URL")
            api_key = os.getenv("QDRANT_API_KEY")
            user_name = ""
            password = ""
        else:
            uri = self.uri
            api_key = ""
            user_name = self.user_name
            password = os.getenv("MILVUS_PASSWORD")
        
        return VectorDBConfig(
            uri=uri,
            user_name=user_name,
            password=password,
            api_key=api_key,
            db_name=self.db_name,
            dimension=self.dimension,
            vector_db_type=vector_db_type
        )

# ==========================================
# 1. Pipeline Class
# ==========================================
class MemoryPipeline:
    def __init__(self, config=None, vector_db_type="milvus", clear_db=False, mode='eval', dataset_name=""):
        """åˆå§‹åŒ–MemoryPipeline
        
        Args:
            config: MilvusConfigæˆ–VectorDBConfigå®ä¾‹ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
            vector_db_type: æŒ‡å®šä½¿ç”¨çš„å‘é‡æ•°æ®åº“ç±»å‹ï¼Œæ”¯æŒ"milvus"æˆ–"qdrant"
            clear_db: æ˜¯å¦æ¸…ç©ºæ•°æ®åº“ï¼Œé»˜è®¤ä¸ºFalse
            dataset_name: æ•°æ®é›†åç§°ï¼Œç”¨äºé›†åˆåç§°åç¼€ï¼Œé»˜è®¤ä¸ºç©º
        """
        # å¦‚æœæ²¡æœ‰æä¾›é…ç½®ï¼Œåˆ›å»ºé»˜è®¤é…ç½®
        if config is None:
            config = MilvusConfig()
        
        self.config = config
        
        # è½¬æ¢ä¸ºVectorDBConfig
        if hasattr(config, 'to_vector_db_config'):
            vector_db_config = config.to_vector_db_config(vector_db_type=vector_db_type)
        else:
            # å¦‚æœå·²ç»æ˜¯VectorDBConfigå®ä¾‹ï¼Œç›´æ¥ä½¿ç”¨
            vector_db_config = config
        
        # ä½¿ç”¨å·¥å‚ç±»åˆ›å»ºå‘é‡æ•°æ®åº“å®¢æˆ·ç«¯
        self.client = VectorDBFactory.create_db(vector_db_config)
        
        # æ ¹æ®æ¨¡å¼å’Œæ•°æ®é›†åç§°è®¾ç½®é›†åˆåç§°
        base_suffix = "_test" if mode == 'test' else ""
        dataset_suffix = f"_{dataset_name}" if dataset_name else ""
        full_suffix = f"{base_suffix}{dataset_suffix}"
        
        self.mem_col = f"memories{full_suffix}"
        self.fact_col = f"facts{full_suffix}"
        self.chunk_col = f"chunks{full_suffix}"
        
        self.dim = vector_db_config.dimension  # Save dimension as instance variable
        # åˆå§‹åŒ–æ“ä½œæ¬¡æ•°è®¡æ•°å™¨
        self.operation_counts = {"ADD": 0, "UPDATE": 0, "DELETE": 0, "INFER": 0, "NOOP": 0}
        self._init_collections(clear_db=clear_db)

    def _init_collections(self, clear_db=False):
        dim = self.config.dimension
        
        # å¦‚æœéœ€è¦æ¸…ç©ºæ•°æ®åº“ï¼Œå…ˆåˆ é™¤æ‰€æœ‰é›†åˆ
        if clear_db:
            print("æ­£åœ¨æ¸…ç©ºæ•°æ®åº“...")
            # ç›´æ¥åˆ é™¤é›†åˆï¼Œä¸æ£€æŸ¥å­˜åœ¨æ€§
            self.client.drop_collection(self.mem_col)
            self.client.drop_collection(self.fact_col)
            self.client.drop_collection(self.chunk_col)
            print("æ•°æ®åº“æ¸…ç©ºå®Œæˆ.")
        
        # æ£€æŸ¥å¹¶åˆ›å»ºé›†åˆ
        
        # å¤„ç† memories é›†åˆ
        if hasattr(self.client, 'DataType'):
            # è¿™æ˜¯ Milvus å®¢æˆ·ç«¯
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            if not self.client.has_collection(self.mem_col):
                # åˆ›å»ºå®Œæ•´çš„schema
                s = self.client.create_schema(auto_id=False, enable_dynamic_field=True)
                s.add_field("memory_id", self.client.DataType.VARCHAR, max_length=64, is_primary=True)
                s.add_field("embedding", self.client.DataType.FLOAT_VECTOR, dim=dim)
                s.add_field("content", self.client.DataType.VARCHAR, max_length=65535)
                s.add_field("user_id", self.client.DataType.VARCHAR, max_length=64)
                s.add_field("status", self.client.DataType.VARCHAR, max_length=16)
                s.add_field("created_at", self.client.DataType.INT64)
                s.add_field("updated_at", self.client.DataType.INT64)
                s.add_field("relations", self.client.DataType.JSON) 
                
                # åˆ›å»ºé›†åˆ
                self.client.create_collection(self.mem_col, schema=s)
                print(f"Collection '{self.mem_col}' created.")
                
                # ç›´æ¥åˆ›å»ºç´¢å¼•ï¼Œä¸æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
                # Milvusçš„create_indexæ–¹æ³•ä¼šåœ¨ç´¢å¼•å·²å­˜åœ¨æ—¶è‡ªåŠ¨è·³è¿‡æˆ–è¿”å›æˆåŠŸ
                try:
                    print(f"ä¸ºé›†åˆ '{self.mem_col}' åˆ›å»ºç´¢å¼•...")
                    idx_params = self.client.prepare_index_params()
                    idx_params.add_index(field_name="embedding", index_type="IVF_FLAT", metric_type="COSINE", params={"nlist": 128})
                    self.client.create_index(self.mem_col, index_params=idx_params)
                    print(f"é›†åˆ '{self.mem_col}' çš„ç´¢å¼•åˆ›å»ºæˆåŠŸæˆ–å·²å­˜åœ¨")
                except Exception as e:
                    print(f"åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
            else:
                print(f"Collection '{self.mem_col}' already exists, skipping creation.")
        else:
            # éMilvuså®¢æˆ·ç«¯ï¼Œç›´æ¥åˆ›å»ºé›†åˆ
            self.client.create_collection(self.mem_col)
            print(f"Collection '{self.mem_col}' created or exists.")
        
        # å¤„ç† facts é›†åˆ
        if hasattr(self.client, 'DataType'):
            # è¿™æ˜¯ Milvus å®¢æˆ·ç«¯
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            if not self.client.has_collection(self.fact_col):
                s = self.client.create_schema(auto_id=False, enable_dynamic_field=True)
                s.add_field("fact_id", self.client.DataType.VARCHAR, max_length=64, is_primary=True)
                s.add_field("linked_memory_ids", self.client.DataType.JSON)
                s.add_field("linked_chunk_id", self.client.DataType.VARCHAR, max_length=64)
                s.add_field("text", self.client.DataType.VARCHAR, max_length=65535)
                s.add_field("details", self.client.DataType.JSON)  # æ·»åŠ detailså­—æ®µ
                s.add_field("timestamp", self.client.DataType.INT64)
                s.add_field("user_id", self.client.DataType.VARCHAR, max_length=64)  # æ·»åŠ user_idå­—æ®µ
                s.add_field("embedding", self.client.DataType.FLOAT_VECTOR, dim=dim)
                
                # åˆ›å»ºé›†åˆ
                self.client.create_collection(self.fact_col, schema=s)
                print(f"Collection '{self.fact_col}' created.")
                
                # ç›´æ¥åˆ›å»ºç´¢å¼•ï¼Œä¸æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
                # Milvusçš„create_indexæ–¹æ³•ä¼šåœ¨ç´¢å¼•å·²å­˜åœ¨æ—¶è‡ªåŠ¨è·³è¿‡æˆ–è¿”å›æˆåŠŸ
                try:
                    print(f"ä¸ºé›†åˆ '{self.fact_col}' åˆ›å»ºç´¢å¼•...")
                    idx_params = self.client.prepare_index_params()
                    idx_params.add_index(field_name="embedding", index_type="IVF_FLAT", metric_type="COSINE", params={"nlist": 128})
                    self.client.create_index(self.fact_col, index_params=idx_params)
                    print(f"é›†åˆ '{self.fact_col}' çš„ç´¢å¼•åˆ›å»ºæˆåŠŸæˆ–å·²å­˜åœ¨")
                except Exception as e:
                    print(f"åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
            else:
                print(f"Collection '{self.fact_col}' already exists, skipping creation.")
        else:
            # éMilvuså®¢æˆ·ç«¯ï¼Œç›´æ¥åˆ›å»ºé›†åˆ
            self.client.create_collection(self.fact_col)
            print(f"Collection '{self.fact_col}' created or exists.")
        
        # å¤„ç† chunks é›†åˆ
        if hasattr(self.client, 'DataType'):
            # è¿™æ˜¯ Milvus å®¢æˆ·ç«¯
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            if not self.client.has_collection(self.chunk_col):
                s = self.client.create_schema(auto_id=False, enable_dynamic_field=True)
                s.add_field("chunk_id", self.client.DataType.VARCHAR, max_length=64, is_primary=True)
                s.add_field("text", self.client.DataType.VARCHAR, max_length=65535)
                s.add_field("timestamp", self.client.DataType.INT64)
                s.add_field("embedding", self.client.DataType.FLOAT_VECTOR, dim=dim)
                
                # åˆ›å»ºé›†åˆ
                self.client.create_collection(self.chunk_col, schema=s)
                print(f"Collection '{self.chunk_col}' created.")
                
                # ç›´æ¥åˆ›å»ºç´¢å¼•ï¼Œä¸æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
                # Milvusçš„create_indexæ–¹æ³•ä¼šåœ¨ç´¢å¼•å·²å­˜åœ¨æ—¶è‡ªåŠ¨è·³è¿‡æˆ–è¿”å›æˆåŠŸ
                try:
                    print(f"ä¸ºé›†åˆ '{self.chunk_col}' åˆ›å»ºç´¢å¼•...")
                    idx_params = self.client.prepare_index_params()
                    idx_params.add_index(field_name="embedding", index_type="IVF_FLAT", metric_type="COSINE", params={"nlist": 128})
                    self.client.create_index(self.chunk_col, index_params=idx_params)
                    print(f"é›†åˆ '{self.chunk_col}' çš„ç´¢å¼•åˆ›å»ºæˆåŠŸæˆ–å·²å­˜åœ¨")
                except Exception as e:
                    print(f"åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
            else:
                print(f"Collection '{self.chunk_col}' already exists, skipping creation.")
        else:
            # éMilvuså®¢æˆ·ç«¯ï¼Œç›´æ¥åˆ›å»ºé›†åˆ
            self.client.create_collection(self.chunk_col)
            print(f"Collection '{self.chunk_col}' created or exists.")
        
        # ç›´æ¥åŠ è½½æ‰€æœ‰é›†åˆï¼Œä¸è¿›è¡Œå¤æ‚çš„é”™è¯¯å¤„ç†
        print("Loading collections into memory...")
        
        # åŠ è½½é›†åˆï¼ˆQdrant ä¸éœ€è¦æ˜¾å¼åŠ è½½ï¼‰
        if hasattr(self.client, 'load_collection'):
            # ä¸ºæ¯ä¸ªé›†åˆåˆ›å»ºç´¢å¼•åç›´æ¥åŠ è½½
            print(f"åŠ è½½é›†åˆ '{self.mem_col}'...")
            self.client.load_collection(self.mem_col)
            
            print(f"åŠ è½½é›†åˆ '{self.fact_col}'...")
            self.client.load_collection(self.fact_col)
            
            print(f"åŠ è½½é›†åˆ '{self.chunk_col}'...")
            self.client.load_collection(self.chunk_col)
            
            print("All collections loaded successfully.")

    # --- Step 1: Extract ---
    def step_extract(self, chunk_text: str, extract_mode: str = "whole", timestamp: int = None) -> Dict:
        """
        ä»å¯¹è¯ä¸­æå–äº‹å®
        
        Args:
            chunk_text: å¯¹è¯æ–‡æœ¬
            extract_mode: æå–æ¨¡å¼ï¼Œå¯é€‰å€¼ï¼š
                - "whole": å¯¹æ•´ä¸ªchunkè¿›è¡Œæå–
                - "turn": æŒ‰è½®æ¬¡æå–ï¼Œæ¯è½®user-assistantå¯¹è¯å•ç‹¬æå–
            timestamp: æ—¶é—´æˆ³ï¼Œå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å½“å‰æ—¶é—´
        
        Returns:
            åŒ…å«æå–äº‹å®çš„å­—å…¸
        """
        # print(f"\nğŸ‘€ [1. Extract] Processing: '{chunk_text}'")
        
        # å¦‚æœæ²¡æœ‰æä¾›timestampï¼Œä½¿ç”¨å½“å‰æ—¶é—´
        if timestamp is None:
            timestamp = int(time.time())
        
        # å¦‚æœæ˜¯æŒ‰è½®æ¬¡æå–ï¼Œå…ˆè§£æå¯¹è¯è½®æ¬¡
        if extract_mode == "turn":
            # å°è¯•è§£æå¯¹è¯è½®æ¬¡
            try:
                # ç®€å•çš„è½®æ¬¡æ£€æµ‹ï¼šæŸ¥æ‰¾user:å’Œassistant:çš„ç»„åˆ
                import re
                # åŒ¹é…user: ... assistant: ... çš„æ¨¡å¼
                turn_pattern = r'(user: .*?)(?=assistant: |$)' 
                turns = re.findall(turn_pattern, chunk_text, re.DOTALL)
                
                # å¦‚æœæ‰¾åˆ°è½®æ¬¡ï¼Œå•ç‹¬å¤„ç†æ¯è½®
                if turns:
                    all_facts = []
                    for turn in turns:
                        # ç¡®ä¿æ¯è½®éƒ½æœ‰å®Œæ•´çš„user-assistantå¯¹è¯
                        turn_text = turn.strip()
                        if turn_text:
                            # å¯¹å•è½®å¯¹è¯æå–äº‹å®
                            turn_facts = self._extract_single_turn(turn_text)
                            all_facts.extend(turn_facts)
                    
                    return {"chunk_id": str(uuid.uuid4()), "chunk_text": chunk_text, "new_facts": all_facts, "timestamp": timestamp}
            except Exception as e:
                print(f"è§£æå¯¹è¯è½®æ¬¡å¤±è´¥ï¼Œå›é€€åˆ°wholeæ¨¡å¼: {e}")
        
        # é»˜è®¤æ¨¡å¼ï¼šå¯¹æ•´ä¸ªchunkè¿›è¡Œæå–
        facts = self._extract_single_turn(chunk_text)
        return {"chunk_id": str(uuid.uuid4()), "chunk_text": chunk_text, "new_facts": facts, "timestamp": timestamp}
    
    def _extract_single_turn(self, text: str) -> List[Dict]:
        """
        å¯¹å•ä¸ªæ–‡æœ¬ç‰‡æ®µæå–äº‹å®
        
        Args:
            text: è¦æå–äº‹å®çš„æ–‡æœ¬
            
        Returns:
            æå–åˆ°çš„äº‹å®åˆ—è¡¨
        """
        try:
            response = llm_client.chat.completions.create(
                model="gpt-4.1",
                messages=[
                        {"role": "system", "content": MEMREADER_PROMPT}, 
                        {"role": "user", "content": text}],
                response_format={"type": "json_object"}, temperature=0
            )
            fact_objects = json.loads(response.choices[0].message.content).get("facts", [])
            # ä¿ç•™å®Œæ•´çš„factå¯¹è±¡ï¼ŒåŒ…æ‹¬detailsä¿¡æ¯
            facts = []
            for fact_obj in fact_objects:
                if fact_obj.get("fact"):
                    facts.append({
                        "text": fact_obj.get("fact", ""),
                        "details": fact_obj.get("details", [])
                    })
        except Exception as e: 
            print(f"Extraction failed: {e}")
            facts = [{"text": text, "details": []}]
        return facts

    # --- Step 2: Retrieve ---    
    def step_retrieve(self, extract_result: Dict, limit: int = 3, user_id: str = 'default', similarity_threshold: float = None) -> List[Dict]:
        new_facts = extract_result['new_facts']
        if not new_facts: return []
        
        print(f"ğŸ” [2. Retrieve] Searching Memories for {len(new_facts)} facts...")
        context_bundles = []

        for fact in new_facts:
            query_vec = get_embedding(fact['text'])
            # æ·»åŠ user_idè¿‡æ»¤ï¼Œç¡®ä¿åªæ£€ç´¢å½“å‰ç”¨æˆ·çš„è®°å¿†
            res = self.client.search(
                self.mem_col, [query_vec], filter=f"status == 'active' and user_id == '{user_id}'", limit=limit,
                output_fields=["content", "memory_id", "created_at"],
                similarity_threshold=similarity_threshold
            )
            candidates = []
            if res and res[0]:
                for hit in res[0]:
                    candidates.append(hit['entity'])
            
            # æ£€ç´¢è¿™äº›è®°å¿†å…³è”çš„äº‹å®
            related_facts = []
            if candidates:
                # è·å–æ‰€æœ‰å€™é€‰è®°å¿†çš„ID
                memory_ids = [mem['memory_id'] for mem in candidates]
                # æ„å»ºæŸ¥è¯¢æ¡ä»¶ï¼ŒæŸ¥æ‰¾å…³è”åˆ°è¿™äº›è®°å¿†çš„äº‹å®
                expr_parts = [f'array_contains(linked_memory_ids, "{mem_id}")' for mem_id in memory_ids]
                filter_expr = " || ".join(expr_parts)
                
                try:
                    related_facts = self.client.query(
                        collection_name=self.fact_col,
                        filter=filter_expr,
                        output_fields=["fact_id", "linked_memory_ids", "text", "linked_chunk_id", "timestamp", "details"]
                    )
                except Exception as e:
                    print(f"   âš ï¸ Error retrieving related facts: {e}")
            
            # å°†related_factsæ·»åŠ åˆ°æ¯ä¸ªmemoryå¯¹è±¡ä¸­
            for mem in candidates:
                mem_id = mem['memory_id']
                mem['related_facts'] = [f for f in related_facts if mem_id in f.get('linked_memory_ids', [])]
            
            context_bundles.append({
                "new_fact": fact,
                "candidates": candidates
            })
            
        return context_bundles

    # --- Step 3: Decide (With ID Mapping) ---
    def step_decide(self, extract_result: Dict, context_bundles: List[Dict], user_id: str = 'default', training_mode: bool = False) -> List[Dict]:
        all_new_facts = extract_result['new_facts']
        
        # 1. åˆå¹¶å»é‡ Candidates
        temp_mem_storage = {}
        for bundle in context_bundles:
            for mem in bundle['candidates']:
                temp_mem_storage[mem['memory_id']] = mem
        
        unique_memories_list = list(temp_mem_storage.values())
        if not training_mode:
            print(f"ğŸ§  [3. Manager] Global Decide: {len(all_new_facts)} facts vs {len(unique_memories_list)} memories.")

        # ğŸŒŸ 2. æ„é€  ID æ˜ å°„ (Mapping Logic)
        uuid_mapping = {}  # { "0": "real-uuid", "1": "real-uuid" }
        candidates_str = ""

        if not unique_memories_list:
            candidates_str = "(No relevant memories found. Treat as new topic.)"
        else:
            for idx, mem in enumerate(unique_memories_list):
                simple_id = str(idx)
                real_uuid = mem['memory_id']
                uuid_mapping[simple_id] = real_uuid
                candidates_str += f"[Memory Item ID: {simple_id}]\n- Content: {mem['content']}\n"
                
                # æ·»åŠ å…³è”çš„facts
                related_facts = mem.get('related_facts', [])
                if related_facts:
                    candidates_str += "- Related Facts:\n"
                    for fact_idx, fact in enumerate(related_facts):
                        candidates_str += f"  - Fact {fact_idx + 1}: {fact['text']}\n"
                        # æ·»åŠ factçš„details
                        details = fact.get('details', [])
                        if details:
                            if isinstance(details, list):
                                for detail in details:
                                    if isinstance(detail, dict):
                                        detail_str = ", ".join([f"{k}: {v}" for k, v in detail.items()])
                                        candidates_str += f"    Detail: {detail_str}\n"
                                    else:
                                        candidates_str += f"    Detail: {detail}\n"
                            elif isinstance(details, dict):
                                detail_str = ", ".join([f"{k}: {v}" for k, v in details.items()])
                                candidates_str += f"    Detail: {detail_str}\n"
                candidates_str += "\n"

        # æ„é€ æœ€ç»ˆ Prompt
        system_msg = MEMORY_MANAGER_PROMPT

        # åªæå–äº‹å®çš„textå­—æ®µï¼Œä¸åŒ…å«detailsï¼Œé¿å…LLMå°†detailså½“ä½œç‹¬ç«‹äº‹å®
        fact_texts = [fact['text'] for fact in all_new_facts]
        
        user_content = f"""
        [New Facts Stream]
        {json.dumps(fact_texts, ensure_ascii=False)}
        
        [EXISTING MEMORIES]
        {candidates_str}
        """

        all_decisions = []
        try:
            # ä½¿ç”¨streamingæ¨¡å¼æ¥è·å–å®Œæ•´çš„å“åº”ï¼ŒåŒ…æ‹¬æ€ç»´è¿‡ç¨‹
            response = llm_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": user_content}
                ],
                tools=MEMORY_TOOLS,
                tool_choice="required",
                temperature=0,
                stream=True
            )
            
            # æ”¶é›†å®Œæ•´çš„å“åº”
            collected_messages = []
            for chunk in response:
                try:
                    if hasattr(chunk, 'choices') and chunk.choices:
                        choice = chunk.choices[0]
                        if hasattr(choice, 'delta') and hasattr(choice.delta, 'content') and choice.delta.content is not None:
                            collected_messages.append(choice.delta.content)
                except IndexError:
                    continue
            
            # æ‹¼æ¥å®Œæ•´çš„æ€è€ƒè¿‡ç¨‹
            thinking_process = ''.join(collected_messages)
            if thinking_process and not training_mode:
                print(f"\n   ğŸ§  LLMæ€è€ƒè¿‡ç¨‹:")
                print(f"   {thinking_process}")
            
            # é‡æ–°åˆ›å»ºéæµå¼å“åº”ä»¥è·å–å·¥å…·è°ƒç”¨
            response = llm_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": user_content}
                ],
                tools=MEMORY_TOOLS,
                tool_choice="required",
                temperature=0
            )
            
            # æ£€æŸ¥å“åº”ç»“æ„æ˜¯å¦å®Œæ•´
            if not response.choices or len(response.choices) == 0:
                if not training_mode:
                    print(f"   âš ï¸ Warning: No choices in response")
                return []
            
            choice = response.choices[0]
            if not hasattr(choice, 'message') or not choice.message:
                if not training_mode:
                    print(f"   âš ï¸ Warning: No message in choice")
                return []
            
            tool_calls = choice.message.tool_calls
            if not tool_calls: return []

            # ğŸŒŸ è¾…åŠ©å‡½æ•°: è¿˜åŸ ID
            def resolve_id(simple_id):
                real = uuid_mapping.get(str(simple_id))
                if not real and not training_mode:
                    print(f"   âš ï¸ Warning: LLM hallucinated ID '{simple_id}', ignoring.")
                return real

            for tool_call in tool_calls:
                try:
                    func_name = tool_call.function.name
                    args = json.loads(tool_call.function.arguments)
                    
                    if not training_mode:
                        print(f"   ğŸ¤– Raw Action: {func_name} | Args: {args}")
                    decision = {"action": "NOOP"}

                    if func_name == "create_memory":
                        decision.update({
                            "action": "ADD", 
                            "summary": args.get("content", ""), 
                            "facts_to_link": args.get("evidence_facts", []),
                            "user_id": user_id
                        })
                    
                    elif func_name == "update_memory":
                        if "target_memory_id" in args:
                            real_tid = resolve_id(args["target_memory_id"])
                            if real_tid:
                                orig_created = temp_mem_storage.get(real_tid, {}).get('created_at', int(time.time()))
                                decision.update({
                                    "action": "UPDATE", 
                                    "target_id": real_tid, 
                                    "new_content": args.get("new_content", ""), 
                                    "facts_to_link": args.get("evidence_facts", []), 
                                    "orig_created": orig_created,
                                    "user_id": user_id
                                })

                    elif func_name == "delete_memory":
                        if "target_memory_id" in args:
                            real_tid = resolve_id(args["target_memory_id"])
                            if real_tid:
                                orig_created = temp_mem_storage.get(real_tid, {}).get('created_at', int(time.time()))
                                decision.update({
                                    "action": "DELETE", 
                                    "target_id": real_tid, 
                                    "facts_to_link": args.get("evidence_facts", []), 
                                    "orig_created": orig_created,
                                    "user_id": user_id
                                })

                    elif func_name == "infer_memory":
                        if "source_memory_ids" in args:
                            source_simples = args["source_memory_ids"]
                            # ç¡®ä¿source_simplesæ˜¯åˆ—è¡¨
                            if not isinstance(source_simples, list):
                                source_simples = [source_simples]
                            real_source_ids = [resolve_id(sid) for sid in source_simples if resolve_id(sid)]
                            if real_source_ids:
                                decision.update({
                                    "action": "INFER", 
                                    "source_ids": real_source_ids, 
                                    "summary": args.get("inference_content", ""), 
                                    "facts_to_link": args.get("evidence_facts", []),
                                    "user_id": user_id
                                })

                    elif func_name == "no_operation":
                        decision.update({"reason": args.get("reason", "No reason provided"), "user_id": user_id})
                    
                    if decision["action"] != "NOOP" or "reason" in decision:
                        all_decisions.append(decision)
                except Exception as e:
                    if not training_mode:
                        print(f"   âš ï¸ Error processing tool call: {e}")
                    continue

        except Exception as e:
            if not training_mode:
                print(f"   âš ï¸ Decision Error: {e}")
        
        return all_decisions
        
    # --- Batch Processing for Training with GRPO Support ---
    def batch_process(self, batch_data: List[Dict], user_id: str = 'default', grpo_compatible: bool = True) -> List[Dict]:
        """
        Batch processing for memory management training with GRPO compatibility.
        
        Args:
            batch_data (List[Dict]): List of input data for batch processing.
            user_id (str, optional): User ID for memory operations. Defaults to 'default'.
            grpo_compatible (bool, optional): Whether to return GRPO-compatible format. Defaults to True.
            
        Returns:
            List[Dict]: List of results for each input in the batch.
        """
        results = []
        
        for data in batch_data:
            # Extract facts from input text
            extract_result = self.step_extract(data['text'], extract_mode='whole')
            
            # Retrieve relevant memories
            context_bundles = self.step_retrieve(extract_result, limit=3, user_id=user_id)
            
            # Make decisions (memory operations) in training mode
            decisions = self.step_decide(extract_result, context_bundles, user_id=user_id, training_mode=True)
            
            # Execute decisions
            self.step_execute(decisions, extract_result, user_id=user_id)
            
            if grpo_compatible:
                # Format result for GRPO training
                result = {
                    'input': data['text'],
                    'extract_result': extract_result,
                    'decisions': decisions,
                    # Add GRPO-specific fields
                    'memory_operations': [d['action'] for d in decisions if d['action'] != 'NOOP'],
                    'memory_contents': [d.get('summary', '') for d in decisions if d['action'] != 'NOOP'],
                    # Ensure we have the expected_operation if provided in data
                    'expected_operation': data.get('expected_operation', '')
                }
            else:
                # Standard format for non-GRPO training
                result = {
                    'input': data['text'],
                    'extract_result': extract_result,
                    'decisions': decisions
                }
            
            results.append(result)
        
        return results

    # ==========================================
    # Step 4: Execute (Modified for Fact Inheritance)
    # ==========================================
    def step_execute(self, decisions: List[Dict], extract_result: Dict, user_id: str = 'default'):
        # ä½¿ç”¨extract_resultä¸­çš„timestampï¼Œè€Œä¸æ˜¯å½“å‰æ—¶é—´
        ts = extract_result['timestamp']
        chunk_id = extract_result['chunk_id']
        all_new_facts = extract_result['new_facts']
        
        # 1. ä¿å­˜åŸå§‹ Chunk
        self.client.insert(self.chunk_col, [{"chunk_id": chunk_id, "text": extract_result["chunk_text"], "timestamp": ts, "embedding": get_embedding(extract_result["chunk_text"])}])

        # 2. æ”¶é›†æ‰€æœ‰è¦é“¾æ¥çš„äº‹å®æ–‡æœ¬
        all_facts_to_link = set()
        for decision in decisions:
            action = decision.get("action")
            facts_to_link = decision.get('facts_to_link', [])
            for fact_text in facts_to_link:
                all_facts_to_link.add(fact_text)
        
        # 3. å¯¹æ‰€æœ‰è¦å¤„ç†çš„äº‹å®è¿›è¡Œæœ€ç»ˆå»é‡
        # æ”¶é›†æ‰€æœ‰æ–°äº‹å®
        all_facts = []
        for fact in all_new_facts:
            # åªå¤„ç†åœ¨all_facts_to_linkä¸­çš„äº‹å®
            if fact['text'] in all_facts_to_link:
                all_facts.append(fact)
        
        # å¯¹æ‰€æœ‰äº‹å®è¿›è¡Œå»é‡
        unique_all_facts = []
        seen_fact_keys = set()
        for fact in all_facts:
            fact_key = f"{fact['text']}::{json.dumps(fact['details'], sort_keys=True)}"
            # ä¹Ÿè€ƒè™‘å»æ‰"User"å‰ç¼€çš„æƒ…å†µ
            stripped_fact_key = f"{fact['text'].lower().replace('user ', '')}::{json.dumps(fact['details'], sort_keys=True)}"
            if fact_key not in seen_fact_keys and stripped_fact_key not in seen_fact_keys:
                seen_fact_keys.add(fact_key)
                seen_fact_keys.add(stripped_fact_key)
                unique_all_facts.append(fact)
        
        if len(unique_all_facts) < len(all_facts):
            print(f"   âœ… æœ€ç»ˆå»é‡ {len(all_facts) - len(unique_all_facts)} ä¸ªé‡å¤äº‹å®")
        
        # æ›´æ–°all_facts_to_linkä¸ºå»é‡åçš„äº‹å®æ–‡æœ¬é›†åˆ
        all_facts_to_link = {fact['text'] for fact in unique_all_facts}

        # 3. å¤„ç†æ¯ä¸ªå†³ç­–
        has_non_noop_action = False
        
        # æ”¶é›†æ‰€æœ‰è¦é“¾æ¥çš„äº‹å®ï¼Œç¡®ä¿å»é‡
        all_matched_facts = []
        seen_fact_keys = set()
        
        for decision in decisions:
            action = decision.get("action")
            if action == "NOOP":
                self.operation_counts["NOOP"] += 1
                print(f"   ğŸš« No operation: {decision.get('reason', 'No reason provided')}")
                continue

            has_non_noop_action = True
            target_mem_id = None
            relations = []

            # --- CASE 1: ADD ---
            if action == "ADD":
                self.operation_counts["ADD"] += 1
                target_mem_id = str(uuid.uuid4())
                self._upsert_mem(target_mem_id, decision['summary'], ts, ts, "active", [], decision.get('user_id', 'default'))
                print(f"   âœ… Created Mem: {target_mem_id[:8]}... | Content: {decision['summary']}")

            # --- CASE 2: UPDATE ---
            elif action == "UPDATE":
                self.operation_counts["UPDATE"] += 1
                target_mem_id = decision['target_id']
                
                # æŸ¥è¯¢æ—§çš„memoryå†…å®¹
                old_memories = self.client.query(
                    collection_name=self.mem_col,
                    filter=f"memory_id == '{target_mem_id}'",
                    output_fields=["content", "created_at"]
                )
                
                old_content = "" if not old_memories else old_memories[0].get("content", "")
                new_content = decision['new_content']
                
                # è®°å½•updateå‰åçš„å†…å®¹
                print(f"   ğŸ”„ Updating Mem: {target_mem_id[:8]}...")
                print(f"      Before: {old_content[:]}...")
                print(f"      After:  {new_content[:]}...")
                
                self._upsert_mem(target_mem_id, new_content, decision['orig_created'], ts, "active", [], decision.get('user_id', 'default'))

            # --- CASE 3: DELETE ---
            elif action == "DELETE":
                self.operation_counts["DELETE"] += 1
                target_mem_id = decision['target_id']
                self._upsert_mem(target_mem_id, "(Archived)", decision['orig_created'], ts, "archived", [], decision.get('user_id', 'default'))
                print(f"   âŒ Deleted Mem: {target_mem_id[:8]}...")

            # --- CASE 4: INFER (With Fact Inheritance) ---
            elif action == "INFER":
                self.operation_counts["INFER"] += 1
                target_mem_id = str(uuid.uuid4()) # è¿™æ˜¯ Memory C
                source_ids = decision.get('source_ids', []) # è¿™æ˜¯ [A, B]
                #############################################################
                # æŸ¥è¯¢source_idså¯¹åº”çš„memoryå†…å®¹ï¼Œç”¨äºæ‰“å°
                source_mems = []
                if source_ids:
                    quoted_source_ids = [f'"{sid}"' for sid in source_ids]
                    mem_filter = f"status == 'active' and memory_id in [{','.join(quoted_source_ids)}]"
                    try:
                        source_mems = self.client.query(
                            collection_name=self.mem_col,
                            filter=mem_filter,
                            output_fields=["content", "memory_id", "created_at", "user_id"]
                        )
                    except Exception as e:
                        print(f"   âš ï¸ æŸ¥è¯¢source memoryå¤±è´¥: {e}")
                #############################################################
                # 4.1 åˆ›å»ºæ–°è®°å¿† Cï¼Œå¹¶è®°å½•è¡€ç¼˜å…³ç³» (inferred_from)
                relations = [{"type": "inferred_from", "target_id": sid} for sid in source_ids]
                self._upsert_mem(target_mem_id, decision['summary'], ts, ts, "active", relations, decision.get('user_id', 'default'))
                ####################################################################################
                # å°†inferå‰åçš„memoryå†…å®¹æ‹¼åœ¨åŒä¸€ä¸ªå­—ç¬¦ä¸²é‡Œè¾“å‡º
                infer_output = f"   ğŸ’¡ Inferred Mem: {target_mem_id[:8]}... | From: {[s[:8] for s in source_ids]}\n"
                infer_output += f"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
                
                # æ‹¼æ¥inferå‰çš„memoryå†…å®¹
                if source_mems:
                    infer_output += f"   â”‚ ğŸ“‹ Infer å‰çš„ Memory ({len(source_mems)}ä¸ª):\n"
                    for mem in source_mems:
                        mem_id = mem.get("memory_id", "unknown")
                        content = mem.get("content", "")
                        infer_output += f"   â”‚      ğŸ“Œ ID: {mem_id[:8]}... | å†…å®¹: {content[:]}...\n"
                
                # æ‹¼æ¥inferç”Ÿæˆçš„memoryå†…å®¹
                infer_output += f"   â”‚ ğŸ“ Inferç”Ÿæˆçš„ Memory:\n"
                infer_output += f"   â”‚      ğŸ“Œ ID: {target_mem_id[:8]}... | å†…å®¹: {decision['summary'][:]}...\n"
                infer_output += f"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
                
                # ä¸€æ¬¡æ€§è¾“å‡ºæ•´ä¸ªå­—ç¬¦ä¸²
                print(infer_output)
                #################################################################################
                # 4.2 ğŸŒŸ æ ¸å¿ƒä¿®æ”¹ï¼šç»§æ‰¿æ—§ Facts
                # é€»è¾‘ï¼šæ‰¾å‡ºæ‰€æœ‰æ”¯æŒ A æˆ– B çš„ Factï¼ŒæŠŠ C ä¹ŸåŠ åˆ°å®ƒä»¬çš„æ”¯æŒåˆ—è¡¨é‡Œ
                if source_ids:
                    # æ„å»ºæŸ¥è¯¢è¡¨è¾¾å¼ï¼šarray_contains(linked_memory_ids, 'A') or ...
                    expr_parts = [f'array_contains(linked_memory_ids, "{sid}")' for sid in source_ids]
                    filter_expr = " || ".join(expr_parts)
                    
                    try:
                        # æŸ¥å‡ºæ—§ Facts
                        old_related_facts = self.client.query(
                            collection_name=self.fact_col,
                            filter=filter_expr,
                            output_fields=["fact_id", "linked_memory_ids", "text", "linked_chunk_id", "timestamp", "details", "embedding"]
                        )
                        
                        if old_related_facts:
                            updated_rows = []
                            for fact in old_related_facts:
                                current_links = fact.get("linked_memory_ids", [])
                                # å¦‚æœ C è¿˜æ²¡å…³è”ä¸Šï¼Œå°±åŠ è¿›å»
                                if target_mem_id not in current_links:
                                    current_links.append(target_mem_id)
                                    # ç¡®ä¿factsåŒ…å«detailså­—æ®µ
                                    if "details" not in fact:
                                        fact["details"] = []
                                    # ç¡®ä¿factsåŒ…å«embeddingå­—æ®µ
                                    if "embedding" not in fact or not isinstance(fact["embedding"], list):
                                        text = fact.get("text", "")
                                        details = fact.get("details", [])
                                        fact["embedding"] = self._generate_fact_embedding(text, details)
                                    # ç¡®ä¿factsåŒ…å«user_idå­—æ®µ
                                    if "user_id" not in fact:
                                        fact["user_id"] = user_id
                                    updated_rows.append(fact)
                            
                            # å†™å›æ•°æ®åº“ (Upsert)
                            if updated_rows:
                                self.client.upsert(self.fact_col, updated_rows)
                                print(f"      â†³ ğŸ§¬ Inherited {len(updated_rows)} old facts from sources.")
                    except Exception as e:
                        print(f"      âš ï¸ Error inheriting facts: {e}")

            # --- Common: Link NEW Facts for this decision ---
            # æ— è®ºæ˜¯ ADD, UPDATE è¿˜æ˜¯ INFERï¼Œéƒ½ä¼šæŠŠå½“å‰å†³ç­–çš„æ–°è¯æ®å…³è”ä¸Šå»
            facts_to_link = decision.get('facts_to_link', [])
            if target_mem_id and facts_to_link:
                # æŸ¥æ‰¾ä¸å¾…é“¾æ¥äº‹å®æ–‡æœ¬åŒ¹é…çš„å®Œæ•´äº‹å®å¯¹è±¡ï¼ˆåŒ…å«detailså’Œfact_idï¼‰
                for fact_text in facts_to_link:
                    # åœ¨æ‰€æœ‰æ–°äº‹å®ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ–‡æœ¬ï¼Œä»¥è·å–å®Œæ•´çš„factå¯¹è±¡ï¼ˆåŒ…å«detailså’Œfact_idï¼‰
                    matching_fact = next((f for f in all_new_facts if f['text'] == fact_text), None)
                    if matching_fact:
                        # æ£€æŸ¥äº‹å®æ˜¯å¦å·²ç»è¢«å¤„ç†è¿‡
                        fact_key = f"{matching_fact['text']}::{json.dumps(matching_fact['details'], sort_keys=True)}"
                        # ä¹Ÿè€ƒè™‘å»æ‰"User"å‰ç¼€çš„æƒ…å†µ
                        stripped_fact_key = f"{matching_fact['text'].lower().replace('user ', '')}::{json.dumps(matching_fact['details'], sort_keys=True)}"
                        if fact_key not in seen_fact_keys and stripped_fact_key not in seen_fact_keys:
                            seen_fact_keys.add(fact_key)
                            seen_fact_keys.add(stripped_fact_key)
                            # æ·»åŠ ç›®æ ‡è®°å¿†IDåˆ°äº‹å®ä¸­
                            fact_with_target = matching_fact.copy()
                            fact_with_target['target_mem_id'] = target_mem_id
                            all_matched_facts.append(fact_with_target)
                    else:
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„å®Œæ•´äº‹å®å¯¹è±¡ï¼Œä½¿ç”¨æ–‡æœ¬åˆ›å»ºä¸€ä¸ªç®€å•çš„äº‹å®å¯¹è±¡
                        new_fact = {'text': fact_text, 'details': [], 'fact_id': str(uuid.uuid4()), 'target_mem_id': target_mem_id}
                        fact_key = f"{new_fact['text']}::{json.dumps(new_fact['details'], sort_keys=True)}"
                        stripped_fact_key = f"{new_fact['text'].lower().replace('user ', '')}::{json.dumps(new_fact['details'], sort_keys=True)}"
                        if fact_key not in seen_fact_keys and stripped_fact_key not in seen_fact_keys:
                            seen_fact_keys.add(fact_key)
                            seen_fact_keys.add(stripped_fact_key)
                            all_matched_facts.append(new_fact)
        
        # æ‰¹é‡å¤„ç†æ‰€æœ‰åŒ¹é…çš„äº‹å®ï¼Œç¡®ä¿æ¯ä¸ªäº‹å®åªè¢«å…³è”åˆ°ç›¸åº”çš„è®°å¿†
        if all_matched_facts:
            # æŒ‰äº‹å®IDåˆ†ç»„
            facts_by_id = {}
            for fact in all_matched_facts:
                fact_id = fact['fact_id']
                if fact_id not in facts_by_id:
                    facts_by_id[fact_id] = {
                        'fact': fact,
                        'target_mem_ids': set()
                    }
                facts_by_id[fact_id]['target_mem_ids'].add(fact['target_mem_id'])
            
            # æ„å»ºè¦å†™å…¥æ•°æ®åº“çš„è¡Œ
            rows = []
            for fact_info in facts_by_id.values():
                fact = fact_info['fact']
                fact_id = fact['fact_id']  # è·å–å½“å‰factçš„fact_id
                target_mem_ids = list(fact_info['target_mem_ids'])
                
                rows.append({
                    "fact_id": fact_id,
                    "linked_memory_ids": target_mem_ids, # å…³è”åˆ°æ‰€æœ‰ç›¸å…³çš„è®°å¿†
                    "linked_chunk_id": chunk_id,
                    "text": fact['text'],
                    "details": fact['details'],  # ä¿å­˜äº‹å®çš„detailsä¿¡æ¯
                    "timestamp": ts,
                    "user_id": decision.get('user_id', user_id),
                    "embedding": self._generate_fact_embedding(fact['text'], fact['details'])
                })
            
            if rows:
                self.client.upsert(self.fact_col, rows)
                print(f"   ğŸ”— æ‰¹é‡å…³è” {len(rows)} ä¸ªäº‹å®åˆ°å¯¹åº”è®°å¿†")

        # 4. å¤„ç†æœªå…³è”åˆ°ä»»ä½•è®°å¿†çš„æ–°äº‹å®ï¼ˆå½“æ‰€æœ‰å†³ç­–éƒ½æ˜¯NOOPæ—¶ï¼‰
        # æ‰¾å‡ºæ‰€æœ‰æœªè¢«å…³è”çš„æ–°äº‹å®
        unlinked_facts = []
        for fact in all_new_facts:
            if fact['text'] not in all_facts_to_link:
                unlinked_facts.append(fact)

        # å¦‚æœæœ‰æœªå…³è”çš„æ–°äº‹å®ï¼Œç›´æ¥ä¿å­˜åˆ°fact_colé›†åˆä¸­
        if unlinked_facts:
            rows = []
            for fact in unlinked_facts:
                # ä½¿ç”¨é¢„å¤„ç†æ­¥éª¤ä¸­ç”Ÿæˆçš„fact_idï¼Œè€Œä¸æ˜¯é‡æ–°ç”Ÿæˆ
                fact_id = fact.get('fact_id', str(uuid.uuid4()))
                rows.append({
                    "fact_id": fact_id,
                    "linked_memory_ids": [],  # ä¸å…³è”åˆ°ä»»ä½•è®°å¿†
                    "linked_chunk_id": chunk_id,
                    "text": fact['text'],
                    "details": fact['details'],  # ä¿å­˜äº‹å®çš„detailsä¿¡æ¯
                    "timestamp": ts,
                    "user_id": user_id,
                    "embedding": self._generate_fact_embedding(fact['text'], fact['details'])
                })
            if rows:
                self.client.upsert(self.fact_col, rows)
                print(f"   ğŸ’¾ Saved {len(rows)} unlinked facts to database...")

        # 5. å¤„ç†æ‰€æœ‰å†³ç­–éƒ½æ˜¯NOOPä½†æœ‰æ–°äº‹å®çš„æƒ…å†µ
        # å¦‚æœæ‰€æœ‰å†³ç­–éƒ½æ˜¯NOOPï¼Œä¸”æœ‰æ–°äº‹å®ï¼Œç¡®ä¿å®ƒä»¬éƒ½è¢«ä¿å­˜
        if not has_non_noop_action and all_new_facts:
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æœªä¿å­˜çš„æ–°äº‹å®
            all_fact_texts = {fact['text'] for fact in all_new_facts}
            saved_fact_texts = all_facts_to_link  # å·²ç»é€šè¿‡å†³ç­–å…³è”çš„äº‹å®
            unsaved_fact_texts = all_fact_texts - saved_fact_texts
            
            if unsaved_fact_texts:
                rows = []
                for fact in all_new_facts:
                    if fact['text'] in unsaved_fact_texts:
                        # ä½¿ç”¨é¢„å¤„ç†æ­¥éª¤ä¸­ç”Ÿæˆçš„fact_idï¼Œè€Œä¸æ˜¯é‡æ–°ç”Ÿæˆ
                        fact_id = fact.get('fact_id', str(uuid.uuid4()))
                        rows.append({
                            "fact_id": fact_id,
                            "linked_memory_ids": [],  # ä¸å…³è”åˆ°ä»»ä½•è®°å¿†
                            "linked_chunk_id": chunk_id,
                            "text": fact['text'],
                            "details": fact['details'],  # ä¿å­˜äº‹å®çš„detailsä¿¡æ¯
                            "timestamp": ts,
                            "user_id": user_id,
                            "embedding": self._generate_fact_embedding(fact['text'], fact['details'])
                        })
                if rows:
                    self.client.upsert(self.fact_col, rows)
                    print(f"   ğŸ’¾ Saved {len(rows)} unlinked facts to database (all actions were NOOP)...")
                    
    def _upsert_mem(self, mem_id, content, c_at, u_at, status, relations, user_id):
        self.client.upsert(self.mem_col, [{
            "memory_id": mem_id,
            "embedding": get_embedding(content),
            "content": content,
            "user_id": user_id,
            "status": status,
            "created_at": c_at,
            "updated_at": u_at,
            "relations": relations
        }])

    def step_preprocess_facts(self, extract_result: Dict, user_id: str = 'default') -> Dict:
        """
        é¢„å¤„ç†æå–å‡ºçš„äº‹å®ï¼Œæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨äºæ•°æ®åº“ä¸­ï¼Œç¡®ä¿ä»æºå¤´ä¸Šå»é‡
        
        Args:
            extract_result: æå–ç»“æœå­—å…¸ï¼ŒåŒ…å«new_facts
            user_id: ç”¨æˆ·æ ‡è¯†ï¼Œç¡®ä¿åªå¤„ç†å½“å‰ç”¨æˆ·çš„äº‹å®
            
        Returns:
            æ›´æ–°åçš„æå–ç»“æœå­—å…¸ï¼ŒåŒ…å«fact_idä¿¡æ¯
        """
        new_facts = extract_result['new_facts']
        processed_facts = []
        ts = extract_result['timestamp']
        chunk_id = extract_result['chunk_id']
        
        print(f"ğŸ” [Preprocess Facts] æ£€æŸ¥ {len(new_facts)} ä¸ªäº‹å®æ˜¯å¦å·²å­˜åœ¨...")
        
        # 1. å…ˆå¯¹åŒä¸€æ‰¹æ¬¡å†…çš„äº‹å®è¿›è¡Œå»é‡ï¼Œé¿å…åŒä¸€æ‰¹æ¬¡ä¸­é‡å¤çš„äº‹å®è¢«å¤„ç†
        unique_facts_in_batch = []
        seen_fact_keys = set()
        for fact in new_facts:
            # ä½¿ç”¨fact_textå’Œdetailsçš„ç»„åˆä½œä¸ºå”¯ä¸€æ ‡è¯†
            fact_key = f"{fact['text']}::{json.dumps(fact['details'], sort_keys=True)}"
            if fact_key not in seen_fact_keys:
                seen_fact_keys.add(fact_key)
                unique_facts_in_batch.append(fact)
        
        if len(unique_facts_in_batch) < len(new_facts):
            print(f"   âœ… åŒä¸€æ‰¹æ¬¡å†…å»é‡ {len(new_facts) - len(unique_facts_in_batch)} ä¸ªé‡å¤äº‹å®")
        
        for fact in unique_facts_in_batch:
            fact_text = fact['text']
            fact_details = fact['details']

            
            # 3. æŸ¥è¯¢æ•°æ®åº“ä¸­æ˜¯å¦å­˜åœ¨ç›¸åŒçš„fact
            existing_fact = None
            try:
                # å…ˆå°è¯•æœç´¢ç›¸å…³äº‹å®ï¼Œé¿å…å…¨é‡æŸ¥è¯¢
                # ä½¿ç”¨æ›´å®‰å…¨çš„æŸ¥è¯¢æ–¹å¼ï¼ŒåŸºäºtextçš„å‰ç¼€åŒ¹é…
                # åªæŸ¥è¯¢textå­—æ®µåŒ…å«fact_textå…³é”®è¯çš„äº‹å®
                search_vec = get_embedding(fact_text)
                search_results = self.client.search(
                    self.fact_col, [search_vec], 
                    output_fields=["fact_id", "details", "timestamp", "linked_memory_ids", "linked_chunk_id", "text"],
                    limit=20,  # åªæŸ¥è¯¢å‰20ä¸ªæœ€ç›¸ä¼¼çš„äº‹å®
                    similarity_threshold=0.8  # è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œåªè¿”å›ç›¸ä¼¼åº¦è¾ƒé«˜çš„äº‹å®
                )
                
                # å¤„ç†æœç´¢ç»“æœï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å®Œå…¨åŒ¹é…çš„äº‹å®
                if search_results and search_results[0]:
                    for hit in search_results[0]:
                        res = hit['entity']
                        res_text = res.get("text", "")
                        res_details = res.get("details", [])
                        # æ£€æŸ¥æ˜¯å¦æ˜¯ç›¸åŒçš„äº‹å®ï¼Œè€ƒè™‘åˆ°è¡¨è¿°å¯èƒ½ç•¥æœ‰ä¸åŒ
                        # 1. å®Œå…¨ç›¸åŒçš„æƒ…å†µ
                        if res_text == fact_text and res_details == fact_details:
                            existing_fact = res
                            break
                        # 2. æ ¸å¿ƒå†…å®¹ç›¸åŒä½†è¡¨è¿°ç•¥æœ‰ä¸åŒçš„æƒ…å†µï¼ˆå¦‚æœ‰æ— "User"å‰ç¼€ï¼‰
                        stripped_res_text = res_text.lower().replace("user ", "").strip()
                        stripped_fact_text = fact_text.lower().replace("user ", "").strip()
                        if stripped_res_text == stripped_fact_text and res_details == fact_details:
                            existing_fact = res
                            break
            
            except Exception as e:
                print(f"   âš ï¸ æŸ¥è¯¢äº‹å®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            
            if existing_fact:
                # äº‹å®å·²å­˜åœ¨ï¼Œæ›´æ–°timestamp
                fact_id = existing_fact["fact_id"]
                old_ts = existing_fact["timestamp"]
                
                # è·å–ç°æœ‰çš„linked_memory_idså’Œlinked_chunk_id
                existing_links = existing_fact.get("linked_memory_ids", [])
                existing_chunk = existing_fact.get("linked_chunk_id", "")
                
                # æ›´æ–°timestampå’Œå…³è”ä¿¡æ¯
                self.client.upsert(self.fact_col, [{
                    "fact_id": fact_id,
                    "linked_memory_ids": existing_links,
                    "linked_chunk_id": existing_chunk,
                    "text": fact_text,
                    "details": fact_details,
                    "timestamp": ts,
                    "user_id": user_id,
                    "embedding": self._generate_fact_embedding(fact_text, fact_details)
                }])
                
                # å°†ç°æœ‰äº‹å®æ·»åŠ åˆ°processed_facts
                processed_fact = {
                    "text": fact_text,
                    "details": fact_details,
                    "fact_id": fact_id
                }
                processed_facts.append(processed_fact)
                
                print(f"   ğŸ”„ äº‹å®å·²å­˜åœ¨ï¼Œæ›´æ–°timestamp: {fact_id} (æ—§: {old_ts}, æ–°: {ts})")
            else:
                # äº‹å®ä¸å­˜åœ¨ï¼Œç”Ÿæˆæ–°çš„fact_idå¹¶ä¿å­˜
                fact_id = str(uuid.uuid4())
                # print(f"   ğŸ†• æ–°äº‹å®: {fact_id}")
                
                # ä¿å­˜æ–°äº‹å®åˆ°æ•°æ®åº“
                self.client.upsert(self.fact_col, [{
                    "fact_id": fact_id,
                    "linked_memory_ids": [],
                    "linked_chunk_id": chunk_id,
                    "text": fact_text,
                    "details": fact_details,
                    "timestamp": ts,
                    "user_id": user_id,
                    "embedding": self._generate_fact_embedding(fact_text, fact_details)
                }])
                
                processed_fact = {
                    "text": fact_text,
                    "details": fact_details,
                    "fact_id": fact_id
                }
                
                processed_facts.append(processed_fact)
        
        # æ›´æ–°æå–ç»“æœ
        extract_result['new_facts'] = processed_facts
        return extract_result
    
    def process(self, text, retrieve_limit: int = 3, extract_mode: str = "whole", user_id: str = 'default', similarity_threshold: float = None, timestamp: int = None):
        res = self.step_extract(text, extract_mode=extract_mode, timestamp=timestamp)
        if not res['new_facts']: return
        
        # é¢„å¤„ç†äº‹å®ï¼Œæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        res = self.step_preprocess_facts(res, user_id=user_id)
        
        # æ£€æŸ¥é¢„å¤„ç†åæ˜¯å¦è¿˜æœ‰æ–°äº‹å®
        if not res['new_facts']:
            print(f"   âœ… æ‰€æœ‰äº‹å®éƒ½å·²å­˜åœ¨ï¼Œæ— éœ€å¤„ç†")
            return
        
        print(f"   æ–°è¯æ®: {res['new_facts']}")
        
        ctx_bundles = self.step_retrieve(res, limit=retrieve_limit, user_id=user_id, similarity_threshold=similarity_threshold)
        decisions = self.step_decide(res, ctx_bundles, user_id=user_id)
        self.step_execute(decisions, res, user_id=user_id)
        
    def process_user_memory_infer(self, line, retrieve_limit: int = 3, extract_mode: str = "whole", user_id: str = 'default', similarity_threshold: float = None):
        """å¤„ç†ç”¨æˆ·è®°å¿†ä¼šè¯ï¼Œæ”¯æŒlongmemevalæ•°æ®é›†æ ¼å¼"""
        # é‡ç½®æ“ä½œè®¡æ•°ï¼Œç¡®ä¿æ¯ä¸ªç”¨æˆ·çš„è®¡æ•°ç‹¬ç«‹
        self.operation_counts = {"ADD": 0, "UPDATE": 0, "DELETE": 0, "INFER": 0, "NOOP": 0}
        dates = line.get("haystack_dates")
        sessions = line.get("haystack_sessions")

        for session_id, session in enumerate(sessions):
            date = dates[session_id] + " UTC"
            date_format = "%Y/%m/%d (%a) %H:%M UTC"
            date_string = datetime.strptime(date, date_format).replace(tzinfo=timezone.utc)
            # ç”Ÿæˆtimestamp
            timestamp = int(date_string.timestamp())
            
            parsed_messages = parse_messages(session)
            print(f"å¤„ç†ä¼šè¯ {session_id + 1}/{len(sessions)}: {dates[session_id]}")
            
            # ä½¿ç”¨ç°æœ‰çš„processæ–¹æ³•å¤„ç†ä¼šè¯æ¶ˆæ¯ï¼Œä¼ é€’user_idã€similarity_thresholdå’Œtimestamp
            self.process(parsed_messages, retrieve_limit=retrieve_limit, extract_mode=extract_mode, user_id=user_id, similarity_threshold=similarity_threshold, timestamp=timestamp)
        
        # è¿”å›æ“ä½œæ¬¡æ•°ç»Ÿè®¡
        return self.operation_counts
        
    def search_memories(self, query_text, top_k=5, fact_top_k=5, user_id: str = 'default', threshold: float = 0.0, similarity_threshold: float = None, enhanced_search: bool = False, use_fact_retrieval: bool = True):
        """æœç´¢è®°å¿†å¹¶è¿”å›æ¯ä¸ªè®°å¿†å…³è”çš„topkä¸ªäº‹å®ï¼Œå¹¶æ ¹æ®å…³è”äº‹å®è¿›è¡Œrerank
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›çš„è®°å¿†æ•°é‡ä¸Šé™
            fact_top_k: æ¯ä¸ªè®°å¿†å…³è”çš„äº‹å®æ•°é‡ä¸Šé™
            user_id: ç”¨æˆ·æ ‡è¯†ï¼Œç¡®ä¿åªæ£€ç´¢å½“å‰ç”¨æˆ·çš„è®°å¿†
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºè¯¥é˜ˆå€¼çš„è®°å¿†å°†è¢«è¿‡æ»¤æ‰
            similarity_threshold: å‘é‡æ•°æ®åº“æœç´¢æ—¶çš„ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºè¯¥é˜ˆå€¼çš„è®°å¿†å°†è¢«è¿‡æ»¤æ‰
            enhanced_search: æ˜¯å¦å¯ç”¨å¢å¼ºå‹æœç´¢æ¨¡å¼ï¼Œå¯ç”¨åä¼šå¢å¼ºreranké€»è¾‘
            use_fact_retrieval: æ˜¯å¦ä½¿ç”¨äº‹å®æ£€ç´¢æ¨¡å¼ï¼Œå¯ç”¨åä¼šæœç´¢äº‹å®é›†åˆå¹¶æ ¹æ®å…³è”çš„memory_idè·å–æ›´å¤šè®°å¿†
        """
        query_vec = get_embedding(query_text)
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        filter_expr = f"status == 'active' and user_id == '{user_id}'"
        print(f"   ğŸ” æœç´¢è¿‡æ»¤æ¡ä»¶: {filter_expr}, é˜ˆå€¼: {threshold}, å‘é‡æœç´¢é˜ˆå€¼: {similarity_threshold}")
        
        # ===========================
        # 1. æœç´¢è®°å¿†é›†åˆï¼Œè·å–memoryA
        # ===========================
        mem_res = self.client.search(
            self.mem_col, [query_vec], filter=filter_expr, limit=top_k,  # æœç´¢æ›´å¤šè®°å¿†ï¼Œé¿å…é—æ¼
            output_fields=["content", "memory_id", "created_at", "user_id"],  # åŒ…å«user_idå­—æ®µç”¨äºè°ƒè¯•
            similarity_threshold=similarity_threshold
        )
        
        # ===========================
        # 2. æœç´¢äº‹å®é›†åˆï¼Œè·å–ç›¸å…³äº‹å®
        # ===========================
        combined_items = []  # å­˜å‚¨memoryå’ŒfactåŠå…¶åˆ†æ•°ï¼Œç”¨äºç»Ÿä¸€æ’åº
        memory_dict = {}  # ä¸´æ—¶å­˜å‚¨memoryå¯¹è±¡
        fact_dict = {}  # ä¸´æ—¶å­˜å‚¨factå¯¹è±¡
        
        # å…ˆå¤„ç†memoryA
        if mem_res and mem_res[0]:
            for hit in mem_res[0]:
                memory = hit['entity']
                memory_id = memory['memory_id']
                # ä¿å­˜åŸå§‹åˆ†æ•°ï¼ˆå†…ç§¯ï¼‰
                memory["original_score"] = hit['distance']
                memory_dict[memory_id] = memory
                # å°†memoryæ·»åŠ åˆ°combined_itemsä¸­ï¼Œç”¨äºç»Ÿä¸€æ’åº
                combined_items.append({
                    "type": "memory",
                    "item": memory,
                    "score": hit['distance'],  # ä½¿ç”¨å†…ç§¯ä½œä¸ºåˆ†æ•°
                    "memory_id": memory_id
                })
        
        # å¤„ç†fact
        if use_fact_retrieval:
            # æœç´¢äº‹å®é›†åˆ
            fact_res = self.client.search(
                self.fact_col, [query_vec], filter=f"user_id == '{user_id}'", limit=top_k,  # æœç´¢æ›´å¤šäº‹å®ï¼Œé¿å…é—æ¼
                output_fields=["text", "timestamp", "fact_id", "details", "linked_memory_ids", "user_id", "embedding"]  # æ·»åŠ embeddingå­—æ®µ
            )
            
            if fact_res and fact_res[0]:
                for hit in fact_res[0]:
                    fact = hit['entity']
                    fact_id = fact['fact_id']
                    # è®¡ç®—factä¸queryçš„å†…ç§¯
                    try:
                        # ç›´æ¥ä½¿ç”¨æ•°æ®åº“ä¸­å­˜å‚¨çš„embeddingï¼Œè€Œä¸æ˜¯é‡æ–°è®¡ç®—
                        fact_vec = fact.get("embedding")
                        if not fact_vec or not isinstance(fact_vec, list):
                            # å¦‚æœæ²¡æœ‰embeddingå­—æ®µæˆ–ä¸æ˜¯åˆ—è¡¨ï¼Œé‡æ–°è®¡ç®—ï¼Œä½¿ç”¨textå’Œdetailsæ‹¼æ¥
                            fact_vec = self._generate_fact_embedding(fact["text"], fact.get("details", []))
                    
                        fact_dot_product = sum(a * b for a, b in zip(query_vec, fact_vec))
                        fact["similarity"] = fact_dot_product
                        fact_dict[fact_id] = fact
                        
                        # å°†factæ·»åŠ åˆ°combined_itemsä¸­ï¼Œç”¨äºç»Ÿä¸€æ’åº
                        combined_items.append({
                            "type": "fact",
                            "item": fact,
                            "score": fact_dot_product,  # ä½¿ç”¨å†…ç§¯ä½œä¸ºåˆ†æ•°
                            "fact_id": fact_id,
                            "linked_memory_ids": fact.get("linked_memory_ids", [])
                        })
                    except Exception as e:
                        print(f"è®¡ç®—äº‹å®ç›¸å…³æ€§å¤±è´¥: {e}")
                        continue
        
        # ===========================
        # 3. ç»Ÿä¸€æ’åºæ‰€æœ‰itemï¼Œå–topk
        # ===========================
        # æŒ‰åˆ†æ•°é™åºæ’åº
        combined_items.sort(key=lambda x: x.get("score", 0), reverse=True)
        
        # å–topk*2ä¸ªitemï¼Œç¡®ä¿åŒ…å«è¶³å¤Ÿå¤šçš„ç›¸å…³ç»“æœ
        top_items = combined_items[:top_k * 2]
        
        # ä»top_itemsä¸­æå–æ‰€æœ‰çš„memory_idï¼Œç¡®ä¿è¿™äº›memoryä¼šè¢«è¿”å›
        top_memory_ids = set()
        # å­˜å‚¨æ¯ä¸ªmemory_idå¯¹åº”çš„æœ€é«˜factåˆ†æ•°
        memory_fact_scores = {}
        # ä¿å­˜æ‰€æœ‰é«˜åº¦ç›¸å…³çš„factåˆ°å­—å…¸ä¸­ï¼Œfact_id -> fact
        top_facts_dict = {}
        # è®°å½•æ¯ä¸ªmemory_idç›´æ¥å…³è”çš„é«˜åº¦ç›¸å…³factï¼Œmemory_id -> [fact]
        memory_direct_facts = {}
        
        for item in top_items:
            if item["type"] == "memory":
                # å¯¹äºmemoryï¼Œç›´æ¥æ·»åŠ å…¶memory_id
                top_memory_ids.add(item["memory_id"])
            elif item["type"] == "fact":
                # ä¿å­˜é«˜åº¦ç›¸å…³çš„factåˆ°å­—å…¸ä¸­
                fact = item["item"]
                fact_id = item["fact_id"]
                top_facts_dict[fact_id] = fact
                
                # å¯¹äºfactï¼Œæå–å…¶å…³è”çš„memory_id
                linked_memory_ids = item.get("linked_memory_ids", [])
                fact_score = item.get("score", 0)
                for mem_id in linked_memory_ids:
                    # è®°å½•æ¯ä¸ªmemory_idå¯¹åº”çš„æœ€é«˜factåˆ†æ•°
                    if mem_id not in memory_fact_scores or fact_score > memory_fact_scores[mem_id]:
                        memory_fact_scores[mem_id] = fact_score
                    
                    # è®°å½•æ¯ä¸ªmemory_idç›´æ¥å…³è”çš„é«˜åº¦ç›¸å…³fact
                    if mem_id not in memory_direct_facts:
                        memory_direct_facts[mem_id] = []
                    memory_direct_facts[mem_id].append(fact)
                    
                    top_memory_ids.add(mem_id)
        
        # æŸ¥è¯¢æ‰€æœ‰top_memory_idså¯¹åº”çš„memoryå¯¹è±¡
        all_memories_dict = {}
        if top_memory_ids:
            # å…ˆå¤„ç†å·²æœ‰çš„memoryï¼ˆmemoryAï¼‰
            for memory_id in top_memory_ids:
                if memory_id in memory_dict:
                    all_memories_dict[memory_id] = memory_dict[memory_id]
            
            # æŸ¥è¯¢å‰©ä½™çš„memory_idå¯¹åº”çš„memoryB
            remaining_memory_ids = top_memory_ids - set(memory_dict.keys())
            if remaining_memory_ids:
                quoted_memory_ids = [f'"{mid}"' for mid in remaining_memory_ids]
                mem_id_filter = f"status == 'active' and user_id == '{user_id}' and memory_id in [{','.join(quoted_memory_ids)}]"
                try:
                    memory_b_res = self.client.query(
                        collection_name=self.mem_col,
                        filter=mem_id_filter,
                        output_fields=["content", "memory_id", "created_at", "user_id"]
                    )
                    
                    # å°†memoryBæ·»åŠ åˆ°all_memories_dictä¸­
                    for mem_b in memory_b_res:
                        memory_id = mem_b['memory_id']
                        # è®¡ç®—memoryBä¸queryçš„å†…ç§¯
                        mem_vec = get_embedding(mem_b["content"])
                        mem_dot_product = sum(a * b for a, b in zip(query_vec, mem_vec))
                        mem_b["original_score"] = mem_dot_product
                        all_memories_dict[memory_id] = mem_b
                except Exception as e:
                    print(f"æŸ¥è¯¢å…³è”è®°å¿†å¤±è´¥: {e}")
        
        # ===========================
        # 4. æ”¶é›†å¹¶æŸ¥è¯¢å…³è”memoryçš„å…³è”memory
        # ===========================
        # ä»é«˜åº¦ç›¸å…³çš„factä¸­æ”¶é›†æ‰€æœ‰å…³è”çš„memory_id
        all_linked_memory_ids = set()
        for fact in top_facts_dict.values():
            linked_memory_ids = fact.get("linked_memory_ids", [])
            all_linked_memory_ids.update(linked_memory_ids)
        
        # æŸ¥è¯¢è¿™äº›å…³è”memoryçš„å…¶ä»–å…³è”memory
        additional_memory_ids = set()
        if all_linked_memory_ids:
            try:
                # æŸ¥è¯¢ä¸è¿™äº›memoryå…³è”çš„æ‰€æœ‰fact
                # æ„å»ºæ­£ç¡®çš„array_containsæ¡ä»¶
                array_contains_conditions = []
                for mid in all_linked_memory_ids:
                    array_contains_conditions.append(f'array_contains(linked_memory_ids, "{mid}")')
                
                # æ„å»ºå®Œæ•´çš„è¿‡æ»¤æ¡ä»¶
                fact_linked_filter = f'user_id == "{user_id}" and ({" or ".join(array_contains_conditions)})'
                linked_facts_res = self.client.query(
                    collection_name=self.fact_col,
                    filter=fact_linked_filter,
                    output_fields=["linked_memory_ids"]
                )
                
                # æ”¶é›†è¿™äº›factå…³è”çš„æ‰€æœ‰memory_idï¼Œä½œä¸ºadditional_memory_ids
                for linked_fact in linked_facts_res:
                    linked_memory_ids = linked_fact.get("linked_memory_ids", [])
                    additional_memory_ids.update(linked_memory_ids)
                
                # ç§»é™¤å·²ç»å­˜åœ¨çš„memory_id
                additional_memory_ids = additional_memory_ids - set(all_memories_dict.keys())
                
                # æŸ¥è¯¢è¿™äº›additional_memory_idså¯¹åº”çš„memory
                if additional_memory_ids:
                    quoted_additional_ids = [f'"{mid}"' for mid in additional_memory_ids]
                    additional_mem_filter = f"status == 'active' and user_id == '{user_id}' and memory_id in [{','.join(quoted_additional_ids)}]"
                    additional_mem_res = self.client.query(
                        collection_name=self.mem_col,
                        filter=additional_mem_filter,
                        output_fields=["content", "memory_id", "created_at", "user_id"]
                    )
                    
                    # å°†additional_memoryæ·»åŠ åˆ°all_memories_dictä¸­
                    for additional_mem in additional_mem_res:
                        memory_id = additional_mem['memory_id']
                        # è®¡ç®—additional_memoryä¸queryçš„å†…ç§¯
                        mem_vec = get_embedding(additional_mem["content"])
                        mem_dot_product = sum(a * b for a, b in zip(query_vec, mem_vec))
                        additional_mem["original_score"] = mem_dot_product
                        all_memories_dict[memory_id] = additional_mem
            except Exception as e:
                print(f"æŸ¥è¯¢å…³è”è®°å¿†çš„å…³è”è®°å¿†å¤±è´¥: {e}")
        
        # ===========================
        # 5. å¤„ç†åˆå¹¶åçš„è®°å¿†
        # ===========================
        results = []
        
        # æ”¶é›†æ‰€æœ‰memoryå…³è”çš„å…¶ä»–memory_id
        additional_memory_ids = set()
        
        for memory_id, memory in all_memories_dict.items():
            # ===========================
            # 5.1 å¤„ç†ç›´æ¥å…³è”çš„é«˜åº¦ç›¸å…³fact
            # ===========================
            direct_facts = memory_direct_facts.get(memory_id, [])
            direct_fact_ids = set()
            
            # ä¿å­˜ç›´æ¥å…³è”çš„é«˜åº¦ç›¸å…³fact
            for fact in direct_facts:
                direct_fact_ids.add(fact.get("fact_id"))
            
            # ===========================
            # 5.2 æŸ¥è¯¢ä¸å½“å‰è®°å¿†å…³è”çš„æ‰€æœ‰å…¶ä»–fact
            # ===========================
            other_facts = []
            try:
                other_facts = self.client.query(
                    collection_name=self.fact_col,
                    filter=f'array_contains(linked_memory_ids, "{memory_id}")',
                    output_fields=["text", "timestamp", "fact_id", "details", "embedding"]
                )
            except Exception as e:
                print(f"æŸ¥è¯¢è®°å¿† {memory_id} çš„å…³è”äº‹å®å¤±è´¥: {e}")
            
            # ===========================
            # 5.3 åˆå¹¶ç›´æ¥å…³è”çš„factå’Œå…¶ä»–factï¼Œé¿å…é‡å¤
            # ===========================
            all_related_facts = []
            all_fact_ids = direct_fact_ids.copy()
            
            # å…ˆæ·»åŠ ç›´æ¥å…³è”çš„é«˜åº¦ç›¸å…³fact
            all_related_facts.extend(direct_facts)
            
            # å†æ·»åŠ å…¶ä»–factï¼Œç¡®ä¿ä¸é‡å¤
            for fact in other_facts:
                fact_id = fact.get("fact_id")
                if fact_id not in all_fact_ids:
                    all_fact_ids.add(fact_id)
                    all_related_facts.append(fact)
            
            # ===========================
            # 5.4 è®¡ç®—æ‰€æœ‰factä¸queryçš„å†…ç§¯å¹¶æ’åº
            # ===========================
            fact_scores = []
            for fact in all_related_facts:
                try:
                    # ç›´æ¥ä½¿ç”¨æ•°æ®åº“ä¸­å­˜å‚¨çš„embeddingï¼Œè€Œä¸æ˜¯é‡æ–°è®¡ç®—
                    fact_vec = fact.get("embedding")
                    if not fact_vec or not isinstance(fact_vec, list):
                        # å¦‚æœæ²¡æœ‰embeddingå­—æ®µæˆ–ä¸æ˜¯åˆ—è¡¨ï¼Œé‡æ–°è®¡ç®—ï¼Œä½¿ç”¨textå’Œdetailsæ‹¼æ¥
                        fact_vec = self._generate_fact_embedding(fact["text"], fact.get("details", []))
                    
                    # è®¡ç®—å†…ç§¯
                    dot_product = sum(a * b for a, b in zip(query_vec, fact_vec))
                    fact["similarity"] = dot_product
                    fact_scores.append(dot_product)
                except Exception as e:
                    print(f"è®¡ç®—äº‹å®ç›¸å…³æ€§å¤±è´¥: {e}")
                    fact["similarity"] = 0
                    fact_scores.append(0)
            
            # æŒ‰å†…ç§¯é™åºæ’åº
            all_related_facts.sort(key=lambda x: x.get("similarity", 0), reverse=True)
            memory["related_facts"] = all_related_facts[:fact_top_k]
            # ä¿å­˜äº‹å®åˆ†æ•°ï¼Œé¿å…é‡å¤è®¡ç®—
            memory["fact_scores"] = fact_scores[:fact_top_k]
            
            # ===========================
            # 5.5 æ”¶é›†å½“å‰memoryå…³è”çš„å…¶ä»–memory
            # ===========================
            # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦ä»factæˆ–å…¶ä»–åœ°æ–¹è·å–å…³è”memory
            # ç›®å‰ç®€å•å®ç°ï¼Œåç»­å¯ä»¥æ‰©å±•
            
            # ===========================
            # 5.6 è®¡ç®—ç»¼åˆåˆ†æ•°
            # ===========================
            original_score = memory.get("original_score", 0)
            highest_fact_score = memory_fact_scores.get(memory_id, 0)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç›´æ¥å…³è”çš„é«˜åº¦ç›¸å…³fact
            has_direct_fact = memory_id in memory_direct_facts and len(memory_direct_facts[memory_id]) > 0
            
            if has_direct_fact:
                # å¦‚æœæœ‰ç›´æ¥å…³è”çš„é«˜åº¦ç›¸å…³factï¼Œç»™äºˆæ›´é«˜çš„æƒé‡
                # ç›´æ¥å…³è”çš„factå¯¹ç»¼åˆåˆ†æ•°å½±å“æ›´å¤§
                memory["combined_score"] = (original_score + highest_fact_score * 1.5) / 2
            else:
                # å¦åˆ™ä½¿ç”¨é»˜è®¤æƒé‡
                memory["combined_score"] = (original_score + highest_fact_score) / 2
            
            # æ ¹æ®é˜ˆå€¼è¿‡æ»¤ç»“æœ
            if memory["combined_score"] >= threshold:
                results.append(memory)
        
        # ===========================
        # 6. æ·»åŠ å…³è”memoryçš„å…³è”memory
        # ===========================
        # æ”¶é›†æ‰€æœ‰memoryå…³è”çš„å…¶ä»–memory
        # ç›®å‰ç®€å•å®ç°ï¼Œåç»­å¯ä»¥æ‰©å±•
        
        # æ ¹æ®ç»¼åˆåˆ†æ•°é‡æ–°æ’åºè®°å¿†
        results.sort(key=lambda x: x.get("combined_score", 0), reverse=True)
        
        # ç¡®ä¿è¿”å›çš„è®°å¿†æ•°é‡ä¸è¶…è¿‡top_k
        return results[:top_k]
        
    def _calculate_memory_score(self, memory, enhanced_search=False):
        """ç›´æ¥è¿”å›memoryä¸queryçš„å†…ç§¯ï¼Œä¸è€ƒè™‘å…³è”äº‹å®çš„ç›¸å…³æ€§"""
        original_score = memory.get("original_score", 0)
        # ç›´æ¥ä½¿ç”¨memoryä¸queryçš„å†…ç§¯ä½œä¸ºç»¼åˆåˆ†æ•°
        memory["combined_score"] = original_score
        return memory
        
    def _generate_fact_embedding(self, text, details):
        """ç”Ÿæˆäº‹å®çš„embeddingï¼Œå°†textå’Œdetailsæ‹¼æ¥èµ·æ¥
        
        Args:
            text: äº‹å®çš„æ–‡æœ¬
            details: äº‹å®çš„è¯¦ç»†ä¿¡æ¯ï¼Œç±»å‹ä¸ºåˆ—è¡¨
            
        Returns:
            ç”Ÿæˆçš„embeddingå‘é‡
        """
        # å°†detailsæ‹¼æ¥æˆå­—ç¬¦ä¸²
        details_str = ""
        if isinstance(details, list) and details:
            # éå†detailsåˆ—è¡¨ï¼Œå°†æ¯ä¸ªdetailsé¡¹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            for i, detail in enumerate(details):
                if isinstance(detail, dict):
                    # å¦‚æœdetailæ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸ºé”®å€¼å¯¹å­—ç¬¦ä¸²
                    detail_str = ", ".join([f"{k}: {v}" for k, v in detail.items()])
                    details_str += f"Detail {i+1}: {detail_str}\n"
                else:
                    # å¦åˆ™ç›´æ¥è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    details_str += f"Detail {i+1}: {str(detail)}\n"
        elif isinstance(details, dict):
            # å¦‚æœdetailsæ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸ºé”®å€¼å¯¹å­—ç¬¦ä¸²
            details_str = ", ".join([f"{k}: {v}" for k, v in details.items()])
        
        # å°†textå’Œdetailsæ‹¼æ¥æˆå®Œæ•´çš„æ–‡æœ¬
        if details_str:
            full_text = f"{text}\n\nDetails:\n{details_str.strip()}"
        else:
            full_text = text
        
        # ç”Ÿæˆembedding
        return get_embedding(full_text)
        
    def generate_response(self, question, question_date, context):
        """ç”Ÿæˆé—®é¢˜å“åº”"""
        prompt = LME_ANSWER_PROMPT.format(
            question=question,
            question_date=question_date,
            context=context
        )
        response = llm_client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "system", "content": prompt}],
                    temperature=0,
                )
        
        return response

# ==========================================
# è¯„ä¼°ç›¸å…³å‡½æ•°
# ==========================================
def response_user(line, pipeline, retrieve_limit=20, max_facts_per_memory=3, user_id='default', threshold: float = 0.0, enhanced_search: bool = False):
    """å¤„ç†ç”¨æˆ·é—®é¢˜ï¼Œç”Ÿæˆå“åº”
    
    Args:
        line: åŒ…å«é—®é¢˜å’Œå…¶ä»–ä¿¡æ¯çš„å­—å…¸
        pipeline: MemoryPipelineå®ä¾‹
        retrieve_limit: æ£€ç´¢è®°å¿†çš„æ•°é‡é™åˆ¶
        max_facts_per_memory: æ¯ä¸ªè®°å¿†çš„äº‹å®æ•°é‡é™åˆ¶
        user_id: ç”¨æˆ·æ ‡è¯†ï¼Œç¡®ä¿åªæ£€ç´¢å½“å‰ç”¨æˆ·çš„è®°å¿†
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºè¯¥é˜ˆå€¼çš„è®°å¿†å°†è¢«è¿‡æ»¤æ‰
        enhanced_search: æ˜¯å¦å¯ç”¨å¢å¼ºå‹æœç´¢æ¨¡å¼ï¼Œå¯ç”¨åä¼šè°ƒå¤§topkå¹¶å¢å¼ºrerank
    """
    question = line.get("question")
    question_date = line.get("question_date")
    question_date = question_date + " UTC"
    question_date_format = "%Y/%m/%d (%a) %H:%M UTC"
    question_date_string = datetime.strptime(question_date, question_date_format).replace(tzinfo=timezone.utc)
    
    # å¢å¼ºå‹æœç´¢æ¨¡å¼ï¼šè°ƒå¤§topk
    if enhanced_search:
        # è°ƒå¤§åˆå§‹æ£€ç´¢æ•°é‡ï¼Œä¾‹å¦‚ä¹˜ä»¥2
        enhanced_top_k = retrieve_limit * 2
        print(f"   ğŸš€ å¯ç”¨å¢å¼ºå‹æœç´¢æ¨¡å¼ï¼Œåˆå§‹æ£€ç´¢æ•°é‡: {enhanced_top_k}")
    else:
        enhanced_top_k = retrieve_limit
    
    # æœç´¢è®°å¿†ï¼Œä¼ é€’user_idã€thresholdå’Œenhanced_searchå‚æ•°
    retrieved_memories = pipeline.search_memories(question, top_k=enhanced_top_k, user_id=user_id, threshold=threshold, enhanced_search=enhanced_search)
    
    # ç¡®ä¿retrieved_memoriesä¸æ˜¯None
    retrieved_memories = retrieved_memories or []
    
    # æ„å»ºä¸Šä¸‹æ–‡ï¼ŒåŒ…å«è®°å¿†å’Œå…³è”çš„äº‹å®
    memories_with_facts = []
    
    for mem in retrieved_memories:
        # æ·»åŠ è®°å¿†å†…å®¹
        memory_line = f"- [{datetime.fromtimestamp(mem['created_at'], timezone.utc).isoformat()}] {mem['content']}"
        memories_with_facts.append(memory_line)
        
        # æ·»åŠ å…³è”çš„äº‹å®ï¼ˆå¦‚æœæœ‰ï¼‰
        related_facts = mem.get("related_facts", [])
        if related_facts:
            # ç›´æ¥ä½¿ç”¨search_memoriesä¸­å·²ç»è®¡ç®—å¥½çš„ç›¸ä¼¼åº¦åˆ†æ•°
            fact_with_scores = []
            for fact in related_facts:
                # è·å–å·²è®¡ç®—çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œé»˜è®¤ä¸º0
                similarity = fact.get("similarity", 0)
                fact_with_scores.append((fact, similarity))
            
            # æ ¹æ®ç›¸å…³æ€§åˆ†æ•°å¯¹äº‹å®è¿›è¡Œæ’åº
            fact_with_scores.sort(key=lambda x: x[1], reverse=True)
            
            # æ·»åŠ æ’åºåçš„äº‹å®ï¼Œé™åˆ¶æ•°é‡
            for i, (fact, score) in enumerate(fact_with_scores[:max_facts_per_memory]):
                # ä¼˜åŒ–äº‹å®è¾“å‡ºæ ¼å¼
                fact_text = fact['text']
                details = fact['details']
                
                # æ ¼å¼åŒ–ç»†èŠ‚
                if details:
                    # å°†ç»†èŠ‚åˆ—è¡¨è½¬æ¢ä¸ºæ›´æ˜“è¯»çš„æ ¼å¼
                    details_str = "; ".join(details)
                    # å¦‚æœç»†èŠ‚å¤ªé•¿ï¼Œæˆªæ–­
                    if len(details_str) > 100:
                        details_str = details_str[:97] + "..."
                    fact_line = f"  â”œâ”€â”€ [{i+1}] äº‹å®: {fact_text}\n  â”‚     ç»†èŠ‚: {details_str}"
                else:
                    fact_line = f"  â”œâ”€â”€ [{i+1}] äº‹å®: {fact_text}"
                
                memories_with_facts.append(fact_line)
    
    memories_str = "\n".join(memories_with_facts)
    
    # ç”Ÿæˆå“åº”
    response = pipeline.generate_response(question, question_date_string, memories_str)
    answer = response.choices[0].message.content
    
    return retrieved_memories, answer

def process_and_evaluate_user(line, user_index, infer=True, retrieve_limit: int = 3, extract_mode: str = "whole", vector_db_type="milvus", dataset_name=""):
    """
    å°è£…å•ä¸ªç”¨æˆ·çš„æ‰€æœ‰å¤„ç†æ­¥éª¤ï¼Œä»¥ä¾¿å¹¶è¡Œæ‰§è¡Œã€‚
    è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸ã€‚
    """
    try:
        # ä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆå”¯ä¸€çš„user_idï¼Œç¡®ä¿è®°å¿†éš”ç¦»
        user_id = f"user_{user_index}"
        
        # ä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºç‹¬ç«‹çš„pipelineå®ä¾‹ï¼Œé¿å…å¤šçº¿ç¨‹ç«äº‰
        # æ³¨æ„ï¼šæ¯ä¸ªç”¨æˆ·çš„pipelineå®ä¾‹ä¸åº”è¯¥æ¸…ç©ºæ•°æ®åº“ï¼Œclear_dbå›ºå®šä¸ºFalse
        pipeline = MemoryPipeline(vector_db_type=vector_db_type, clear_db=False, dataset_name=dataset_name)
        
        # å¤„ç†ç”¨æˆ·è®°å¿†ä¼šè¯ï¼Œä¼ é€’user_id
        memory_counts = pipeline.process_user_memory_infer(line, retrieve_limit=retrieve_limit, extract_mode=extract_mode, user_id=user_id)
        
        # ç”Ÿæˆé—®é¢˜å“åº”ï¼Œä¼ é€’user_id
        retrieved_memories, answer = response_user(line, pipeline, retrieve_limit, user_id=user_id)
        
        # ç¡®ä¿retrieved_memoriesä¸æ˜¯None
        retrieved_memories = retrieved_memories or []
        
        # æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²ç”¨äºåç»­å¤„ç†
        memories_with_facts = []
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼Œç”¨äºè®¡ç®—äº‹å®ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§
        query_vec = get_embedding(line.get("question", ""))
        
        for mem in retrieved_memories:
            # æ·»åŠ è®°å¿†å†…å®¹
            memory_line = f"- [{datetime.fromtimestamp(mem['created_at'], timezone.utc).isoformat()}] {mem['content']}"
            memories_with_facts.append(memory_line)

            # print("#"*50)
            # print("mem:\n", mem)
            # print("#"*50)
            
            # æ·»åŠ å…³è”çš„äº‹å®ï¼ˆå¦‚æœæœ‰ï¼‰
            related_facts = mem.get("related_facts", [])
            max_facts_per_memory = 3  # æ¯ä¸ªè®°å¿†çš„äº‹å®æ•°é‡é™åˆ¶
            if related_facts:
                # è®¡ç®—æ¯ä¸ªäº‹å®ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§åˆ†æ•°
                fact_with_scores = []
                for fact in related_facts:
                    try:
                        fact_vec = get_embedding(fact["text"])
                        # ä½¿ç”¨å‘é‡ç‚¹ç§¯ä½œä¸ºç›¸å…³æ€§åˆ†æ•°
                        dot_product = sum(a * b for a, b in zip(query_vec, fact_vec))
                        fact_with_scores.append((fact, dot_product))
                    except Exception as e:
                        print(f"è®¡ç®—äº‹å®ç›¸å…³æ€§å¤±è´¥: {e}")
                        fact_with_scores.append((fact, 0))
                
                # æ ¹æ®ç›¸å…³æ€§åˆ†æ•°å¯¹äº‹å®è¿›è¡Œæ’åº
                # fact_with_scores.sort(key=lambda x: x[1], reverse=True)
                
                
                # æ·»åŠ æ’åºåçš„äº‹å®ï¼Œé™åˆ¶æ•°é‡
                for i, (fact, score) in enumerate(fact_with_scores[:max_facts_per_memory]):
                    # ä¼˜åŒ–äº‹å®è¾“å‡ºæ ¼å¼
                    fact_text = fact['text']
                    details = fact['details']
                    
                    # æ ¼å¼åŒ–ç»†èŠ‚
                    if details:
                        # å°†ç»†èŠ‚åˆ—è¡¨è½¬æ¢ä¸ºæ›´æ˜“è¯»çš„æ ¼å¼
                        details_str = "; ".join(details)
                        # å¦‚æœç»†èŠ‚å¤ªé•¿ï¼Œæˆªæ–­
                        if len(details_str) > 100:
                            details_str = details_str[:97] + "..."
                        fact_line = f"  â”œâ”€â”€ [{i+1}] äº‹å®: {fact_text}\n  â”‚     ç»†èŠ‚: {details_str}"
                    else:
                        fact_line = f"  â”œâ”€â”€ [{i+1}] äº‹å®: {fact_text}"
                    
                    memories_with_facts.append(fact_line)
        
        memories_str = "\n".join(memories_with_facts)
        
        # è·å–æ ‡å‡†ç­”æ¡ˆå’Œé—®é¢˜ç±»å‹
        golden_answer = line.get("answer")
        question = line.get("question")
        question_type = line.get("question_type", "unknown")
        
        # è¯„ä¼°ç­”æ¡ˆæ­£ç¡®æ€§
        is_correct = lme_grader(llm_client, question, golden_answer, answer)
        
        return {
            "index": user_index,
            "is_correct": is_correct,
            "counts": memory_counts,
            "question": question,
            "question_type": question_type,
            "answer": answer,
            "golden_answer": golden_answer,
            "retrieved_memories": retrieved_memories,
            "context": memories_str,
        }
    except Exception as e:
        print(f"å¤„ç†ç”¨æˆ· {user_index} å‡ºé”™ ({line.get('question', 'Unknown')[:20]}...): {e}")
        return {
            "index": user_index,
            "is_correct": False,
            "counts": {"ADD": 0, "UPDATE": 0, "DELETE": 0, "INFER": 0, "NOOP": 0},
            "question": line.get("question", "N/A"),
            "question_type": line.get("question_type", "unknown"),
            "context": "N/A",
            "answer": "N/A",
            "golden_answer": line.get("answer", "N/A"),
            "retrieved_memories": []
        }

# ==========================================
# Main Test & Evaluation
# ==========================================
if __name__ == "__main__":
    import argparse
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="Memory Pipeline with longmemeval Evaluation")
    parser.add_argument("--eval", action="store_true", help="æ˜¯å¦è¿›è¡Œè¯„ä¼°")
    parser.add_argument("--infer", action="store_true", default=True, help="æ˜¯å¦ä½¿ç”¨æ¨ç†åŠŸèƒ½")
    parser.add_argument("--num_users", type=int, default=50, help="è¯„ä¼°ç”¨æˆ·æ•°é‡")
    parser.add_argument("--max_workers", type=int, default=10, help="å¹¶è¡Œå¤„ç†çš„å·¥ä½œçº¿ç¨‹æ•°")
    parser.add_argument("--retrieve_limit", type=int, default=3, help="æ£€ç´¢æ—¶è¿”å›çš„è®°å¿†æ•°é‡")
    parser.add_argument("--threshold", type=float, default=0.7, help="è®°å¿†ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºè¯¥é˜ˆå€¼çš„è®°å¿†å°†è¢«è¿‡æ»¤æ‰")
    parser.add_argument("--extract-mode", type=str, default="whole", choices=["whole", "turn"], help="æå–æ¨¡å¼ï¼šwhole-å¯¹æ•´ä¸ªchunkè¿›è¡Œæå–ï¼Œturn-æŒ‰è½®æ¬¡æå–")
    parser.add_argument("--vector-db-type", type=str, default="milvus", choices=["milvus", "qdrant"], help="æŒ‡å®šä½¿ç”¨çš„å‘é‡æ•°æ®åº“ç±»å‹")
    parser.add_argument("--clear-db", action="store_true", help="è¿è¡Œå‰æ¸…ç©ºæ•°æ®åº“")
    parser.add_argument("--data-path", type=str, help="æŒ‡å®šæ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--dataset-type", type=str, default="longmemeval", choices=["longmemeval", "hotpotqa"], help="æŒ‡å®šæ•°æ®é›†ç±»å‹")
    args = parser.parse_args()
    
    # åˆå§‹åŒ–å†…å­˜ç®¡é“
    pipeline = MemoryPipeline(vector_db_type=args.vector_db_type, clear_db=args.clear_db, mode='eval' if args.eval else 'test', dataset_name=args.dataset_type)
    
    if args.eval:
        # è¯„ä¼°æ¨¡å¼
        try:
            # æ ¹æ®æ•°æ®é›†ç±»å‹è®¾ç½®é»˜è®¤æ•°æ®è·¯å¾„
            if args.dataset_type == "hotpotqa":
                data_path = args.data_path or "./data/hotpotqa-val.jsonl"
            else:  # longmemeval
                data_path = args.data_path or "./data/longmemeval_s_cleaned.json"
                
            print(f"è°ƒè¯•ä¿¡æ¯ï¼š")
            print(f"  æ•°æ®é›†ç±»å‹ï¼š{args.dataset_type}")
            print(f"  æŒ‡å®šçš„æ•°æ®è·¯å¾„ï¼š{args.data_path}")
            print(f"  å®é™…ä½¿ç”¨çš„æ•°æ®è·¯å¾„ï¼š{data_path}")
            print(f"  æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼š{os.path.exists(data_path)}")
            
            if not os.path.exists(data_path):
                print(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
                exit()
            
            # åˆ¤æ–­æ–‡ä»¶ç±»å‹å¹¶åŠ è½½æ•°æ®
            lines = []
            print(f"  æ–‡ä»¶æ ¼å¼ï¼š{'JSONL' if data_path.endswith('.jsonl') else 'JSON'}")
            if data_path.endswith(".jsonl"):
                # å¤„ç†JSONLæ ¼å¼æ–‡ä»¶
                print(f"  å¼€å§‹åŠ è½½JSONLæ–‡ä»¶...")
                with open(data_path, "r") as f:
                    for i, line in enumerate(f):
                        lines.append(json.loads(line.strip()))
                        if i < 2:  # æ‰“å°å‰2æ¡æ•°æ®çš„å…³é”®å­—æ®µ
                            loaded_item = lines[-1]
                            print(f"    ç¬¬{i+1}æ¡æ•°æ®å…³é”®å­—æ®µï¼š")
                            print(f"      æ˜¯å¦åŒ…å«contextï¼š{'context' in loaded_item}")
                            print(f"      æ˜¯å¦åŒ…å«haystack_datesï¼š{'haystack_dates' in loaded_item}")
                            print(f"      æ•°æ®IDï¼š{loaded_item.get('id', 'æœªçŸ¥')}")
            else:
                # å¤„ç†JSONæ ¼å¼æ–‡ä»¶
                print(f"  å¼€å§‹åŠ è½½JSONæ–‡ä»¶...")
                with open(data_path, "r") as f:
                    lines = json.load(f)
                    if lines and len(lines) > 0:
                        print(f"    å…±åŠ è½½ {len(lines)} æ¡æ•°æ®")
                        if len(lines) > 0:
                            loaded_item = lines[0]
                            print(f"    ç¬¬1æ¡æ•°æ®å…³é”®å­—æ®µï¼š")
                            print(f"      æ˜¯å¦åŒ…å«contextï¼š{'context' in loaded_item}")
                            print(f"      æ˜¯å¦åŒ…å«haystack_datesï¼š{'haystack_dates' in loaded_item}")
                            print(f"      æ•°æ®IDï¼š{loaded_item.get('id', 'æœªçŸ¥')}")
                    
            # å¦‚æœnum_usersä¸º-1ï¼ŒåŠ è½½æ‰€æœ‰æ•°æ®ï¼›å¦åˆ™åŠ è½½æŒ‡å®šæ•°é‡
            if args.num_users != -1:
                lines = lines[:args.num_users]
            
            # æ•°æ®é›†æ ¼å¼åˆ¤æ–­å’Œè½¬æ¢
            def is_valid_format(line):
                """åˆ¤æ–­æ•°æ®æ¡ç›®æ˜¯å¦ç¬¦åˆlongmemevalæ ¼å¼è¦æ±‚"""
                return "haystack_dates" in line and "haystack_sessions" in line
            
            def convert_hotpotqa_to_expected_format(hotpotqa_item):
                """å°†HotpotQAæ¡ç›®è½¬æ¢ä¸ºé¢„æœŸæ ¼å¼"""
                # ç”Ÿæˆå›ºå®šæ ¼å¼çš„æ—¥æœŸ
                date = "2023/01/01 (Sun) 12:00"
                
                # æ„å»ºç³»ç»Ÿæ¶ˆæ¯ï¼ŒåŒ…å«æ‰€æœ‰èƒŒæ™¯çŸ¥è¯†
                context = hotpotqa_item["context"]
                system_content = "ä»¥ä¸‹æ˜¯èƒŒæ™¯çŸ¥è¯†ï¼š\n"
                for title, sentences in zip(context["title"], context["sentences"]):
                    system_content += f"\n{title}:\n"
                    for sentence in sentences:
                        system_content += f"- {sentence}\n"
                
                # æ„å»ºç”¨æˆ·æ¶ˆæ¯ï¼ŒåŒ…å«é—®é¢˜
                user_content = hotpotqa_item["question"]
                
                # æ„å»ºä¼šè¯ç»“æ„
                session = [
                    {"role": "system", "content": system_content.strip()},
                    {"role": "user", "content": user_content}
                ]
                
                # è¿”å›è½¬æ¢åçš„æ ¼å¼ï¼ŒåŒ…å«question_typeå­—æ®µ
                return {
                    "haystack_dates": [date],
                    "haystack_sessions": [session],
                    "id": hotpotqa_item["id"],
                    "answer": hotpotqa_item["answer"],
                    "question_type": hotpotqa_item.get("type", "unknown")  # ä½¿ç”¨hotpotqaçš„typeå­—æ®µä½œä¸ºquestion_type
                }
            
            # å¦‚æœæ˜¯hotpotqaæ•°æ®é›†ï¼Œæ£€æŸ¥æ ¼å¼å¹¶è½¬æ¢
            if args.dataset_type == "hotpotqa":
                # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ¡ç›®æ˜¯å¦ç¬¦åˆæ ¼å¼è¦æ±‚
                if lines and not is_valid_format(lines[0]):
                    print(f"HotpotQAæ•°æ®é›†æ ¼å¼ä¸ç¬¦åˆè¦æ±‚ï¼Œæ­£åœ¨è½¬æ¢ {len(lines)} ä¸ªæ¡ç›®...")
                    # è½¬æ¢æ‰€æœ‰æ¡ç›®
                    converted_lines = []
                    for i, item in enumerate(lines):
                        if i % 100 == 0:  # æ¯å¤„ç†100æ¡æ‰“å°ä¸€æ¬¡è¿›åº¦
                            print(f"å·²è½¬æ¢ {i}/{len(lines)} æ¡æ•°æ®")
                        converted_lines.append(convert_hotpotqa_to_expected_format(item))
                    lines = converted_lines
                    print(f"è½¬æ¢å®Œæˆï¼Œå…±è½¬æ¢ {len(lines)} ä¸ªæ¡ç›®")
                else:
                    print("HotpotQAæ•°æ®é›†æ ¼å¼ç¬¦åˆè¦æ±‚ï¼Œæ­£åœ¨æ£€æŸ¥å¹¶ç¡®ä¿æ‰€æœ‰æ¡ç›®åŒ…å«question_typeå­—æ®µ...")
                    # ç¡®ä¿æ‰€æœ‰æ¡ç›®éƒ½åŒ…å«question_typeå­—æ®µ
                    for i, line in enumerate(lines):
                        if "question_type" not in line:
                            # å°è¯•ä»åŸå§‹æ•°æ®ä¸­è·å–typeå­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
                            lines[i]["question_type"] = line.get("type", "unknown")
                    print("æ£€æŸ¥å®Œæˆï¼Œæ‰€æœ‰æ¡ç›®éƒ½åŒ…å«question_typeå­—æ®µ")
            
            print(f"å·²åŠ è½½ {len(lines)} ä¸ªç”¨æˆ·/é—®é¢˜ã€‚")

            user_detail_results = []
            total_memory_counts = {"ADD": 0, "UPDATE": 0, "DELETE": 0, "INFER": 0, "NOOP": 0}
            
            # å¹¶è¡Œå¤„ç†ç”¨æˆ·
            with ThreadPoolExecutor(max_workers=args.max_workers) as executor:
                # æäº¤ä»»åŠ¡ - ç¡®ä¿å‚æ•°é¡ºåºæ­£ç¡®ï¼šline, idx, args.infer, args.retrieve_limit, args.extract_mode, args.vector_db_type, args.dataset_type
                # æ³¨æ„ï¼šè¿™é‡Œclear_dbå›ºå®šä¸ºFalseï¼Œåªåœ¨ä¸»å‡½æ•°ä¸­æ‰§è¡Œä¸€æ¬¡æ¸…ç©ºæ“ä½œ
                future_to_user = {executor.submit(process_and_evaluate_user, line, idx, args.infer, args.retrieve_limit, args.extract_mode, args.vector_db_type, args.dataset_type): (line, idx) for idx, line in enumerate(lines)}
                
                # å¤„ç†ç»“æœ
                for future in tqdm(as_completed(future_to_user), total=len(future_to_user)):
                    line, idx = future_to_user[future]
                    try:
                        result = future.result()
                        user_detail_results.append(result)
                        
                        # ç»Ÿè®¡æ€»æ“ä½œæ¬¡æ•°
                        for key, value in result["counts"].items():
                            total_memory_counts[key] += value
                    except Exception as e:
                        print(f"å¤„ç†ç”¨æˆ· {idx} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            
            # è®¡ç®—æ€»å‡†ç¡®ç‡
            correct_count = sum(1 for result in user_detail_results if result["is_correct"])
            accuracy = correct_count / len(user_detail_results) * 100 if user_detail_results else 0
            
            # æŒ‰question_typeç»Ÿè®¡æ¯ç±»é—®é¢˜çš„å‡†ç¡®ç‡
            question_type_stats = {}
            for result in user_detail_results:
                q_type = result.get("question_type", "unknown")
                if q_type not in question_type_stats:
                    question_type_stats[q_type] = {"total": 0, "correct": 0}
                question_type_stats[q_type]["total"] += 1
                if result["is_correct"]:
                    question_type_stats[q_type]["correct"] += 1
            
            # è¾“å‡ºè¯„ä¼°ç»“æœ
            print("\n" + "="*50)
            print(f"{args.dataset_type} è¯„ä¼°ç»“æœ")
            print("="*50)
            print(f"æ€»ç”¨æˆ·æ•°: {len(user_detail_results)}")
            print(f"æ­£ç¡®å›ç­”æ•°: {correct_count}")
            print(f"æ€»å‡†ç¡®ç‡: {accuracy:.2f}%")
            print(f"è®°å¿†æ“ä½œæ€»æ•°:")
            for op, count in total_memory_counts.items():
                print(f"  {op}: {count}")
            
            # è¾“å‡ºæŒ‰question_typeåˆ†ç±»çš„å‡†ç¡®ç‡
            print("\n" + "="*50)
            print("æŒ‰é—®é¢˜ç±»å‹åˆ†ç±»çš„å‡†ç¡®ç‡")
            print("="*50)
            for q_type, stats in question_type_stats.items():
                type_accuracy = stats["correct"] / stats["total"] * 100 if stats["total"] > 0 else 0
                print(f"{q_type}: {stats['correct']}/{stats['total']} ({type_accuracy:.2f}%)")
            
            print("="*50)
            
            # è¾“å‡ºè¯¦ç»†ç»“æœ
            print("\nè¯¦ç»†ç»“æœ:")
            for result in user_detail_results:  
                print(f"ç”¨æˆ· {result['index']}: {'âœ“' if result['is_correct'] else 'âœ—'}")
                print(f"  é—®é¢˜: {result['question']}")
                print(f"  é—®é¢˜ç±»å‹: {result.get('question_type', 'unknown')}")
                print(f"  ä¸Šä¸‹æ–‡: {result['context']}")
                print(f"  å›ç­”: {result['answer']}...")
                print(f"  æ ‡å‡†ç­”æ¡ˆ: {result['golden_answer']}...")
                print(f"  è®°å¿†æ“ä½œ: {result['counts']}")
                print()
                
        except Exception as e:
            print(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
    else:
        try:
            # æµ‹è¯•æ¨¡å¼
            # æ ¹æ®æ•°æ®é›†ç±»å‹è®¾ç½®é»˜è®¤æ•°æ®è·¯å¾„
            if args.dataset_type == "hotpotqa":
                data_path = args.data_path or "./data/hotpotqa-val.jsonl"
            else:  # longmemeval
                data_path = args.data_path or "./data/longmemeval_s_cleaned_test.json"
                
            if not os.path.exists(data_path):
                print(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
                exit()
            
            # åˆ¤æ–­æ–‡ä»¶ç±»å‹å¹¶åŠ è½½æ•°æ®
            lines = []
            if data_path.endswith(".jsonl"):
                # å¤„ç†JSONLæ ¼å¼æ–‡ä»¶
                with open(data_path, "r") as f:
                    for line in f:
                        lines.append(json.loads(line.strip()))
            else:
                # å¤„ç†JSONæ ¼å¼æ–‡ä»¶
                with open(data_path, "r") as f:
                    lines = json.load(f)
                    
            if args.num_users != -1:
                lines = lines[:args.num_users]
            
            print(f"å·²åŠ è½½ {len(lines)} ä¸ªç”¨æˆ·/é—®é¢˜ã€‚")

            user_detail_results = []
            total_memory_counts = {"ADD": 0, "UPDATE": 0, "DELETE": 0, "INFER": 0, "NOOP": 0}
            
            # å¹¶è¡Œå¤„ç†ç”¨æˆ·
            with ThreadPoolExecutor(max_workers=args.max_workers) as executor:
                # æäº¤ä»»åŠ¡ï¼ŒåŒ…å«extract_modeå’Œdataset_typeå‚æ•°
                future_to_user = {executor.submit(process_and_evaluate_user, line, idx, args.infer, args.retrieve_limit, args.extract_mode, args.vector_db_type, args.dataset_type): (line, idx) for idx, line in enumerate(lines)}
                
                # å¤„ç†ç»“æœ
                for future in tqdm(as_completed(future_to_user), total=len(future_to_user)):
                    line, idx = future_to_user[future]
                    try:
                        result = future.result()
                        user_detail_results.append(result)
                        
                        # ç»Ÿè®¡æ€»æ“ä½œæ¬¡æ•°
                        for key, value in result["counts"].items():
                            total_memory_counts[key] += value
                    except Exception as e:
                        print(f"å¤„ç†ç”¨æˆ· {idx} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            
            # è®¡ç®—æ€»å‡†ç¡®ç‡
            correct_count = sum(1 for result in user_detail_results if result["is_correct"])
            accuracy = correct_count / len(user_detail_results) * 100 if user_detail_results else 0
            
            # æŒ‰question_typeç»Ÿè®¡æ¯ç±»é—®é¢˜çš„å‡†ç¡®ç‡
            question_type_stats = {}
            for result in user_detail_results:
                q_type = result.get("question_type", "unknown")
                if q_type not in question_type_stats:
                    question_type_stats[q_type] = {"total": 0, "correct": 0}
                question_type_stats[q_type]["total"] += 1
                if result["is_correct"]:
                    question_type_stats[q_type]["correct"] += 1
            
            # è¾“å‡ºè¯„ä¼°ç»“æœ
            print("\n" + "="*50)
            print("LongMemEval è¯„ä¼°ç»“æœ")
            print("="*50)
            print(f"æ€»ç”¨æˆ·æ•°: {len(user_detail_results)}")
            print(f"æ­£ç¡®å›ç­”æ•°: {correct_count}")
            print(f"æ€»å‡†ç¡®ç‡: {accuracy:.2f}%")
            print(f"è®°å¿†æ“ä½œæ€»æ•°:")
            for op, count in total_memory_counts.items():
                print(f"  {op}: {count}")
            
            # è¾“å‡ºæŒ‰question_typeåˆ†ç±»çš„å‡†ç¡®ç‡
            print("\n" + "="*50)
            print("æŒ‰é—®é¢˜ç±»å‹åˆ†ç±»çš„å‡†ç¡®ç‡")
            print("="*50)
            for q_type, stats in question_type_stats.items():
                type_accuracy = stats["correct"] / stats["total"] * 100 if stats["total"] > 0 else 0
                print(f"{q_type}: {stats['correct']}/{stats['total']} ({type_accuracy:.2f}%)")
            
            print("="*50)
            
            # è¾“å‡ºè¯¦ç»†ç»“æœ
            print("\nè¯¦ç»†ç»“æœ:")
            for result in user_detail_results:  
                print(f"ç”¨æˆ· {result['index']}: {'âœ“' if result['is_correct'] else 'âœ—'}")
                print(f"  é—®é¢˜ç±»å‹: {result.get('question_type', 'unknown')}")
                print(f"  é—®é¢˜: {result['question']}")
                print(f"  ä¸Šä¸‹æ–‡: {result['context']}")
                print(f"  å›ç­”: {result['answer']}...")
                print(f"  æ ‡å‡†ç­”æ¡ˆ: {result['golden_answer']}...")
                print(f"  è®°å¿†æ“ä½œ: {result['counts']}")
                print()
                
        except Exception as e:
            print(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()