import os
import time
import uuid
import json
import numpy as np
from typing import List, Dict, Optional, Any, Union
from dataclasses import dataclass
from dotenv import load_dotenv
from openai import OpenAI
from utils import (MEMREADER_PROMPT, 
                   get_embedding, parse_messages, LME_JUDGE_MODEL_TEMPLATE, 
                   LME_ANSWER_PROMPT, remove_code_blocks, extract_json)
from lme_eval import lme_grader
from datetime import datetime, timezone
import pytz
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from vector_db import VectorDBConfig, VectorDBFactory, QdrantDB
# ==========================================
# 0. Setup & Prompts
# ==========================================
load_dotenv()

# âš ï¸ è¯·ç¡®ä¿ç¯å¢ƒå˜é‡ä¸­æœ‰ OPENAI_API_KEY å’Œ MILVUS_URI
# å¦‚æœæ˜¯æœ¬åœ°æµ‹è¯•ï¼Œç¡®ä¿ Docker ä¸­ Milvus å·²å¯åŠ¨

# Select provider: "openai" or "gemini"
# LLM_PROVIDER = os.getenv("LLM_PROVIDER", "openai")

# Select LLM mode: "local" or "online"
LLM_MODE = os.getenv("LLM_MODE", "online")

if LLM_MODE == "local":
    # Initialize client for Local LLM
    llm_client = OpenAI(
        api_key=os.getenv("LOCAL_LLM_API_KEY", ""), 
        base_url=os.getenv("LOCAL_LLM_BASE_URL")
    )
    print(f"ğŸš€ Using Local LLM for generation: {os.getenv('LOCAL_LLM_BASE_URL')}")
else:
    # Initialize client for OpenAI (Online)
    llm_client = OpenAI(
        api_key=os.getenv("OPENAI_API_KEY"), 
        base_url=os.getenv("OPENAI_BASE_URL")
    )
    print("ğŸŒ Using OpenAI for generation.")

# Initialize a separate client for embeddings (always OpenAI)
embedding_client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY")
)

# Always use text-embedding-3-small as requested
EMBEDDING_MODEL = "text-embedding-3-small"


MEMORY_MANAGER_PROMPT = """You are a specialized Memory Manager Agent.
Your role is to maintain the consistency and growth of a memory graph using the provided tools.

[INPUTS]
You will receive:
1. "New Facts": A list of atomic facts extracted from the latest user input.
2. "Existing Memories": A list of retrieved memory items, each with a simplified Integer ID (e.g., "0", "1", "2").
   - These memories include those directly related to the new facts, as well as other related facts connected with these memories.
   - They form a connected graph of information relevant to the new facts.

[MANDATORY OUTPUT FORMAT]
For every new fact you process, you MUST:
1. First generate a detailed thinking process
2. Then call the appropriate tool

[THINKING PROCESS REQUIREMENTS]
Your thinking process MUST include:
- The specific new fact you're analyzing
- Which existing memories are relevant (with their IDs)
- How memories are connected through related facts
- Your comparison and reasoning
- Which operation you've decided to perform and why

[OPERATIONS & GUIDELINES]
Compare New Facts with Existing Memories and perform the following operations using the available tools. 
DO NOT output raw JSON text. You MUST use the provided function tools.

1. **ADD (create_memory)**
   - **Condition**: If a fact contains completely NEW information not present in Existing Memories.
   - **Action**: Call `create_memory` with a concise summary of the facts, not just a simple concatenation.
   - **Important**: Memory content should be a meaningful and concise summary.
  
2. **UPDATE (update_memory)**
   - **Condition**: If a fact adds detail, corrects, or updates a specific Existing Memory.
   - **Constraint**: You MUST use the Integer ID (e.g., "0") provided in the input as the `target_memory_id`.
   - **Logic**: Merge the old content and new fact into a comprehensive statement, not just a simple concatenation.
   - **Example**:
     - Old (ID="0"): "User likes generic pizza."
     - New Fact: "User loves pepperoni pizza."
     - Action: `update_memory(target_memory_id="0", new_content="User loves pepperoni pizza", ...)`

3. **DELETE (delete_memory)**
   - **Condition**: If a fact explicitly contradicts an Existing Memory (and the new fact is trusted), or if the memory is no longer valid.
   - **Constraint**: Use the Integer ID (e.g., "1") as `target_memory_id`.

4. **INFER (infer_memory)**
   - **Condition**: Look for higher-level insights. If combining "Memory A" and "Memory B" reveals a hidden connection or causality.
   - **Action**: Call `infer_memory`.
   - **Example**:
     - Memory A (ID="2"): "User moved to Singapore."
     - Memory B (ID="3"): "User bought a Type G power adapter."
     - Inference: "User is preparing electronics for Singapore power standards."
     - Action: `infer_memory(source_memory_ids=["2", "3"], inference_content="...")`

5. **NOOP (no_operation)**
   - **Condition**: If the fact is redundant (already exactly covered by memory), similar to existing facts associated with the retrieved memories, or trivial.

[STRICT ID RULES]
- When calling `update_memory` or `delete_memory`, **ONLY** use the string integer IDs (e.g., "0", "1", "2") found in the [EXISTING MEMORIES] list.
- **NEVER** invent a UUID or use an ID that is not in the provided list.
"""


# MEMORY_MANAGER_PROMPT = """You are a specialized Memory Manager Agent.
# Your role is to maintain the consistency and growth of a memory graph using the provided tools.

# [INPUTS]
# You will receive:
# 1. "New Facts": A list of atomic facts extracted from the latest user input.
# 2. "Existing Memories": A list of retrieved memory items, each with a simplified Integer ID (e.g., "0", "1", "2").
#    - These memories include those directly related to the new facts, as well as other related facts connected with these memories.
#    - They form a connected graph of information relevant to the new facts.

# [MANDATORY OUTPUT FORMAT]
# For every new fact you process, you MUST:
# 1. First generate a detailed thinking process
# 2. Then call the appropriate tool

# [THINKING PROCESS REQUIREMENTS]
# Your thinking process MUST include:
# - The specific new fact you're analyzing
# - Which existing memories are relevant (with their IDs)
# - How memories are connected through related facts
# - Your comparison and reasoning
# - Which operation you've decided to perform and why

# [OPERATIONS & GUIDELINES]
# Compare New Facts with Existing Memories and perform the following operations using the available tools. 
# DO NOT output raw JSON text. You MUST use the provided function tools.

# 1. **ADD (create_memory)**
#    - **Condition**: If a fact contains completely NEW information not present in Existing Memories.
#    - **Action**: Call `create_memory` with a concise summary of the facts, not just a simple concatenation.
#    - **Important**: Memory content should be a meaningful and concise summary.
  
# 2. **UPDATE (update_memory)**
#    - **Condition**: If a fact adds detail, corrects, or updates a specific Existing Memory.
#    - **Constraint**: You MUST use the Integer ID (e.g., "0") provided in the input as the `target_memory_id`.
#    - **Logic**: Merge the old content and new fact into a comprehensive statement, not just a simple concatenation.
#    - **Example**:
#      - Old (ID="0"): "User likes generic pizza."
#      - New Fact: "User loves pepperoni pizza."
#      - Action: `update_memory(target_memory_id="0", new_content="User loves pepperoni pizza", ...)`

# 3. **DELETE (delete_memory)**
#    - **Condition**: If a fact explicitly contradicts an Existing Memory (and the new fact is trusted), or if the memory is no longer valid.
#    - **Constraint**: Use the Integer ID (e.g., "1") as `target_memory_id`.

# 4. **INFER (infer_memory)**
#    - **Condition**: Look for **higher-level insights**, **patterns**, or **implicit goals**. Do not just store facts; synthesize them to find user habits or personality traits.
#    - **Action**: Call `infer_memory` to create a new insight that connects multiple memories.
#    - **Examples**:
#      - *Case 1: Causality/Goal*
#        - Input: Memory A (ID="2"): "User moved to Singapore." + Memory B (ID="3"): "User bought a Type G power adapter."
#        - Inference: "User is preparing electronics for Singapore power standards."
#        - Action: `infer_memory(source_memory_ids=["2", "3"], inference_content="User is preparing electronics for Singapore power standards")`
     
#      - *Case 2: Pattern Recognition*
#        - Input: Memory C (ID="5"): "User ordered Sichuan Hotpot." + Memory D (ID="6"): "User loves extra spicy curry."
#        - Inference: "User has a high tolerance for spicy food and prefers spicy cuisine."
#        - Action: `infer_memory(source_memory_ids=["5", "6"], inference_content="User has a high tolerance for spicy food and prefers spicy cuisine")`

# 5. **NOOP (no_operation)**
#    - **Condition**: If the fact is redundant (already exactly covered by memory), similar to existing facts associated with the retrieved memories, or trivial.

# [STRICT ID RULES]
# - When calling `update_memory` or `delete_memory`, **ONLY** use the string integer IDs (e.g., "0", "1", "2") found in the [EXISTING MEMORIES] list.
# - **NEVER** invent a UUID or use an ID that is not in the provided list.
# """

FACT_MANAGER_PROMPT = """You are a specialized Fact Manager Agent.
Your role is to manage the lifecycle of atomic facts, ensuring new information is captured and related events are consolidated.

[INPUTS]
You will receive:
1. "New Facts": A list of atomic facts extracted from the latest user input. Each fact is formatted as "YYYY-MM-DD: ... (Details: ...)".
2. "Retrieved Facts": A list of existing facts retrieved from the database that are relevant to the new facts.

[OPERATIONS]
Compare New Facts with Retrieved Facts and perform the following operations:

1. **fact_add**
   - **Condition**: If a fact contains completely NEW information not present in Existing facts.
   - **Action**: Call `fact_add`.

2. **fact_trajectorize**
   - **Condition**: If new and old facts have similar content or are related events (e.g., updates on the same topic, sequential events).
   - **Action**: Combine multiple related events into a single "Trajectory" item. This item must explicitly list the chronological progression of events, preserving all original details (dates, specific items, locations, reasons, etc.) from the source facts. Do NOT just summarize; instead, structure it as a timeline log.
   - **Constraint**: You MUST retain the specific "Details" from each original fact in the trajectory.
   - **Example**: 
     - Old Fact: "2025-02-14: User is looking for a birthday gift for mom. (Reason: Mom's interest in Gardening)"
     - New Fact: "2025-02-16: User bought a set of ceramic pots for mom. (Price: $45, Store: HomeDepot)"
     - Trajectory: "2025-02-14: User is looking for a birthday gift for mom. (Reason: Mom's interest in Gardening) -> 2025-02-16: User purchased a set of ceramic pots for mom. (Price: $45, Store: HomeDepot)"
"""

CORE_MEMORY_MANAGER_PROMPT = """You are a Core Memory Manager Agent.
Your role is to build and maintain a comprehensive, long-term profile of the user by managing the "Core Memory" block.

[WHAT TO EXTRACT AND SAVE]
Analyze the provided facts and context to deeply understand the user. Capture and distill information that defines who they are and how to interact with them:
- **Identity & Personal Details**: Name, age, identity, role, and significant dates. (e.g., "Name is John Doe", "Born on 1990-05-15")
- **Personality & Values**: Traits, characteristics, communication style, and core beliefs. (e.g., "Introverted and analytical", "Values sustainability and non-toxic living")
- **Personal Preferences**: Specific likes/dislikes in food, products, brands, and entertainment. (e.g., "Loves Italian cuisine", "Prefers RPG games over shooters", "Favorite brand is Apple")
- **Activity & Service Preferences**: Preferences for dining, travel, hobbies, and other services. (e.g., "Prefers boutique hotels", "Enjoys morning runs", "Always books window seats")
- **Professional & Career**: Job titles, work habits, and professional goals. (e.g., "Senior Data Scientist at Meta", "Aims to lead an AI research team")
- **Relationships**: Family, friends, colleagues, and key social connections. (e.g., "Has a younger brother named Mike", "Close friend with Emma who is a marathon runner")
- **Plans & Intentions**: Upcoming events, trips, and long-term aspirations. (e.g., "Planning a hiking trip to Yosemite next month", "Wants to build a personal knowledge management system")
- **Health & Wellness**: Dietary restrictions, fitness routines, and wellness habits. (e.g., "Follows a gluten-free diet", "Practices yoga every morning")
- **Behaviors & Habits**: Recurring user behaviors and miscellaneous personal details. (e.g., "Usually reads for 30 minutes before bed", "Always drinks coffee while working")
- **Milestones**: Critical life events and significant milestones. (e.g., "Graduated from MIT in 2012", "Started first company in 2020")
- **Contextual Gold**: Any unique information that enhances future personalization. (e.g., "Is a huge fan of vintage mechanical watches", "Used to live in Paris for 5 years and speaks fluent French")

[EXAMPLES OF GOOD CORE MEMORY ENTRIES]
- "Is a software engineer at Google, specializing in machine learning"
- "Loves to play Cyberpunk 2077, prefers RPG games over shooters"
- "Has publications: 1. Paper on NLP transformers 2. Book on AI Ethics"
- "Close friend: Emma (marathon runner), meets weekly for coffee"
- "Working on long-term project: Building a personal knowledge management system"
- "Personality: Introverted, analytical, values deep conversations over small talk"

[INPUTS]
You will receive:
1. "New Facts": Atomic facts extracted from the current conversation.
2. "Old Core Memory": The existing content of the Core Memory block.
3. "Retrieved Memories": Relevant historical context for reference.

[OPERATIONS]
Analyze the inputs and perform one of the following operations:

1. **core_memory_add**
   - **Condition**: Add new, significant information to the Core Memory block.
   - **Example**:
     - New Fact: "User just started a new job as a Data Scientist at Amazon."
     - Action: `core_memory_add(content="Is a Data Scientist at Amazon (Started 2024)")`

2. **core_memory_update**
   - **Condition**: Update specific outdated or incorrect information. 
   - **Requirement**: You MUST specify both `old_text` and `new_text` for matching.
   - **Example**:
     - Old Text in Core Memory: "Is a software engineer at Google"
     - New Fact: "User has been promoted to Senior Software Engineer at Google."
     - Action: `core_memory_update(old_text="Is a software engineer at Google", new_text="Is a Senior software engineer at Google")`

3. **core_memory_rewrite**
   - **Condition**: Reorganize and consolidate the entire block. Used when its length exceeds 5000 chars or when major updates are needed.
   - **Action**: Completely rewrite the block to be more concise and better organized while preserving all core information.

[FULL PROFILE EXAMPLE]
# Basic Information
Sophia Lee is a ceramic artist based in San Francisco. She holds a degree in Art and Ceramics from SFSU.

# Personality & Values
Introverted and analytical, she values deep conversations over small talk. She emphasizes sustainability and non-toxic living.

# Interests & Preferences
- **Hobbies**: Playing guitar (Fender Stratocaster) and planning to learn ukulele.
- **Gaming**: Loves RPGs like Cyberpunk 2077.
- **Reading**: Enjoys poetry anthologies focusing on marginalized voices.

# Current Focus & Goals
Currently working on a personal knowledge management system and planning a project in Nigeria to connect villages to running water.
"""

# --- NEW MEMREADER PROMPT WITH HISTORY SUPPORT ---
# MEMREADER_PROMPT_WITH_HISTORY = """You are a Personal Information Organizer, specialized in accurately storing facts, user memories, assistant memories and preferences. Your primary role is to extract relevant pieces of information from conversations and organize them into distinct, manageable facts. This allows for easy retrieval and personalization in future interactions. Below are the types of information you need to focus on and the detailed instructions on how to handle the input conversation.

# Types of Information to Remember:

# 1. Store Personal Preferences: Keep track of likes, dislikes, and specific preferences in various categories such as food, products, activities, and entertainment.
# 2. Maintain Important Personal Details: Remember significant personal information like names, relationships, and important dates.
# 3. Track Plans and Intentions: Note upcoming events, trips, goals, and any plans the user has shared.
# 4. Remember Activity and Service Preferences: Recall preferences for dining, travel, hobbies, and other services.
# 5. Monitor Health and Wellness Preferences: Keep a record of dietary restrictions, fitness routines, and other wellness-related information.
# 6. Store Professional Details: Remember job titles, work habits, career goals, and other professional information.
# 7. Miscellaneous Information Management: Keep track of favorite books, movies, brands, and other miscellaneous details that the user and assistant share.

# Here are some few shot examples:

# Input:
# **Today's Date**: 2025-05-13
# **Previous Chat History**:
# user: I am planning a hiking trip to Yosemite National Park next weekend.
# assistant: That sounds wonderful! The views there are breathtaking. Who are you going with?

# **Current Conversation Turn**:
# user: I'm going with my brother, Mike. We hope to reach the Half Dome summit.
# Output: {{"facts" : [
#     {{"fact": "Going to Yosemite National Park with brother Mike", "details": ["Activity: Hiking", "Time: 2025-05-19 to 2025-05-25 (Next weekend)"]}},
#     {{"fact": "Hopes to summit Half Dome", "details": ["Location: Yosemite National Park"]}}
# ]}}

# Input:
# **Today's Date**: 2025-08-10
# **Previous Chat History**:
# assistant: I recall you mentioned being vegetarian. Is that still the case?

# **Current Conversation Turn**:
# user: Actually, no. I've started eating chicken recently for the protein, but I still avoid red meat.
# Output: {{"facts" : [
#     {{"fact": "Starts eating chicken", "details": ["Reason: For protein", "Status: Current diet update", "Avoid: Red meat", "Date: 2025-08-10"]}},
#     {{"fact": "Is no longer strictly vegetarian", "details": ["Reason: Eating chicken for the protein"]}}
# ]}}

# Input:
# **Today's Date**: 2025-10-01
# **Previous Chat History**:
# user: My laptop is broken, so I'm using the library computer.
# assistant: Oh no, I hope you can get it fixed. Are you working on your novel?

# **Current Conversation Turn**:
# user: Yes, I managed to write 2000 words for the 'The Lost City' draft despite the slow internet here.
# Output: {{"facts" : [
#     {{"fact": "Wrote 2000 words for 'The Lost City'", "details": ["Location: Library", "Device: Library computer"]}},
#     {{"fact": "Experiencing slow internet", "details": ["Location: Library", "Device: Library computer"]}}
# ]}}

# Input:
# **Today's Date**: 2025-11-15
# **Previous Chat History**:
# user: I'm looking for a gift for my wife.
# assistant: Does she have any specific hobbies?
# user: She loves outdoor sports.
# assistant: How about tennis gear?

# **Current Conversation Turn**:
# user: She already has plenty of that. She is really into rock climbing these days.
# Output: {{"facts" : [
#     {{"fact": "Wife is into rock climbing", "details": ["Interest: Outdoor sports"]}},
#     {{"fact": "Wife already has tennis gear", "details": []}}
# ]}}

# Input:
# **Today's Date**: 2026-02-01
# **Previous Chat History**:
# user: I need to book a flight to London.
# assistant: When are you planning to fly?

# **Current Conversation Turn**:
# user: I want to leave on the 15th and return on the 22nd. I strictly want to avoid overnight layovers.
# Output: {{"facts" : [
#     {{"fact": "Planning a flight to London", "details": ["Departure: 2026-02-15", "Return: 2026-02-22"]}},
#     {{"fact": "Strictly avoids overnight layovers", "details": ["Preference: Flight booking constraint"]}}
# ]}}

# Return the facts and preferences in a json format as shown above.

# Remember the following:
# - Today's date is {today_date}.
# - **Supplementary Details**: The `details` list must act as **METADATA** to supplement the fact (e.g., Time, Location, Price, Platform, Reason), **NOT** just splitting the fact's words. (e.g., If fact is "Bought apple", details should be ["Price: $1", "Store: Aldi"], NOT ["Action: Buy", "Object: Apple"]).
# - **Context Propagation**: Ensure every extracted fact is **self-contained**. If a shared context (e.g., location, platform, activity, or timeframe) is established anywhere in the input chunk or previous chat history, explicitly include it in the `details` of all relevant facts, even if not repeated in every sentence.
# - **Date Resolution**: ALWAYS resolve relative time expressions (e.g., "next weekend", "the 15th", "tomorrow") into absolute ISO dates (YYYY-MM-DD) based on Today's date provided in the input.
# - Do not return anything from the custom few shot example prompts provided above.
# - If you do not find anything relevant in the below conversation, you can return an empty list corresponding to the "facts" key.
# - Create the facts based on the user and assistant messages only. Do not pick anything from the system messages.
# - Make sure to return the response in the format mentioned in the examples. The response should be in json with a key as "facts", where each item has a "fact" string and a "details" list of strings.
# - When processing with previous chat history, leverage the context to extract more accurate and comprehensive facts, especially for anaphoric references and context-dependent information.

# Following is a conversation between the user and the assistant. You have to extract the relevant facts and preferences about the user and facts about the assistant, if any, from the conversation and return them in the json format as shown above.
# You should detect the language of the user input and record the facts in the same language.
# """

MEMREADER_PROMPT_WITH_HISTORY = """You are a Personal Information Organizer, specialized in accurately storing facts, user memories, assistant memories and preferences. Your primary role is to extract relevant pieces of information from conversations and organize them into distinct, manageable facts. This allows for easy retrieval and personalization in future interactions. Below are the types of information you need to focus on and the detailed instructions on how to handle the input conversation.

Types of Information to Remember:

1. Store Personal Preferences: Keep track of likes, dislikes, and specific preferences in various categories such as food, products, activities, and entertainment.
2. Maintain Important Personal Details: Remember significant personal information like names, relationships, and important dates.
3. Track Plans and Intentions: Note upcoming events, trips, goals, and any plans the user has shared.
4. Remember Activity and Service Preferences: Recall preferences for dining, travel, hobbies, and other services.
5. Monitor Health and Wellness Preferences: Keep a record of dietary restrictions, fitness routines, and other wellness-related information.
6. Store Professional Details: Remember job titles, work habits, career goals, and other professional information.
7. Miscellaneous Information Management: Keep track of favorite books, movies, brands, and other miscellaneous details that the user and assistant share.

Here are some few shot examples:

Input:
**Today's Date**: 2025-05-13
**Previous Chat History**:
user: I am planning a hiking trip to Yosemite National Park next weekend.
assistant: That sounds wonderful! The views there are breathtaking. Who are you going with?

**Current Conversation Turn**:
user: I'm going with my brother, Mike. We hope to reach the Half Dome summit.
assistant: That sounds like a great challenge. I'll make a note that you are hiking Half Dome with Mike.
Output: {{"facts" : [
    {{"fact": "Going to Yosemite National Park with brother Mike", "details": ["Activity: Hiking", "Time: 2025-05-24 to 2025-05-25 (Next weekend)"]}},
    {{"fact": "Hopes to summit Half Dome", "details": ["Location: Yosemite National Park", "Intent: Hiking goal"]}}
]}}

Input:
**Today's Date**: 2025-08-10
**Previous Chat History**:
assistant: I recall you mentioned being vegetarian. Is that still the case?

**Current Conversation Turn**:
user: Actually, no. I've started eating chicken recently for the protein, but I still avoid red meat.
assistant: Understood. I've updated your dietary preferences to include chicken but exclude red meat.
Output: {{"facts" : [
    {{"fact": "Starts eating chicken", "details": ["Reason: For protein", "Status: Current diet update", "Avoid: Red meat", "Date: 2025-08-10"]}},
    {{"fact": "Is no longer strictly vegetarian", "details": ["Reason: Eating chicken for the protein"]}}
]}}

Input:
**Today's Date**: 2025-10-01
**Previous Chat History**:
user: My laptop is broken, so I'm using the library computer.
assistant: Oh no, I hope you can get it fixed. Are you working on your novel?

**Current Conversation Turn**:
user: Yes, I managed to write 2000 words for the 'The Lost City' draft despite the slow internet here.
assistant: That is impressive dedication! Writing 2000 words on a public computer is no small feat.
Output: {{"facts" : [
    {{"fact": "Wrote 2000 words for 'The Lost City'", "details": ["Location: Library", "Device: Library computer"]}},
    {{"fact": "Experiencing slow internet", "details": ["Location: Library", "Device: Library computer"]}}
]}}

Input:
**Today's Date**: 2025-11-15
**Previous Chat History**:
user: I'm looking for a gift for my wife.
assistant: Does she have any specific hobbies?
user: She loves outdoor sports.
assistant: How about tennis gear?

**Current Conversation Turn**:
user: She already has plenty of that. She is really into rock climbing these days.
assistant: Got it. Since she likes rock climbing, I recommend checking out the new indoor climbing gym downtown.
Output: {{"facts" : [
    {{"fact": "Wife is into rock climbing", "details": ["Interest: Outdoor sports"]}},
    {{"fact": "Wife already has tennis gear", "details": []}},
    {{"fact": "[Assistant] Recommends new indoor climbing gym downtown", "details": ["Reason: Matches wife's rock climbing interest"]}}
]}}

Input:
**Today's Date**: 2026-02-01
**Previous Chat History**:
user: I need to book a flight to London.
assistant: When are you planning to fly?

**Current Conversation Turn**:
user: I want to leave on the 15th and return on the 22nd. I strictly want to avoid overnight layovers.
assistant: Noted. I will filter for direct flights. Just a reminder: London is currently 8 hours ahead of your timezone.
Output: {{"facts" : [
    {{"fact": "Planning a flight to London", "details": ["Departure: 2026-02-15", "Return: 2026-02-22"]}},
    {{"fact": "Strictly avoids overnight layovers", "details": ["Preference: Flight booking constraint"]}},
    {{"fact": "[Assistant] London is 8 hours ahead of user's timezone", "details": ["Context: Timezone reminder"]}}
]}}

Return the facts and preferences in a json format as shown above.

Remember the following:
- Today's date is {today_date}.
- **Supplementary Details**: The `details` list must act as **METADATA** to supplement the fact (e.g., Time, Location, Price, Platform, Reason), **NOT** just splitting the fact's words.
- **Source Attribution**: If a fact or suggestion originates explicitly from the **assistant**, you MUST prefix the fact text with **"[Assistant]"**. (e.g., "[Assistant] Recommends checking out..."). Facts from the user do not need a prefix.
- **Context Propagation**: Ensure every extracted fact is **self-contained**. If a shared context (e.g., location, platform, activity, or timeframe) is established anywhere in the input chunk or previous chat history, explicitly include it in the `details` of all relevant facts, even if not repeated in every sentence.
- **Date Resolution**: ALWAYS resolve relative time expressions (e.g., "next weekend", "the 15th", "tomorrow") into absolute ISO dates (YYYY-MM-DD) based on Today's date provided in the input.
- Do not return anything from the custom few shot example prompts provided above.
- If you do not find anything relevant in the below conversation, you can return an empty list corresponding to the "facts" key.
- Create the facts based on the user and assistant messages only. Do not pick anything from the system messages.
- Make sure to return the response in the format mentioned in the examples. The response should be in json with a key as "facts", where each item has a "fact" string and a "details" list of strings.
- When processing with previous chat history, leverage the context to extract more accurate and comprehensive facts, especially for anaphoric references and context-dependent information.

Following is a conversation between the user and the assistant. You have to extract the relevant facts and preferences about the user and facts about the assistant, if any, from the conversation and return them in the json format as shown above.
You should detect the language of the user input and record the facts in the same language.
"""

# MEMREADER_PROMPT_WITH_HISTORY = """You are a Personal Information Organizer, specialized in accurately storing facts, user memories, assistant memories and preferences. Your primary role is to extract relevant pieces of information from conversations and organize them into distinct, manageable facts. This allows for easy retrieval and personalization in future interactions. Below are the types of information you need to focus on and the detailed instructions on how to handle the input conversation.

# Types of Information to Remember:

# 1. Store Personal Preferences: Keep track of likes, dislikes, and specific preferences in various categories such as food, products, activities, and entertainment.
# 2. Maintain Important Personal Details: Remember significant personal information like names, relationships, and important dates.
# 3. Track Plans and Intentions: Note upcoming events, trips, goals, and any plans the user has shared.
# 4. Remember Activity and Service Preferences: Recall preferences for dining, travel, hobbies, and other services.
# 5. Monitor Health and Wellness Preferences: Keep a record of dietary restrictions, fitness routines, and other wellness-related information.
# 6. Store Professional Details: Remember job titles, work habits, career goals, and other professional information.
# 7. Miscellaneous Information Management: Keep track of favorite books, movies, brands, and other miscellaneous details that the user and assistant share.

# Here are some few shot examples:

# Input:
# **Today's Date**: 2025-05-13
# **Previous Chat History**:
# user: I am planning a hiking trip to Yosemite National Park next weekend.
# assistant: That sounds wonderful! The views there are breathtaking. Who are you going with?

# **Current Conversation Turn**:
# user: I'm going with my brother, Mike. We hope to reach the Half Dome summit.
# assistant: That sounds like a great challenge. I'll make a note that you are hiking Half Dome with Mike.
# Output: {{"facts" : [
#     {{"fact": "Going to Yosemite National Park with brother Mike", "details": ["Activity: Hiking", "Time: 2025-05-24 to 2025-05-25 (Next weekend)"]}},
#     {{"fact": "Hopes to summit Half Dome", "details": ["Location: Yosemite National Park", "Intent: Hiking goal"]}}
# ]}}

# Input:
# **Today's Date**: 2025-08-10
# **Previous Chat History**:
# assistant: I recall you mentioned being vegetarian. Is that still the case?

# **Current Conversation Turn**:
# user: Actually, no. I've started eating chicken recently for the protein, but I still avoid red meat.
# assistant: Understood. I've updated your dietary preferences to include chicken but exclude red meat.
# Output: {{"facts" : [
#     {{"fact": "Starts eating chicken", "details": ["Reason: For protein", "Status: Current diet update", "Avoid: Red meat", "Date: 2025-08-10"]}},
#     {{"fact": "Is no longer strictly vegetarian", "details": ["Reason: Eating chicken for the protein"]}}
# ]}}

# Input:
# **Today's Date**: 2025-10-01
# **Previous Chat History**:
# user: My laptop is broken, so I'm using the library computer.
# assistant: Oh no, I hope you can get it fixed. Are you working on your novel?

# **Current Conversation Turn**:
# user: Yes, I managed to write 2000 words for the 'The Lost City' draft despite the slow internet here.
# assistant: That is impressive dedication! Writing 2000 words on a public computer is no small feat.
# Output: {{"facts" : [
#     {{"fact": "Wrote 2000 words for 'The Lost City'", "details": ["Location: Library", "Device: Library computer"]}},
#     {{"fact": "Experiencing slow internet", "details": ["Location: Library", "Device: Library computer"]}}
# ]}}

# Input:
# **Today's Date**: 2025-11-15
# **Previous Chat History**:
# user: I'm looking for a gift for my wife.
# assistant: Does she have any specific hobbies?
# user: She loves outdoor sports.
# assistant: How about tennis gear?

# **Current Conversation Turn**:
# user: She already has plenty of that. She is really into rock climbing these days.
# assistant: Got it. Since she likes rock climbing, I recommend checking out the new indoor climbing gym downtown.
# Output: {{"facts" : [
#     {{"fact": "Wife is into rock climbing", "details": ["Interest: Outdoor sports"]}},
#     {{"fact": "Wife already has tennis gear", "details": []}},
#     {{"fact": "Recommends new indoor climbing gym downtown", "details": ["Reason: Matches wife's rock climbing interest"]}}
# ]}}

# Input:
# **Today's Date**: 2026-02-01
# **Previous Chat History**:
# user: I need to book a flight to London.
# assistant: When are you planning to fly?

# **Current Conversation Turn**:
# user: I want to leave on the 15th and return on the 22nd. I strictly want to avoid overnight layovers.
# assistant: Noted. I will filter for direct flights. Just a reminder: London is currently 8 hours ahead of your timezone.
# Output: {{"facts" : [
#     {{"fact": "Planning a flight to London", "details": ["Departure: 2026-02-15", "Return: 2026-02-22"]}},
#     {{"fact": "Strictly avoids overnight layovers", "details": ["Preference: Flight booking constraint"]}},
#     {{"fact": "London is 8 hours ahead of user's timezone", "details": ["Context: Timezone reminder"]}}
# ]}}

# Return the facts and preferences in a json format as shown above.

# Remember the following:
# - Today's date is {today_date}.
# - **Supplementary Details**: The `details` list must act as **METADATA** to supplement the fact (e.g., Time, Location, Price, Platform, Reason), **NOT** just splitting the fact's words.
# - **Context Propagation**: Ensure every extracted fact is **self-contained**. If a shared context (e.g., location, platform, activity, or timeframe) is established anywhere in the input chunk or previous chat history, explicitly include it in the `details` of all relevant facts, even if not repeated in every sentence.
# - **Date Resolution**: ALWAYS resolve relative time expressions (e.g., "next weekend", "the 15th", "tomorrow") into absolute ISO dates (YYYY-MM-DD) based on Today's date provided in the input.
# - Do not return anything from the custom few shot example prompts provided above.
# - If you do not find anything relevant in the below conversation, you can return an empty list corresponding to the "facts" key.
# - Create the facts based on the user and assistant messages only. Do not pick anything from the system messages.
# - Make sure to return the response in the format mentioned in the examples. The response should be in json with a key as "facts", where each item has a "fact" string and a "details" list of strings.
# - When processing with previous chat history, leverage the context to extract more accurate and comprehensive facts, especially for anaphoric references and context-dependent information.

# Following is a conversation between the user and the assistant. You have to extract the relevant facts and preferences about the user and facts about the assistant, if any, from the conversation and return them in the json format as shown above.
# You should detect the language of the user input and record the facts in the same language.
# """

# --- TOOLS ---
MEMORY_TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "create_memory",
            "description": "Create a NEW independent memory node with a concise summary of the facts.",
            "parameters": {
                "type": "object",
                "properties": {
                    "content": {"type": "string", "description": "The concise summary content of the new memory, not just a list of facts."}
                },
                "required": ["content"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "update_memory",
            "description": "Update an existing memory by merging the old content and new fact into a comprehensive, concise statement.",
            "parameters": {
                "type": "object",
                "properties": {
                    "target_memory_id": {"type": "string", "description": "The simplified Integer ID (e.g., '0') of the memory to update, found in the [EXISTING MEMORIES] list."},
                    "new_content": {"type": "string", "description": "The merged/updated comprehensive statement."}
                },
                "required": ["target_memory_id", "new_content"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "infer_memory",
            "description": "Look for higher-level insights. If combining multiple existing memories reveals a hidden connection or causality, create an inferred memory.",
            "parameters": {
                "type": "object",
                "properties": {
                    "source_memory_ids": {"type": "array", "items": {"type": "string"}, "description": "List of simplified Integer IDs (e.g., ['0', '1']) acting as premises, found in the [EXISTING MEMORIES] list."},
                    "inference_content": {"type": "string", "description": "The higher-level insight or inference derived from combining the source memories."}
                },
                "required": ["source_memory_ids", "inference_content"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "delete_memory",
            "description": "Archive/Soft-delete a memory if it explicitly contradicts a new fact (and the new fact is trusted), or if the memory is no longer valid.",
            "parameters": {
                "type": "object",
                "properties": {
                    "target_memory_id": {"type": "string", "description": "The simplified Integer ID (e.g., '1') of the memory to delete, found in the [EXISTING MEMORIES] list."}
                },
                "required": ["target_memory_id"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "no_operation",
            "description": "No action needed if the fact is redundant (already exactly covered by memory or its associated facts).",
            "parameters": {
                "type": "object",
                "properties": {"reason": {"type": "string", "description": "The reason for no operation."}},
                "required": ["reason"]
            }
        }
    }
]

FACT_TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "fact_add",
            "description": "Register a completely new fact.",
            "parameters": {
                "type": "object",
                "properties": {
                    "content": {"type": "string", "description": "The content of the new fact."},
                    "details": {"type": "array", "items": {"type": "string"}, "description": "Details or metadata of the fact."}
                },
                "required": ["content", "details"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "fact_trajectorize",
            "description": "Create timeline with timestamp range, synthesizing events and drawing well supported conclusions from patterns.",
            "parameters": {
                "type": "object",
                "properties": {
                    "related_fact_ids": {"type": "array", "items": {"type": "string"}, "description": "IDs of the old facts to be archived."},
                    "content": {"type": "string", "description": "The combined timeline summary including timestamp range."}
                },
                "required": ["related_fact_ids", "content"]
            }
        }
    },
]

CORE_MEMORY_TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "core_memory_add",
            "description": "Add new information to the Core Memory block.",
            "parameters": {
                "type": "object",
                "properties": {
                    "content": {"type": "string", "description": "The new information to add."}
                },
                "required": ["content"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "core_memory_update",
            "description": "Update specific information in the Core Memory block.",
            "parameters": {
                "type": "object",
                "properties": {
                    "old_text": {"type": "string", "description": "The specific text to be replaced."},
                    "new_text": {"type": "string", "description": "The new text to replace with."}
                },
                "required": ["old_text", "new_text"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "core_memory_rewrite",
            "description": "Rewrite the entire Core Memory block.",
            "parameters": {
                "type": "object",
                "properties": {
                    "new_block_content": {"type": "string", "description": "The completely new content for the Core Memory block."}
                },
                "required": ["new_block_content"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "no_operation",
            "description": "No action needed if the core memory is up to date.",
            "parameters": {
                "type": "object",
                "properties": {"reason": {"type": "string", "description": "The reason for no operation."}},
                "required": ["reason"]
            }
        }
    }
]

# --- UTILS ---
def get_embedding(text: str) -> List[float]:
    text = text.replace("\n", " ")
    # Use embedding_client (always OpenAI) instead of llm_client (which might be local)
    return embedding_client.embeddings.create(input=[text], model=EMBEDDING_MODEL).data[0].embedding

@dataclass
class MilvusConfig:
    """Milvusé…ç½®ç±»ï¼ˆå…¼å®¹æ—§ä»£ç ï¼‰"""
    uri: str = os.getenv("MILVUS_URI")
    user_name: str = os.getenv("MILVUS_USER_NAME")
    # password: str = os.getenv("MILVUS_PASSWORD")
    db_name: str = os.getenv("MILVUS_DB_NAME", "default")
    dimension: int = 1536
    
    def to_vector_db_config(self, vector_db_type: str = "milvus") -> VectorDBConfig:
        """è½¬æ¢ä¸ºVectorDBConfig"""
        # ç¡®ä¿vector_db_typeæ˜¯å­—ç¬¦ä¸²ç±»å‹
        if not isinstance(vector_db_type, str):
            vector_db_type = "milvus"  # é»˜è®¤ä½¿ç”¨milvus
        
        # æ ¹æ®vector_db_typeé€‰æ‹©ä¸åŒçš„URL
        if vector_db_type == "qdrant":
            uri = os.getenv("QDRANT_URL")
            api_key = os.getenv("QDRANT_API_KEY")
            user_name = ""
            password = ""
        else:
            uri = self.uri
            api_key = ""
            user_name = self.user_name
            password = os.getenv("MILVUS_PASSWORD")
        
        return VectorDBConfig(
            uri=uri,
            user_name=user_name,
            password=password,
            api_key=api_key,
            db_name=self.db_name,
            dimension=self.dimension,
            vector_db_type=vector_db_type
        )

# ==========================================
# 1. Pipeline Class
# ==========================================
class MemoryPipeline:
    def __init__(self, config=None, vector_db_type="milvus", clear_db=False, mode='eval', dataset_name=""):
        """åˆå§‹åŒ–MemoryPipeline
        
        Args:
            config: MilvusConfigæˆ–VectorDBConfigå®ä¾‹ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
            vector_db_type: æŒ‡å®šä½¿ç”¨çš„å‘é‡æ•°æ®åº“ç±»å‹ï¼Œæ”¯æŒ"milvus"æˆ–"qdrant"
            clear_db: æ˜¯å¦æ¸…ç©ºæ•°æ®åº“ï¼Œé»˜è®¤ä¸ºFalse
            dataset_name: æ•°æ®é›†åç§°ï¼Œç”¨äºé›†åˆåç§°åç¼€ï¼Œé»˜è®¤ä¸ºç©º
        """
        # å¦‚æœæ²¡æœ‰æä¾›é…ç½®ï¼Œåˆ›å»ºé»˜è®¤é…ç½®
        if config is None:
            config = MilvusConfig()
        
        self.config = config
        
        # è½¬æ¢ä¸ºVectorDBConfig
        if hasattr(config, 'to_vector_db_config'):
            vector_db_config = config.to_vector_db_config(vector_db_type=vector_db_type)
        else:
            # å¦‚æœå·²ç»æ˜¯VectorDBConfigå®ä¾‹ï¼Œç›´æ¥ä½¿ç”¨
            vector_db_config = config
        
        # ä½¿ç”¨å·¥å‚ç±»åˆ›å»ºå‘é‡æ•°æ®åº“å®¢æˆ·ç«¯
        self.client = VectorDBFactory.create_db(vector_db_config)
        
        # æ ¹æ®æ¨¡å¼å’Œæ•°æ®é›†åç§°è®¾ç½®é›†åˆåç§°
        base_suffix = "_test" if mode == 'test' else ""
        dataset_suffix = f"_{dataset_name}" if dataset_name else ""
        full_suffix = f"{base_suffix}{dataset_suffix}"
        
        self.semantic_col = f"memories{full_suffix}_v1"
        self.fact_col = f"facts{full_suffix}_v1"
        self.chunk_col = f"chunks{full_suffix}_v1"
        
        self.dim = vector_db_config.dimension  # Save dimension as instance variable
        # åˆå§‹åŒ–æ“ä½œæ¬¡æ•°è®¡æ•°å™¨
        self.operation_counts = {"ADD": 0, "UPDATE": 0, "DELETE": 0, "INFER": 0, "NOOP": 0}
        # æ·»åŠ å¸¦å†å²æ”¯æŒçš„memreader prompt
        self.MEMREADER_PROMPT_WITH_HISTORY = MEMREADER_PROMPT_WITH_HISTORY
        
        # Initialize Core Memory
        self.core_memory = ""
        
        self._init_collections(clear_db=clear_db)

    def _init_collections(self, clear_db=False):
        dim = self.config.dimension
        
        # å¦‚æœéœ€è¦æ¸…ç©ºæ•°æ®åº“ï¼Œå…ˆåˆ é™¤æ‰€æœ‰é›†åˆ
        if clear_db:
            print("æ­£åœ¨æ¸…ç©ºæ•°æ®åº“...")
            # ç›´æ¥åˆ é™¤é›†åˆï¼Œä¸æ£€æŸ¥å­˜åœ¨æ€§
            self.client.drop_collection(self.semantic_col)
            self.client.drop_collection(self.fact_col)
            self.client.drop_collection(self.chunk_col)
            print("æ•°æ®åº“æ¸…ç©ºå®Œæˆ.")
        
        # æ£€æŸ¥å¹¶åˆ›å»ºé›†åˆ
        
        # å¤„ç† memories é›†åˆ
        if hasattr(self.client, 'DataType'):
            # è¿™æ˜¯ Milvus å®¢æˆ·ç«¯
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            if not self.client.has_collection(self.semantic_col):
                # åˆ›å»ºå®Œæ•´çš„schema
                s = self.client.create_schema(auto_id=False, enable_dynamic_field=True)
                s.add_field("memory_id", self.client.DataType.VARCHAR, max_length=64, is_primary=True)
                s.add_field("embedding", self.client.DataType.FLOAT_VECTOR, dim=dim)
                s.add_field("content", self.client.DataType.VARCHAR, max_length=65535)
                s.add_field("user_id", self.client.DataType.VARCHAR, max_length=64)
                s.add_field("status", self.client.DataType.VARCHAR, max_length=16)
                s.add_field("created_at", self.client.DataType.INT64)
                s.add_field("updated_at", self.client.DataType.INT64)
                s.add_field("relations", self.client.DataType.JSON) 
                
                # åˆ›å»ºé›†åˆ
                self.client.create_collection(self.semantic_col, schema=s)
                print(f"Collection '{self.semantic_col}' created.")
                
                # ç›´æ¥åˆ›å»ºç´¢å¼•ï¼Œä¸æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
                # Milvusçš„create_indexæ–¹æ³•ä¼šåœ¨ç´¢å¼•å·²å­˜åœ¨æ—¶è‡ªåŠ¨è·³è¿‡æˆ–è¿”å›æˆåŠŸ
                try:
                    print(f"ä¸ºé›†åˆ '{self.semantic_col}' åˆ›å»ºç´¢å¼•...")
                    idx_params = self.client.prepare_index_params()
                    idx_params.add_index(field_name="embedding", index_type="IVF_FLAT", metric_type="COSINE", params={"nlist": 128})
                    self.client.create_index(self.semantic_col, index_params=idx_params)
                    print(f"é›†åˆ '{self.semantic_col}' çš„ç´¢å¼•åˆ›å»ºæˆåŠŸæˆ–å·²å­˜åœ¨")
                except Exception as e:
                    print(f"åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
            else:
                print(f"Collection '{self.semantic_col}' already exists, skipping creation.")
        else:
            # éMilvuså®¢æˆ·ç«¯ï¼Œç›´æ¥åˆ›å»ºé›†åˆ
            self.client.create_collection(self.semantic_col)
            print(f"Collection '{self.semantic_col}' created or exists.")
        
        # å¤„ç† facts é›†åˆ
        if hasattr(self.client, 'DataType'):
            # è¿™æ˜¯ Milvus å®¢æˆ·ç«¯
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            if not self.client.has_collection(self.fact_col):
                s = self.client.create_schema(auto_id=False, enable_dynamic_field=True)
                s.add_field("fact_id", self.client.DataType.VARCHAR, max_length=64, is_primary=True)
                s.add_field("linked_chunk_id", self.client.DataType.VARCHAR, max_length=64)
                s.add_field("text", self.client.DataType.VARCHAR, max_length=65535)
                s.add_field("details", self.client.DataType.JSON)  # æ·»åŠ detailså­—æ®µ
                s.add_field("timestamp", self.client.DataType.INT64)
                s.add_field("user_id", self.client.DataType.VARCHAR, max_length=64)  # æ·»åŠ user_idå­—æ®µ
                s.add_field("embedding", self.client.DataType.FLOAT_VECTOR, dim=dim)
                
                # åˆ›å»ºé›†åˆ
                self.client.create_collection(self.fact_col, schema=s)
                print(f"Collection '{self.fact_col}' created.")
                
                # ç›´æ¥åˆ›å»ºç´¢å¼•ï¼Œä¸æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
                # Milvusçš„create_indexæ–¹æ³•ä¼šåœ¨ç´¢å¼•å·²å­˜åœ¨æ—¶è‡ªåŠ¨è·³è¿‡æˆ–è¿”å›æˆåŠŸ
                try:
                    print(f"ä¸ºé›†åˆ '{self.fact_col}' åˆ›å»ºç´¢å¼•...")
                    idx_params = self.client.prepare_index_params()
                    idx_params.add_index(field_name="embedding", index_type="IVF_FLAT", metric_type="COSINE", params={"nlist": 128})
                    self.client.create_index(self.fact_col, index_params=idx_params)
                    print(f"é›†åˆ '{self.fact_col}' çš„ç´¢å¼•åˆ›å»ºæˆåŠŸæˆ–å·²å­˜åœ¨")
                except Exception as e:
                    print(f"åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
            else:
                print(f"Collection '{self.fact_col}' already exists, skipping creation.")
        else:
            # éMilvuså®¢æˆ·ç«¯ï¼Œç›´æ¥åˆ›å»ºé›†åˆ
            self.client.create_collection(self.fact_col)
            print(f"Collection '{self.fact_col}' created or exists.")
        
        # å¤„ç† chunks é›†åˆ
        if hasattr(self.client, 'DataType'):
            # è¿™æ˜¯ Milvus å®¢æˆ·ç«¯
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            if not self.client.has_collection(self.chunk_col):
                s = self.client.create_schema(auto_id=False, enable_dynamic_field=True)
                s.add_field("chunk_id", self.client.DataType.VARCHAR, max_length=64, is_primary=True)
                s.add_field("text", self.client.DataType.VARCHAR, max_length=65535)
                s.add_field("timestamp", self.client.DataType.INT64)
                s.add_field("embedding", self.client.DataType.FLOAT_VECTOR, dim=dim)
                
                # åˆ›å»ºé›†åˆ
                self.client.create_collection(self.chunk_col, schema=s)
                print(f"Collection '{self.chunk_col}' created.")
                
                # ç›´æ¥åˆ›å»ºç´¢å¼•ï¼Œä¸æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
                # Milvusçš„create_indexæ–¹æ³•ä¼šåœ¨ç´¢å¼•å·²å­˜åœ¨æ—¶è‡ªåŠ¨è·³è¿‡æˆ–è¿”å›æˆåŠŸ
                try:
                    print(f"ä¸ºé›†åˆ '{self.chunk_col}' åˆ›å»ºç´¢å¼•...")
                    idx_params = self.client.prepare_index_params()
                    idx_params.add_index(field_name="embedding", index_type="IVF_FLAT", metric_type="COSINE", params={"nlist": 128})
                    self.client.create_index(self.chunk_col, index_params=idx_params)
                    print(f"é›†åˆ '{self.chunk_col}' çš„ç´¢å¼•åˆ›å»ºæˆåŠŸæˆ–å·²å­˜åœ¨")
                except Exception as e:
                    print(f"åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
            else:
                print(f"Collection '{self.chunk_col}' already exists, skipping creation.")
        else:
            # éMilvuså®¢æˆ·ç«¯ï¼Œç›´æ¥åˆ›å»ºé›†åˆ
            self.client.create_collection(self.chunk_col)
            print(f"Collection '{self.chunk_col}' created or exists.")
        
        # ç›´æ¥åŠ è½½æ‰€æœ‰é›†åˆï¼Œä¸è¿›è¡Œå¤æ‚çš„é”™è¯¯å¤„ç†
        print("Loading collections into memory...")
        
        # åŠ è½½é›†åˆï¼ˆQdrant ä¸éœ€è¦æ˜¾å¼åŠ è½½ï¼‰
        if hasattr(self.client, 'load_collection'):
            # ä¸ºæ¯ä¸ªé›†åˆåˆ›å»ºç´¢å¼•åç›´æ¥åŠ è½½
            print(f"åŠ è½½é›†åˆ '{self.semantic_col}'...")
            self.client.load_collection(self.semantic_col)
            
            print(f"åŠ è½½é›†åˆ '{self.fact_col}'...")
            self.client.load_collection(self.fact_col)
            
            print(f"åŠ è½½é›†åˆ '{self.chunk_col}'...")
            self.client.load_collection(self.chunk_col)
            
            print("All collections loaded successfully.")

    # --- Step 1: Extract ---
    def step_extract(self, session_or_text, extract_mode: str = "whole", timestamp: int = None, max_history_turns: int = 5) -> Dict:
        """
        ä»å¯¹è¯ä¸­æå–äº‹å®
        
        Args:
            session_or_text: å¯¹è¯ä¼šè¯ï¼Œå¯ä»¥æ˜¯åŸå§‹session listæˆ–å¯¹è¯æ–‡æœ¬
            extract_mode: æå–æ¨¡å¼ï¼Œå¯é€‰å€¼ï¼š
                - "whole": å¯¹æ•´ä¸ªchunkè¿›è¡Œæå–
                - "turn": æŒ‰è½®æ¬¡æå–ï¼Œæ¯è½®user-assistantå¯¹è¯å•ç‹¬æå–ï¼Œå¹¶é™„ä¸Šchat history
            timestamp: æ—¶é—´æˆ³ï¼Œå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å½“å‰æ—¶é—´
            max_history_turns: èŠå¤©å†å²çš„æœ€å¤§è½®æ•°ï¼Œä»…åœ¨extract_mode="turn"æ—¶ç”Ÿæ•ˆ
        
        Returns:
            åŒ…å«æå–äº‹å®çš„å­—å…¸
        """
        # print(f"\nğŸ‘€ [1. Extract] Processing...")
        
        # å¦‚æœæ²¡æœ‰æä¾›timestampï¼Œä½¿ç”¨å½“å‰æ—¶é—´
        if timestamp is None:
            timestamp = int(time.time())
        
        # å¦‚æœæ˜¯æŒ‰è½®æ¬¡æå–ï¼Œç›´æ¥å¤„ç†session list
        if extract_mode == "turn" and isinstance(session_or_text, list):
            try:
                all_facts = []
                chat_history = []  # ä¿å­˜å®Œæ•´çš„å¯¹è¯å†å²
                
                # éå†session listï¼Œæˆå¯¹å½¢æˆturn
                for i in range(0, len(session_or_text), 2):
                    # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªuseræ¶ˆæ¯
                    if i < len(session_or_text):
                        user_msg = session_or_text[i]
                        # æ£€æŸ¥æ˜¯å¦æ˜¯userè§’è‰²
                        if user_msg.get("role") == "user":
                            # æ„å»ºå½“å‰turnï¼ŒåŒ…å«useræ¶ˆæ¯
                            turn = [user_msg]
                            # å¦‚æœæœ‰assistantæ¶ˆæ¯ï¼Œæ·»åŠ åˆ°å½“å‰turn
                            if i + 1 < len(session_or_text) and session_or_text[i+1].get("role") == "assistant":
                                turn.append(session_or_text[i+1])
                            
                            # å°†turnè½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
                            turn_text = parse_messages(turn)
                            
                            # æ·»åŠ å½“å‰turnåˆ°chat history
                            chat_history.append(turn)
                            
                            # æ„å»ºèŠå¤©å†å²ï¼Œä½¿ç”¨å½“å‰turnä¹‹å‰çš„max_history_turnsè½®å¯¹è¯
                            history_turns = chat_history[:-1][-max_history_turns:]  # æœ€è¿‘max_history_turnsè½®å†å²
                            history_text = parse_messages([msg for turn in history_turns for msg in turn])
                            
                            # å¯¹å•è½®å¯¹è¯æå–äº‹å®ï¼Œä¼ é€’timestampå’Œchat_historyå‚æ•°
                            turn_facts = self._extract_single_turn(turn_text, timestamp, history_text)
                            
                            # ä¸ºæ¯ä¸ªäº‹å®æ·»åŠ è½®æ¬¡ä¿¡æ¯å’Œchat historyå¼•ç”¨
                            for fact in turn_facts:
                                fact["turn_idx"] = len(chat_history)  # è½®æ¬¡ä»1å¼€å§‹
                                fact["has_history"] = len(history_text) > 0
                                fact["history_turns"] = len(history_turns)  # èŠå¤©å†å²çš„è½®æ•°
                            
                            all_facts.extend(turn_facts)
                    
                # å°†sessionè½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œç”¨äºè¿”å›
                chunk_text = parse_messages(session_or_text)
                return {"chunk_id": str(uuid.uuid4()), "chunk_text": chunk_text, "new_facts": all_facts, "timestamp": timestamp, "chat_history": chat_history}
            except Exception as e:
                print(f"æŒ‰è½®æ¬¡å¤„ç†sessionå¤±è´¥ï¼Œå›é€€åˆ°wholeæ¨¡å¼: {e}")
        
        # é»˜è®¤æ¨¡å¼ï¼šå¯¹æ•´ä¸ªsessionæˆ–æ–‡æœ¬è¿›è¡Œæå–ï¼Œä¼ é€’timestampå‚æ•°
        if isinstance(session_or_text, list):
            chunk_text = parse_messages(session_or_text)
        else:
            chunk_text = session_or_text
            
        facts = self._extract_single_turn(chunk_text, timestamp)
        return {"chunk_id": str(uuid.uuid4()), "chunk_text": chunk_text, "new_facts": facts, "timestamp": timestamp, "chat_history": [chunk_text]}
    
    def _extract_single_turn(self, text: str, timestamp: int = None, chat_history: str = "") -> List[Dict]:
        """
        å¯¹å•ä¸ªæ–‡æœ¬ç‰‡æ®µæå–äº‹å®
        
        Args:
            text: è¦æå–äº‹å®çš„æ–‡æœ¬
            timestamp: æ—¶é—´æˆ³ï¼Œå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å½“å‰æ—¶é—´
            chat_history: ä¹‹å‰çš„å¯¹è¯å†å²ï¼Œç”¨äºæä¾›ä¸Šä¸‹æ–‡
            
        Returns:
            æå–åˆ°çš„äº‹å®åˆ—è¡¨
        """
        try:
            # å°†timestampè½¬æ¢ä¸ºYYYY-MM-DDæ ¼å¼çš„æ—¥æœŸå­—ç¬¦ä¸²
            if timestamp is None:
                today_date = datetime.now(timezone.utc).strftime("%Y-%m-%d")
            else:
                today_date = datetime.fromtimestamp(timestamp, timezone.utc).strftime("%Y-%m-%d")
            
            # æ›¿æ¢promptä¸­çš„today_dateå ä½ç¬¦
            # ä¼˜å…ˆä½¿ç”¨å¸¦å†å²çš„promptï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨åŸprompt
            prompt = getattr(self, 'MEMREADER_PROMPT_WITH_HISTORY', MEMREADER_PROMPT)
            formatted_prompt = prompt.format(today_date=today_date)
            
            # æ„å»ºç”¨æˆ·è¾“å…¥ï¼ŒåŒ…å«chat historyå’Œå½“å‰å¯¹è¯
            user_input = ""
            if chat_history:
                user_input += f"Previous Chat History:\n{chat_history}\n\n"
            user_input += f"Current Conversation Turn:\n{text}"
            
            # æ£€æŸ¥ user_input æ˜¯å¦ä¸ºç©ºï¼Œé¿å… API æŠ¥é”™
            if not user_input.strip():
                print("âš ï¸ Warning: user_input is empty, skipping extraction.")
                return []
                
            # è°ƒè¯•ï¼šæ‰“å° prompt å’Œ user_input çš„é•¿åº¦ï¼Œç¡®è®¤å†…å®¹ä¸ä¸ºç©º
            # print(f"DEBUG: formatted_prompt length: {len(formatted_prompt)}")
            # print(f"DEBUG: user_input length: {len(user_input)}")
            
            # å†æ¬¡æ£€æŸ¥ formatted_prompt æ˜¯å¦ä¸ºç©º
            if not formatted_prompt.strip():
                print("âš ï¸ Warning: formatted_prompt is empty, skipping extraction.")
                return []
            
            max_retries = 3
            fact_objects = []
            
            for attempt in range(max_retries):
                try:
                    response = llm_client.chat.completions.create(
                        # model="gemini-3-pro-preview",
                        model="gpt-4o-mini",
                        # model="gpt-4o",
                        messages=[
                                {"role": "system", "content": formatted_prompt}, 
                                {"role": "user", "content": user_input}],
                        response_format={"type": "json_object"}, temperature=0
                    )
                    raw_content = response.choices[0].message.content
                    
                    if not raw_content:
                        print(f"âš ï¸ Attempt {attempt + 1}/{max_retries}: Received empty response from LLM.")
                        if attempt < max_retries - 1:
                            time.sleep(2)
                            continue
                        else:
                            raise ValueError("Received empty response from LLM after max retries")

                    json_str = extract_json(raw_content)
                    fact_objects = json.loads(json_str).get("facts", [])
                    break  # Success, exit loop
                    
                except json.JSONDecodeError:
                    print(f"âš ï¸ Attempt {attempt + 1}/{max_retries}: JSON Decode Error. Raw content: {raw_content}")
                    if attempt < max_retries - 1:
                        time.sleep(2)
                        continue
                    else:
                        raise
                except Exception as e:
                    import traceback
                    print(f"âš ï¸ Attempt {attempt + 1}/{max_retries}: API Error: {e}")
                    traceback.print_exc()
                    if attempt < max_retries - 1:
                        time.sleep(2)
                        continue
                    else:
                        raise
            
            # ä¿ç•™å®Œæ•´çš„factå¯¹è±¡ï¼ŒåŒ…æ‹¬detailsä¿¡æ¯
            facts = []
            for fact_obj in fact_objects:
                if fact_obj.get("fact"):
                    facts.append({
                        "text": fact_obj.get("fact", ""),
                        "details": fact_obj.get("details", []),
                        "timestamp": timestamp,  # æ·»åŠ æ—¶é—´æˆ³
                        "chat_history_length": len(chat_history.split("\n")) if chat_history else 0  # æ·»åŠ å†å²é•¿åº¦
                    })
        except Exception as e: 
            print(f"Extraction failed: {e}")
            facts = [{"text": text, "details": [], "timestamp": timestamp, "chat_history_length": len(chat_history.split("\n")) if chat_history else 0}]
        return facts

    # --- Step 2: Retrieve ---    
    def step_retrieve(self, extract_result: Dict, limit: int = 3, user_id: str = 'default', similarity_threshold: float = None) -> List[Dict]:
        new_facts = extract_result['new_facts']
        if not new_facts: return []
        
        print(f"ğŸ” [2. Retrieve] Searching Memories & Facts for {len(new_facts)} facts...")
        context_bundles = []

        for fact in new_facts:
            query_vec = get_embedding(fact['text'])
            
            # 1. æ£€ç´¢ç›¸å…³è®°å¿† (Candidates)
            res_mem = self.client.search(
                self.semantic_col, [query_vec], filter=f"status == 'active' and user_id == '{user_id}'", limit=limit,
                output_fields=["content", "memory_id", "created_at"],
                similarity_threshold=similarity_threshold
            )
            candidates = []
            if res_mem and res_mem[0]:
                for hit in res_mem[0]:
                    candidates.append(hit['entity'])
            
            # 2. ğŸŒŸ ç›´æ¥ä» fact_col æ£€ç´¢ç›¸å…³äº‹å® (Related Facts)
            # ä¸å†ä¾èµ– memory-fact çš„å…³è”ï¼Œæ”¹ä¸ºè¯­ä¹‰æ£€ç´¢äº‹å®
            res_fact = self.client.search(
                self.fact_col, [query_vec], filter=f"user_id == '{user_id}'", limit=limit,
                output_fields=["fact_id", "text", "timestamp", "details"],
                similarity_threshold=similarity_threshold
            )
            related_facts = []
            if res_fact and res_fact[0]:
                for hit in res_fact[0]:
                    # æ£€æŸ¥æ˜¯å¦ä¸æ˜¯ Status: Archived
                    entity = hit['entity']
                    details = entity.get('details', [])
                    if "Status: Archived" not in details:
                        related_facts.append(entity)
            
            context_bundles.append({
                "new_fact": fact,
                "candidates": candidates,
                "related_facts": related_facts  # è¿™é‡Œçš„ related_facts æ˜¯ç›´æ¥æ£€ç´¢å‡ºæ¥çš„
            })
            
        return context_bundles

    # --- Step 3: Decide (With ID Mapping) ---
    def step_decide(self, extract_result: Dict, context_bundles: List[Dict], user_id: str = 'default', training_mode: bool = False) -> List[Dict]:
        all_new_facts = extract_result['new_facts']
        
        # 1. åˆå¹¶å»é‡ Candidates & Related Facts
        temp_mem_storage = {}
        related_facts_storage = {}
        
        for bundle in context_bundles:
            # åˆå¹¶è®°å¿†å€™é€‰
            for mem in bundle['candidates']:
                temp_mem_storage[mem['memory_id']] = mem
            
            # åˆå¹¶äº‹å®å€™é€‰ (ç›´æ¥ä» step_retrieve æ£€ç´¢å‡ºæ¥çš„)
            for fact in bundle.get('related_facts', []):
                related_facts_storage[fact['fact_id']] = fact
        
        unique_memories_list = list(temp_mem_storage.values())
        unique_related_facts = list(related_facts_storage.values())
        
        if not training_mode:
            print(f"ğŸ§  [3. Manager] Global Decide: {len(all_new_facts)} new facts, {len(unique_memories_list)} memories, {len(unique_related_facts)} related facts.")

        # ğŸŒŸ 2. æ„é€  ID æ˜ å°„ (Mapping Logic)
        uuid_mapping = {}  # { "0": "real-uuid", "1": "real-uuid" }
        candidates_str = ""

        if not unique_memories_list:
            candidates_str = "(No relevant memories found. Treat as new topic.)"
        else:
            for idx, mem in enumerate(unique_memories_list):
                simple_id = str(idx)
                real_uuid = mem['memory_id']
                uuid_mapping[simple_id] = real_uuid
                candidates_str += f"[Memory Item ID: {simple_id}]\n- Content: {mem['content']}\n\n"
                # ğŸŒŸ æ³¨æ„ï¼šè¿™é‡Œä¸å†å±•ç¤º Related Factsï¼Œå› ä¸ºå®ƒä»¬ä¸å†å…³è”

        # æ„é€  Fact Manager çš„ Retrieved Facts å­—ç¬¦ä¸²
        retrieved_facts_str = ""
        fact_uuid_mapping = {}
        for idx, fact in enumerate(unique_related_facts):
            simple_id = str(idx)
            fact_uuid_mapping[simple_id] = fact['fact_id']
            
            # æ ¼å¼åŒ–æ—¶é—´æˆ³
            ts = fact.get('timestamp')
            date_str = "Unknown Date"
            if ts:
                date_str = datetime.fromtimestamp(ts, timezone.utc).strftime("%Y-%m-%d")
            
            fact_str = f"{date_str}: {fact['text']}"
            if fact.get('details'):
                details_content = fact['details']
                if isinstance(details_content, list):
                    details_str = "; ".join([str(d) for d in details_content])
                else:
                    details_str = str(details_content)
                fact_str += f" ({details_str})"
            
            retrieved_facts_str += f"[Fact ID: {simple_id}]\n- Content: {fact_str}\n\n"

        # 3. å‡†å¤‡ Prompt è¾“å…¥
        # æ„å»ºåŒ…å«timestampå’Œdetailsçš„factsåˆ—è¡¨
        formatted_facts = []
        for fact in all_new_facts:
            # æ ¼å¼åŒ–æ—¶é—´æˆ³
            ts = fact.get('timestamp')
            date_str = "Unknown Date"
            if ts:
                date_str = datetime.fromtimestamp(ts, timezone.utc).strftime("%Y-%m-%d")
            
            fact_str = f"{date_str}: {fact['text']}"
            
            if fact.get('details'):
                details_content = fact['details']
                if isinstance(details_content, list):
                    details_str = "; ".join([str(d) for d in details_content])
                else:
                    details_str = str(details_content)
                fact_str += f" ({details_str})"
            formatted_facts.append(fact_str)
            
        new_facts_json = json.dumps(formatted_facts, ensure_ascii=False)
        
        # A. Fact Manager Input
        fact_user_content = f"""
        [New Facts Stream]
        {new_facts_json}
        
        [Retrieved Facts]
        {retrieved_facts_str}
        """
        
        # B. Memory Manager Input (Existing)
        memory_user_content = f"""
        [New Facts Stream]
        {new_facts_json}
        
        [EXISTING MEMORIES]
        {candidates_str}
        """
        
        # C. Core Memory Manager Input
        core_memory_user_content = f"""
        [New Facts Stream]
        {new_facts_json}
        
        [Retrieved Memories]
        {candidates_str}

        [Current Core Memory Stats]
        - Length: {len(self.core_memory)} characters
        - Limit: 5000 characters (Rewrite recommended if exceeded)

        [Old Core Memory]
        {self.core_memory}
        """

        # 4. å®šä¹‰å¹¶è¡Œè°ƒç”¨çš„å‡½æ•°
        def call_agent(system_prompt, user_content, tools, tool_choice="required"):
            try:
                response = llm_client.chat.completions.create(
                    # model="gpt-4o",
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_content}
                    ],
                    tools=tools,
                    tool_choice=tool_choice,
                    temperature=0
                )
                if not response.choices or not response.choices[0].message.tool_calls:
                    return []
                return response.choices[0].message.tool_calls
            except Exception as e:
                if not training_mode:
                    print(f"   âš ï¸ Agent Call Error: {e}")
                return []

        # 5. å¹¶è¡Œæ‰§è¡Œ
        all_decisions = []
        
        if not training_mode:
            print("   ğŸš€ Launching 3 parallel Memory Agents...")
            
        with ThreadPoolExecutor(max_workers=3) as executor:
            future_fact = executor.submit(call_agent, FACT_MANAGER_PROMPT, fact_user_content, FACT_TOOLS)
            future_mem = executor.submit(call_agent, MEMORY_MANAGER_PROMPT, memory_user_content, MEMORY_TOOLS)
            future_core = executor.submit(call_agent, CORE_MEMORY_MANAGER_PROMPT, core_memory_user_content, CORE_MEMORY_TOOLS)
            
            fact_calls = future_fact.result()
            mem_calls = future_mem.result()
            core_calls = future_core.result()

        # 6. è§£æç»“æœ
        
        # è§£æ Fact Manager ç»“æœ
        for tool_call in fact_calls:
            try:
                func_name = tool_call.function.name
                args = json.loads(tool_call.function.arguments)
                decision = {"action": "NOOP", "user_id": user_id, "source": "fact_manager"}
                
                if func_name == "fact_add":
                    decision.update({"action": "FACT_ADD", "summary": args.get("content", ""), "details": args.get("details", [])})
                elif func_name == "fact_trajectorize":
                     related_simple_ids = args.get("related_fact_ids", [])
                     real_related_ids = [fact_uuid_mapping.get(sid) for sid in related_simple_ids if fact_uuid_mapping.get(sid)]
                     
                     decision.update({
                         "action": "FACT_TRAJECTORIZE",
                         "related_fact_ids": real_related_ids,
                         "content": args.get("content", "")
                     })
                
                if decision["action"] != "NOOP":
                    all_decisions.append(decision)
            except Exception as e:
                if not training_mode: print(f"Error parsing fact tool: {e}")

        # è§£æ Core Memory Manager ç»“æœ
        for tool_call in core_calls:
            try:
                func_name = tool_call.function.name
                args = json.loads(tool_call.function.arguments)
                decision = {"action": "NOOP", "user_id": user_id, "source": "core_memory_manager"}
                
                if func_name == "core_memory_add":
                    decision.update({"action": "CORE_MEMORY_ADD", "content": args.get("content", "")})
                elif func_name == "core_memory_update":
                    decision.update({"action": "CORE_MEMORY_UPDATE", "old_text": args.get("old_text", ""), "new_text": args.get("new_text", "")})
                elif func_name == "core_memory_rewrite":
                    decision.update({"action": "CORE_MEMORY_REWRITE", "new_block_content": args.get("new_block_content", "")})
                
                if decision["action"] != "NOOP":
                    all_decisions.append(decision)
            except Exception as e:
                if not training_mode: print(f"Error parsing core memory tool: {e}")

        # è§£æ Memory Manager ç»“æœ (Original Logic)
        def resolve_id(simple_id):
            real = uuid_mapping.get(str(simple_id))
            if not real and not training_mode:
                print(f"   âš ï¸ Warning: LLM hallucinated ID '{simple_id}', ignoring.")
            return real

        for tool_call in mem_calls:
            try:
                func_name = tool_call.function.name
                args = json.loads(tool_call.function.arguments)
                
                if not training_mode:
                    print(f"   ğŸ¤– Raw Action: {func_name} | Args: {args}")
                decision = {"action": "NOOP"}

                if func_name == "create_memory":
                    decision.update({
                        "action": "ADD", 
                        "summary": args.get("content", ""), 
                        "user_id": user_id
                    })
                
                elif func_name == "update_memory":
                    if "target_memory_id" in args:
                        real_tid = resolve_id(args["target_memory_id"])
                        if real_tid:
                            orig_created = temp_mem_storage.get(real_tid, {}).get('created_at', int(time.time()))
                            decision.update({
                                "action": "UPDATE", 
                                "target_id": real_tid, 
                                "new_content": args.get("new_content", ""), 
                                "orig_created": orig_created,
                                "user_id": user_id
                            })

                elif func_name == "delete_memory":
                    if "target_memory_id" in args:
                        real_tid = resolve_id(args["target_memory_id"])
                        if real_tid:
                            orig_created = temp_mem_storage.get(real_tid, {}).get('created_at', int(time.time()))
                            decision.update({
                                "action": "DELETE", 
                                "target_id": real_tid, 
                                "orig_created": orig_created,
                                "user_id": user_id
                            })

                elif func_name == "infer_memory":
                    if "source_memory_ids" in args:
                        source_simples = args["source_memory_ids"]
                        # ç¡®ä¿source_simplesæ˜¯åˆ—è¡¨
                        if not isinstance(source_simples, list):
                            source_simples = [source_simples]
                        real_source_ids = [resolve_id(sid) for sid in source_simples if resolve_id(sid)]
                        if real_source_ids:
                            decision.update({
                                "action": "INFER", 
                                "source_ids": real_source_ids, 
                                "summary": args.get("inference_content", ""), 
                                "user_id": user_id
                            })

                elif func_name == "no_operation":
                    decision.update({"reason": args.get("reason", "No reason provided"), "user_id": user_id})
                
                if decision["action"] != "NOOP" or "reason" in decision:
                    all_decisions.append(decision)
            except Exception as e:
                if not training_mode:
                    print(f"   âš ï¸ Error processing tool call: {e}")
                continue
        
        return all_decisions
        
    # --- Batch Processing for Training with GRPO Support ---
    def batch_process(self, batch_data: List[Dict], user_id: str = 'default', grpo_compatible: bool = True) -> List[Dict]:
        """
        Batch processing for memory management training with GRPO compatibility.
        
        Args:
            batch_data (List[Dict]): List of input data for batch processing.
            user_id (str, optional): User ID for memory operations. Defaults to 'default'.
            grpo_compatible (bool, optional): Whether to return GRPO-compatible format. Defaults to True.
            
        Returns:
            List[Dict]: List of results for each input in the batch.
        """
        results = []
        
        for data in batch_data:
            # Extract facts from input text
            extract_result = self.step_extract(data['text'], extract_mode='whole')
            
            # Retrieve relevant memories
            context_bundles = self.step_retrieve(extract_result, limit=3, user_id=user_id)
            
            # Make decisions (memory operations) in training mode
            decisions = self.step_decide(extract_result, context_bundles, user_id=user_id, training_mode=True)
            
            # Execute decisions
            self.step_execute(decisions, extract_result, user_id=user_id)
            
            if grpo_compatible:
                # Format result for GRPO training
                result = {
                    'input': data['text'],
                    'extract_result': extract_result,
                    'decisions': decisions,
                    # Add GRPO-specific fields
                    'memory_operations': [d['action'] for d in decisions if d['action'] != 'NOOP'],
                    'memory_contents': [d.get('summary', '') for d in decisions if d['action'] != 'NOOP'],
                    # Ensure we have the expected_operation if provided in data
                    'expected_operation': data.get('expected_operation', '')
                }
            else:
                # Standard format for non-GRPO training
                result = {
                    'input': data['text'],
                    'extract_result': extract_result,
                    'decisions': decisions
                }
            
            results.append(result)
        
        return results

    # --- Step 4: Execute ---
    def step_execute(self, decisions: List[Dict], extract_result: Dict, user_id: str = 'default'):
        # ä½¿ç”¨extract_resultä¸­çš„timestampå’Œchunk_id
        ts = extract_result['timestamp']
        chunk_id = extract_result['chunk_id']
        all_new_facts = extract_result['new_facts']

        if not decisions:
            # å¦‚æœæ²¡æœ‰å†³ç­–ï¼Œç¡®ä¿æ–°äº‹å®ä¾ç„¶è¢«ä¿å­˜
            if all_new_facts:
                self._save_facts(all_new_facts, ts, chunk_id, user_id)
            return

        has_non_noop_action = False
        
        for decision in decisions:
            action = decision.get("action")
            if action == "NOOP":
                self.operation_counts["NOOP"] += 1
                continue

            has_non_noop_action = True

            # --- Memory Operations (Case 1-4) ---
            if action == "ADD":
                self.operation_counts["ADD"] += 1
                target_mem_id = str(uuid.uuid4())
                self._upsert_mem(target_mem_id, decision['summary'], ts, ts, "active", [], decision.get('user_id', 'default'))
                print(f"   âœ… Created Mem: {target_mem_id[:8]}... | Content: {decision['summary']}")

            elif action == "UPDATE":
                self.operation_counts["UPDATE"] += 1
                target_mem_id = decision['target_id']
                
                # æŸ¥è¯¢æ—§çš„memoryå†…å®¹ç”¨äºæ‰“å°
                old_memories = self.client.query(
                    collection_name=self.semantic_col,
                    filter=f"memory_id == '{target_mem_id}'",
                    output_fields=["content"]
                )
                old_content = "" if not old_memories else old_memories[0].get("content", "")
                
                self._upsert_mem(target_mem_id, decision['new_content'], decision['orig_created'], ts, "active", [], decision.get('user_id', 'default'))
                print(f"   ğŸ”„ Updated Mem: {target_mem_id[:8]}...")
                print(f"      Before: {old_content[:]}...")
                print(f"      After:  {decision['new_content'][:]}...")

            elif action == "DELETE":
                self.operation_counts["DELETE"] += 1
                target_mem_id = decision['target_id']
                self._upsert_mem(target_mem_id, "(Archived)", decision['orig_created'], ts, "archived", [], decision.get('user_id', 'default'))
                print(f"   âŒ Deleted Mem: {target_mem_id[:8]}...")

            elif action == "INFER":
                self.operation_counts["INFER"] += 1
                target_mem_id = str(uuid.uuid4())
                source_ids = decision.get('source_ids', [])
                
                # æŸ¥è¯¢ source memories ç”¨äºå±•ç¤º
                source_mems = []
                if source_ids:
                    quoted_source_ids = [f'"{sid}"' for sid in source_ids]
                    mem_filter = f"status == 'active' and memory_id in [{','.join(quoted_source_ids)}]"
                    try:
                        source_mems = self.client.query(
                            collection_name=self.semantic_col,
                            filter=mem_filter,
                            output_fields=["content", "memory_id"]
                        )
                    except Exception as e:
                        print(f"   âš ï¸ æŸ¥è¯¢source memoryå¤±è´¥: {e}")

                relations = [{"type": "inferred_from", "target_id": sid} for sid in source_ids]
                self._upsert_mem(target_mem_id, decision['summary'], ts, ts, "active", relations, decision.get('user_id', 'default'))
                
                # æ‰“å°è¯¦ç»†çš„ Infer è¿‡ç¨‹
                print(f"   ğŸ’¡ Inferred Mem: {target_mem_id[:8]}... | From: {[s[:8] for s in source_ids]}")
                print(f"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
                if source_mems:
                    print(f"   â”‚ ğŸ“‹ Infer å‰çš„ Memory ({len(source_mems)}ä¸ª):")
                    for mem in source_mems:
                        print(f"   â”‚      ğŸ“Œ ID: {mem['memory_id'][:8]}... | å†…å®¹: {mem['content'][:]}...")
                print(f"   â”‚ ğŸ“ Inferç”Ÿæˆçš„ Memory:")
                print(f"   â”‚      ğŸ“Œ ID: {target_mem_id[:8]}... | å†…å®¹: {decision['summary'][:]}...")
                print(f"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

            # --- Fact Operations (Case 5-6) ---
            elif action == "FACT_ADD":
                self.operation_counts["ADD"] += 1 
                print(f"   ğŸ†• Fact Added: {decision['summary']}")

            elif action == "FACT_TRAJECTORIZE":
                self.operation_counts["UPDATE"] += 1
                content = decision['content']
                related_fact_ids = decision.get('related_fact_ids', [])
                
                print(f"   ğŸ“ˆ Fact Trajectory: {content[:]}...")
                print(f"      Archiving {len(related_fact_ids)} facts...")

                # 1. Archive old facts
                if related_fact_ids:
                    for fid in related_fact_ids:
                        try:
                            facts = self.client.query(
                                collection_name=self.fact_col,
                                filter=f'fact_id == "{fid}"',
                                output_fields=["fact_id", "details", "text", "timestamp", "user_id"]
                            )
                            if facts:
                                fact = facts[0]
                                details = fact.get('details', [])
                                if isinstance(details, list):
                                    if "Status: Archived" not in details:
                                        details.append("Status: Archived")
                                        details.append(f"Archived Reason: Trajectorized into {content[:]}...")
                                
                                self.client.upsert(self.fact_col, [{
                                    "fact_id": fid,
                                    "linked_chunk_id": fact.get('linked_chunk_id', chunk_id),
                                    "text": fact['text'],
                                    "details": details,
                                    "timestamp": fact['timestamp'],
                                    "user_id": fact.get('user_id', user_id),
                                    "embedding": self._generate_fact_embedding(fact['text'], details)
                                }])
                        except Exception as e:
                            print(f"Error archiving fact {fid}: {e}")

                # 2. Create new Trajectory Fact
                traj_fact_id = str(uuid.uuid4())
                traj_details = ["Type: Trajectory"]
                self.client.upsert(self.fact_col, [{
                    "fact_id": traj_fact_id,
                    "linked_chunk_id": chunk_id,
                    "text": content,
                    "details": traj_details,
                    "timestamp": ts,
                    "user_id": user_id,
                    "embedding": self._generate_fact_embedding(content, traj_details)
                }])

            # --- Core Memory Operations (Case 7) ---
            elif action == "CORE_MEMORY_ADD":
                content = decision['content']
                self.core_memory += f"\n{content}"
                print(f"   ğŸ§  Core Memory ADD: {content[:]}...")

            elif action == "CORE_MEMORY_UPDATE":
                old_text = decision['old_text'].strip()
                new_text = decision['new_text'].strip()
                
                # å°è¯•ç²¾ç¡®åŒ¹é…ï¼ˆå¿½ç•¥é¦–å°¾ç©ºæ ¼ï¼‰
                if old_text in self.core_memory:
                    self.core_memory = self.core_memory.replace(old_text, new_text)
                    print(f"   ğŸ§  Core Memory UPDATE: {old_text[:]}... -> {new_text[:]}...")
                else:
                    # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼šå¿½ç•¥æ ‡ç‚¹ç¬¦å·å’Œç©ºç™½å­—ç¬¦
                    import re
                    def normalize(t):
                        return re.sub(r'[^\w\s]', '', t).strip()
                    
                    normalized_core = normalize(self.core_memory)
                    normalized_old = normalize(old_text)
                    
                    if normalized_old in normalized_core:
                        # å¦‚æœèƒ½æ¨¡ç³ŠåŒ¹é…åˆ°ï¼Œå°è¯•åœ¨åŸæ–‡æœ¬ä¸­æ‰¾åˆ°å¯¹åº”çš„åŸå§‹æ–‡æœ¬æ®µ
                        # è¿™é‡Œç®€å•å¤„ç†ï¼šå¦‚æœæ¨¡ç³ŠåŒ¹é…æˆåŠŸä½†ç²¾ç¡®å¤±è´¥ï¼Œæ‰“å°æç¤º
                        print(f"   âš ï¸ Core Memory Update: Exact match failed, but fuzzy match possible. Please use rewrite if update fails.")
                    
                    print(f"   âš ï¸ Core Memory Update Failed: Old text not found.")

            elif action == "CORE_MEMORY_REWRITE":
                new_block = decision['new_block_content']
                self.core_memory = new_block
                print(f"   ğŸ§  Core Memory REWRITE.")

        # --- Final Step: Save ALL new facts (independent of memories) ---
        if all_new_facts:
            self._save_facts(all_new_facts, ts, chunk_id, user_id)

    def _save_facts(self, facts: List[Dict], ts: int, chunk_id: str, user_id: str):
        """ä¿å­˜äº‹å®åˆ°æ•°æ®åº“ï¼Œä¸è¿›è¡Œè®°å¿†å…³è”"""
        rows = []
        for fact in facts:
            fact_id = fact.get('fact_id', str(uuid.uuid4()))
            rows.append({
                "fact_id": fact_id,
                "linked_chunk_id": chunk_id,
                "text": fact['text'],
                "details": fact['details'],
                "timestamp": ts,
                "user_id": user_id,
                "embedding": self._generate_fact_embedding(fact['text'], fact['details'])
            })
        if rows:
            self.client.upsert(self.fact_col, rows)
            print(f"   ğŸ’¾ Saved {len(rows)} facts to database (independent).")

    def _upsert_mem(self, mem_id, content, c_at, u_at, status, relations, user_id):
        self.client.upsert(self.semantic_col, [{
            "memory_id": mem_id,
            "embedding": get_embedding(content),
            "content": content,
            "user_id": user_id,
            "status": status,
            "created_at": c_at,
            "updated_at": u_at,
            "relations": relations
        }])

    def step_preprocess_facts(self, extract_result: Dict, user_id: str = 'default') -> Dict:
        """
        é¢„å¤„ç†æå–å‡ºçš„äº‹å®ï¼Œæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨äºæ•°æ®åº“ä¸­ï¼Œç¡®ä¿ä»æºå¤´ä¸Šå»é‡
        
        Args:
            extract_result: æå–ç»“æœå­—å…¸ï¼ŒåŒ…å«new_facts
            user_id: ç”¨æˆ·æ ‡è¯†ï¼Œç¡®ä¿åªå¤„ç†å½“å‰ç”¨æˆ·çš„äº‹å®
            
        Returns:
            æ›´æ–°åçš„æå–ç»“æœå­—å…¸ï¼ŒåŒ…å«fact_idä¿¡æ¯
        """
        new_facts = extract_result['new_facts']
        processed_facts = []
        ts = extract_result['timestamp']
        chunk_id = extract_result['chunk_id']
        
        print(f"ğŸ” [Preprocess Facts] æ£€æŸ¥ {len(new_facts)} ä¸ªäº‹å®æ˜¯å¦å·²å­˜åœ¨...")
        
        # 1. å…ˆå¯¹åŒä¸€æ‰¹æ¬¡å†…çš„äº‹å®è¿›è¡Œå»é‡ï¼Œé¿å…åŒä¸€æ‰¹æ¬¡ä¸­é‡å¤çš„äº‹å®è¢«å¤„ç†
        unique_facts_in_batch = []
        seen_fact_keys = set()
        for fact in new_facts:
            # ä½¿ç”¨fact_textå’Œdetailsçš„ç»„åˆä½œä¸ºå”¯ä¸€æ ‡è¯†
            fact_key = f"{fact['text']}::{json.dumps(fact['details'], sort_keys=True)}"
            if fact_key not in seen_fact_keys:
                seen_fact_keys.add(fact_key)
                unique_facts_in_batch.append(fact)
        
        if len(unique_facts_in_batch) < len(new_facts):
            print(f"   âœ… åŒä¸€æ‰¹æ¬¡å†…å»é‡ {len(new_facts) - len(unique_facts_in_batch)} ä¸ªé‡å¤äº‹å®")
        
        for fact in unique_facts_in_batch:
            fact_text = fact['text']
            fact_details = fact['details']

            
            # 3. æŸ¥è¯¢æ•°æ®åº“ä¸­æ˜¯å¦å­˜åœ¨ç›¸åŒçš„fact
            existing_fact = None
            try:
                # å…ˆå°è¯•æœç´¢ç›¸å…³äº‹å®ï¼Œé¿å…å…¨é‡æŸ¥è¯¢
                # ä½¿ç”¨æ›´å®‰å…¨çš„æŸ¥è¯¢æ–¹å¼ï¼ŒåŸºäºtextçš„å‰ç¼€åŒ¹é…
                # åªæŸ¥è¯¢textå­—æ®µåŒ…å«fact_textå…³é”®è¯çš„äº‹å®
                search_vec = get_embedding(fact_text)
                search_results = self.client.search(
                    self.fact_col, [search_vec], 
                    output_fields=["fact_id", "details", "timestamp", "linked_chunk_id", "text"],
                    limit=20,  # åªæŸ¥è¯¢å‰20ä¸ªæœ€ç›¸ä¼¼çš„äº‹å®
                    similarity_threshold=0.8  # è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œåªè¿”å›ç›¸ä¼¼åº¦è¾ƒé«˜çš„äº‹å®
                )
                
                # å¤„ç†æœç´¢ç»“æœï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å®Œå…¨åŒ¹é…çš„äº‹å®
                if search_results and search_results[0]:
                    for hit in search_results[0]:
                        res = hit['entity']
                        res_text = res.get("text", "")
                        res_details = res.get("details", [])
                        # æ£€æŸ¥æ˜¯å¦æ˜¯ç›¸åŒçš„äº‹å®ï¼Œè€ƒè™‘åˆ°è¡¨è¿°å¯èƒ½ç•¥æœ‰ä¸åŒ
                        # 1. å®Œå…¨ç›¸åŒçš„æƒ…å†µ
                        if res_text == fact_text and res_details == fact_details:
                            existing_fact = res
                            break
                        # 2. æ ¸å¿ƒå†…å®¹ç›¸åŒä½†è¡¨è¿°ç•¥æœ‰ä¸åŒçš„æƒ…å†µï¼ˆå¦‚æœ‰æ— "User"å‰ç¼€ï¼‰
                        stripped_res_text = res_text.lower().replace("user ", "").strip()
                        stripped_fact_text = fact_text.lower().replace("user ", "").strip()
                        if stripped_res_text == stripped_fact_text and res_details == fact_details:
                            existing_fact = res
                            break
            
            except Exception as e:
                print(f"   âš ï¸ æŸ¥è¯¢äº‹å®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            
            if existing_fact:
                # äº‹å®å·²å­˜åœ¨ï¼Œæ›´æ–°timestamp
                fact_id = existing_fact["fact_id"]
                old_ts = existing_fact["timestamp"]
                
                # è·å–ç°æœ‰çš„linked_chunk_id
                existing_chunk = existing_fact.get("linked_chunk_id", "")
                
                # æ›´æ–°timestampå’Œå…³è”ä¿¡æ¯
                self.client.upsert(self.fact_col, [{
                    "fact_id": fact_id,
                    "linked_chunk_id": existing_chunk,
                    "text": fact_text,
                    "details": fact_details,
                    "timestamp": ts,
                    "user_id": user_id,
                    "embedding": self._generate_fact_embedding(fact_text, fact_details)
                }])
                
                # å°†ç°æœ‰äº‹å®æ·»åŠ åˆ°processed_facts
                processed_fact = {
                    "text": fact_text,
                    "details": fact_details,
                    "fact_id": fact_id,
                    "timestamp": ts  # ğŸŒŸ å¿…é¡»åŒ…å« timestamp
                }
                processed_facts.append(processed_fact)
                
                print(f"   ğŸ”„ äº‹å®å·²å­˜åœ¨ï¼Œæ›´æ–°timestamp: {fact_id} (æ—§: {old_ts}, æ–°: {ts})")
            else:
                # äº‹å®ä¸å­˜åœ¨ï¼Œç”Ÿæˆæ–°çš„fact_idå¹¶ä¿å­˜
                fact_id = str(uuid.uuid4())
                # print(f"   ğŸ†• æ–°äº‹å®: {fact_id}")
                
                # ä¿å­˜æ–°äº‹å®åˆ°æ•°æ®åº“
                self.client.upsert(self.fact_col, [{
                    "fact_id": fact_id,
                    "linked_chunk_id": chunk_id,
                    "text": fact_text,
                    "details": fact_details,
                    "timestamp": ts,
                    "user_id": user_id,
                    "embedding": self._generate_fact_embedding(fact_text, fact_details)
                }])
                
                processed_fact = {
                    "text": fact_text,
                    "details": fact_details,
                    "fact_id": fact_id,
                    "timestamp": ts  # ğŸŒŸ å¿…é¡»åŒ…å« timestamp
                }
                
                processed_facts.append(processed_fact)
        
        # æ›´æ–°æå–ç»“æœ
        extract_result['new_facts'] = processed_facts
        return extract_result
    
    def process(self, text, retrieve_limit: int = 3, extract_mode: str = "whole", user_id: str = 'default', similarity_threshold: float = None, timestamp: int = None, max_history_turns: int = 5):
        res = self.step_extract(text, extract_mode=extract_mode, timestamp=timestamp, max_history_turns=max_history_turns)
        if not res['new_facts']: return
        
        # é¢„å¤„ç†äº‹å®ï¼Œæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        res = self.step_preprocess_facts(res, user_id=user_id)
        
        # æ£€æŸ¥é¢„å¤„ç†åæ˜¯å¦è¿˜æœ‰æ–°äº‹å®
        if not res['new_facts']:
            print(f"   âœ… æ‰€æœ‰äº‹å®éƒ½å·²å­˜åœ¨ï¼Œæ— éœ€å¤„ç†")
            return
        
        print(f"   æ–°è¯æ®: {res['new_facts']}")
        
        ctx_bundles = self.step_retrieve(res, limit=retrieve_limit, user_id=user_id, similarity_threshold=similarity_threshold)
        decisions = self.step_decide(res, ctx_bundles, user_id=user_id)
        self.step_execute(decisions, res, user_id=user_id)
        
    def process_user_memory_infer(self, line, retrieve_limit: int = 3, extract_mode: str = "whole", user_id: str = 'default', similarity_threshold: float = None, max_history_turns: int = 5):
        """å¤„ç†ç”¨æˆ·è®°å¿†ä¼šè¯ï¼Œæ”¯æŒlongmemevalæ•°æ®é›†æ ¼å¼"""
        # é‡ç½®æ“ä½œè®¡æ•°ï¼Œç¡®ä¿æ¯ä¸ªç”¨æˆ·çš„è®¡æ•°ç‹¬ç«‹
        self.operation_counts = {"ADD": 0, "UPDATE": 0, "DELETE": 0, "INFER": 0, "NOOP": 0}
        dates = line.get("haystack_dates")
        sessions = line.get("haystack_sessions")

        for session_id, session in enumerate(sessions):
            date = dates[session_id] + " UTC"
            date_format = "%Y/%m/%d (%a) %H:%M UTC"
            date_string = datetime.strptime(date, date_format).replace(tzinfo=timezone.utc)
            # ç”Ÿæˆtimestamp
            timestamp = int(date_string.timestamp())
            
            print(f"å¤„ç†ä¼šè¯ {session_id + 1}/{len(sessions)}: {dates[session_id]}")
            
            # ç›´æ¥ä¼ é€’sessionå¯¹è±¡ç»™processæ–¹æ³•ï¼Œè€Œä¸æ˜¯è½¬æ¢ä¸ºæ–‡æœ¬
            # ä½¿ç”¨ç°æœ‰çš„processæ–¹æ³•å¤„ç†ä¼šè¯æ¶ˆæ¯ï¼Œä¼ é€’user_idã€similarity_thresholdå’Œtimestamp
            self.process(session, retrieve_limit=retrieve_limit, extract_mode=extract_mode, user_id=user_id, similarity_threshold=similarity_threshold, timestamp=timestamp, max_history_turns=max_history_turns)
        
        # è¿”å›æ“ä½œæ¬¡æ•°ç»Ÿè®¡
        return self.operation_counts
        
    def search_memories(self, query_text, top_k=5, fact_top_k=5, user_id: str = 'default', threshold: float = 0.0, similarity_threshold: float = None, enhanced_search: bool = False, use_fact_retrieval: bool = True):
        """æœç´¢è®°å¿†å¹¶è¿”å›æ¯ä¸ªè®°å¿†å…³è”çš„topkä¸ªäº‹å®ï¼Œå¹¶æ ¹æ®å…³è”äº‹å®è¿›è¡Œrerank
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›çš„è®°å¿†æ•°é‡ä¸Šé™
            fact_top_k: æ¯ä¸ªè®°å¿†å…³è”çš„äº‹å®æ•°é‡ä¸Šé™
            user_id: ç”¨æˆ·æ ‡è¯†ï¼Œç¡®ä¿åªæ£€ç´¢å½“å‰ç”¨æˆ·çš„è®°å¿†
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºè¯¥é˜ˆå€¼çš„è®°å¿†å°†è¢«è¿‡æ»¤æ‰
            similarity_threshold: å‘é‡æ•°æ®åº“æœç´¢æ—¶çš„ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºè¯¥é˜ˆå€¼çš„è®°å¿†å°†è¢«è¿‡æ»¤æ‰
            enhanced_search: æ˜¯å¦å¯ç”¨å¢å¼ºå‹æœç´¢æ¨¡å¼ï¼Œå¯ç”¨åä¼šå¢å¼ºreranké€»è¾‘
            use_fact_retrieval: æ˜¯å¦ä½¿ç”¨äº‹å®æ£€ç´¢æ¨¡å¼ï¼Œå¯ç”¨åä¼šæœç´¢äº‹å®é›†åˆå¹¶æ ¹æ®å…³è”çš„memory_idè·å–æ›´å¤šè®°å¿†
        """
        query_vec = get_embedding(query_text)
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        filter_expr = f"status == 'active' and user_id == '{user_id}'"
        print(f"   ğŸ” æœç´¢è¿‡æ»¤æ¡ä»¶: {filter_expr}, é˜ˆå€¼: {threshold}, å‘é‡æœç´¢é˜ˆå€¼: {similarity_threshold}")
        
        # ===========================
        # 1. æœç´¢è®°å¿†é›†åˆï¼Œè·å–memoryA
        # ===========================
        mem_res = self.client.search(
            self.semantic_col, [query_vec], filter=filter_expr, limit=top_k,  # æœç´¢æ›´å¤šè®°å¿†ï¼Œé¿å…é—æ¼
            output_fields=["content", "memory_id", "created_at", "user_id"],  # åŒ…å«user_idå­—æ®µç”¨äºè°ƒè¯•
            similarity_threshold=similarity_threshold
        )
        
        # ===========================
        # 2. æœç´¢äº‹å®é›†åˆï¼Œè·å–ç›¸å…³äº‹å®
        # ===========================
        combined_items = []  # å­˜å‚¨memoryå’ŒfactåŠå…¶åˆ†æ•°ï¼Œç”¨äºç»Ÿä¸€æ’åº
        memory_dict = {}  # ä¸´æ—¶å­˜å‚¨memoryå¯¹è±¡
        fact_dict = {}  # ä¸´æ—¶å­˜å‚¨factå¯¹è±¡
        
        # å…ˆå¤„ç†memoryA
        if mem_res and mem_res[0]:
            for hit in mem_res[0]:
                memory = hit['entity']
                memory_id = memory['memory_id']
                similarity_score = hit['distance']
                # ä¿å­˜ç›¸ä¼¼åº¦å¾—åˆ†
                memory["original_score"] = similarity_score
                memory_dict[memory_id] = memory
                # å°†memoryæ·»åŠ åˆ°combined_itemsä¸­ï¼Œç”¨äºç»Ÿä¸€æ’åº
                combined_items.append({
                    "type": "memory",
                    "item": memory,
                    "score": similarity_score,  # ä½¿ç”¨ç›¸ä¼¼åº¦ä½œä¸ºåˆ†æ•°
                    "memory_id": memory_id
                })
        
        # å¤„ç†fact
        if use_fact_retrieval:
            # æœç´¢äº‹å®é›†åˆ
            fact_res = self.client.search(
                self.fact_col, [query_vec], filter=f"user_id == '{user_id}'", limit=top_k,  # æœç´¢æ›´å¤šäº‹å®ï¼Œé¿å…é—æ¼
                output_fields=["text", "timestamp", "fact_id", "details", "user_id", "embedding"]  # æ·»åŠ embeddingå­—æ®µ
            )
            
            if fact_res and fact_res[0]:
                for hit in fact_res[0]:
                    fact = hit['entity']
                    fact_id = fact['fact_id']
                    # è®¡ç®—factä¸queryçš„å†…ç§¯
                    try:
                        # ç›´æ¥ä½¿ç”¨æ•°æ®åº“ä¸­å­˜å‚¨çš„embeddingï¼Œè€Œä¸æ˜¯é‡æ–°è®¡ç®—
                        fact_vec = fact.get("embedding")
                        if not fact_vec or not isinstance(fact_vec, list):
                            # å¦‚æœæ²¡æœ‰embeddingå­—æ®µæˆ–ä¸æ˜¯åˆ—è¡¨ï¼Œé‡æ–°è®¡ç®—ï¼Œä½¿ç”¨textå’Œdetailsæ‹¼æ¥
                            fact_vec = self._generate_fact_embedding(fact["text"], fact.get("details", []))
                    
                        fact_dot_product = sum(a * b for a, b in zip(query_vec, fact_vec))
                        fact["similarity"] = fact_dot_product
                        fact_dict[fact_id] = fact
                        
                        # å°†factæ·»åŠ åˆ°combined_itemsä¸­ï¼Œç”¨äºç»Ÿä¸€æ’åº
                        combined_items.append({
                            "type": "fact",
                            "item": fact,
                            "score": fact_dot_product,  # ä½¿ç”¨å†…ç§¯ä½œä¸ºåˆ†æ•°
                            "fact_id": fact_id
                        })
                    except Exception as e:
                        print(f"è®¡ç®—äº‹å®ç›¸å…³æ€§å¤±è´¥: {e}")
                        continue
        
        # ===========================
        # 3. åˆ†åˆ«å¯¹ Memory å’Œ Fact è¿›è¡Œæ’åºå¹¶å– TopK
        # ===========================
        # åˆ†ç¦»è®°å¿†å’Œäº‹å®
        memories_items = [item for item in combined_items if item["type"] == "memory"]
        facts_items = [item for item in combined_items if item["type"] == "fact"]
        
        # åˆ†åˆ«æŒ‰åˆ†æ•°é™åºæ’åº
        memories_items.sort(key=lambda x: x.get("score", 0), reverse=True)
        facts_items.sort(key=lambda x: x.get("score", 0), reverse=True)
        
        # å„å– top_k
        top_memories = memories_items[:top_k]
        top_facts = facts_items[:top_k]
        
        # æ„é€ æœ€ç»ˆç»“æœ
        results = []
        
        # å¤„ç† Top è®°å¿†
        for item in top_memories:
            memory = item["item"]
            memory["combined_score"] = item["score"]
            memory["type"] = "memory"
            results.append(memory)
            
        # å¤„ç† Top äº‹å®
        for item in top_facts:
            fact = item["item"]
            results.append({
                "memory_id": fact["fact_id"],
                "content": fact["text"],
                "original_score": item["score"],
                "combined_score": item["score"],
                "details": fact.get("details", []),
                "timestamp": fact.get("timestamp"),
                "created_at": fact.get("timestamp", int(time.time())), # å…¼å®¹ response_user çš„ created_at
                "type": "fact"
            })
        
        return results
    def _calculate_memory_score(self, memory, enhanced_search=False):
        """ç›´æ¥è¿”å›memoryä¸queryçš„å†…ç§¯ï¼Œä¸è€ƒè™‘å…³è”äº‹å®çš„ç›¸å…³æ€§"""
        original_score = memory.get("original_score", 0)
        # ç›´æ¥ä½¿ç”¨memoryä¸queryçš„å†…ç§¯ä½œä¸ºç»¼åˆåˆ†æ•°
        memory["combined_score"] = original_score
        return memory
        
    def _generate_fact_embedding(self, text, details):
        """ç”Ÿæˆäº‹å®çš„embeddingï¼Œå°†textå’Œdetailsæ‹¼æ¥èµ·æ¥
        
        Args:
            text: äº‹å®çš„æ–‡æœ¬
            details: äº‹å®çš„è¯¦ç»†ä¿¡æ¯ï¼Œç±»å‹ä¸ºåˆ—è¡¨
            
        Returns:
            ç”Ÿæˆçš„embeddingå‘é‡
        """
        # å°†detailsæ‹¼æ¥æˆå­—ç¬¦ä¸²
        details_str = ""
        if isinstance(details, list) and details:
            # éå†detailsåˆ—è¡¨ï¼Œå°†æ¯ä¸ªdetailsé¡¹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            for i, detail in enumerate(details):
                if isinstance(detail, dict):
                    # å¦‚æœdetailæ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸ºé”®å€¼å¯¹å­—ç¬¦ä¸²
                    detail_str = ", ".join([f"{k}: {v}" for k, v in detail.items()])
                    details_str += f"Detail {i+1}: {detail_str}\n"
                else:
                    # å¦åˆ™ç›´æ¥è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    details_str += f"Detail {i+1}: {str(detail)}\n"
        elif isinstance(details, dict):
            # å¦‚æœdetailsæ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸ºé”®å€¼å¯¹å­—ç¬¦ä¸²
            details_str = ", ".join([f"{k}: {v}" for k, v in details.items()])
        
        # å°†textå’Œdetailsæ‹¼æ¥æˆå®Œæ•´çš„æ–‡æœ¬
        if details_str:
            full_text = f"{text}\n\nDetails:\n{details_str.strip()}"
        else:
            full_text = text
        
        # ç”Ÿæˆembedding
        return get_embedding(full_text)
        
    def generate_response(self, question, question_date, context):
        """ç”Ÿæˆé—®é¢˜å“åº”"""
        prompt = LME_ANSWER_PROMPT.format(
            question=question,
            question_date=question_date,
            context=context
        )
        response = llm_client.chat.completions.create(
                    # model="gemini-3-pro-preview",
                    # model="gpt-4o",
                    model="gpt-4o-mini",
                    messages=[{"role": "system", "content": prompt}],
                    temperature=0,
                )
        
        return response

# ==========================================
# è¯„ä¼°ç›¸å…³å‡½æ•°
# ==========================================
def response_user(line, pipeline, retrieve_limit=20, max_facts_per_memory=3, user_id='default', threshold: float = 0.0, enhanced_search: bool = False):
    """å¤„ç†ç”¨æˆ·é—®é¢˜ï¼Œç”Ÿæˆå“åº”
    
    Args:
        line: åŒ…å«é—®é¢˜å’Œå…¶ä»–ä¿¡æ¯çš„å­—å…¸
        pipeline: MemoryPipelineå®ä¾‹
        retrieve_limit: æ£€ç´¢è®°å¿†çš„æ•°é‡é™åˆ¶
        max_facts_per_memory: æ¯ä¸ªè®°å¿†çš„äº‹å®æ•°é‡é™åˆ¶
        user_id: ç”¨æˆ·æ ‡è¯†ï¼Œç¡®ä¿åªæ£€ç´¢å½“å‰ç”¨æˆ·çš„è®°å¿†
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºè¯¥é˜ˆå€¼çš„è®°å¿†å°†è¢«è¿‡æ»¤æ‰
        enhanced_search: æ˜¯å¦å¯ç”¨å¢å¼ºå‹æœç´¢æ¨¡å¼ï¼Œå¯ç”¨åä¼šè°ƒå¤§topkå¹¶å¢å¼ºrerank
    """
    question = line.get("question")
    question_date = line.get("question_date")
    question_date = question_date + " UTC"
    question_date_format = "%Y/%m/%d (%a) %H:%M UTC"
    question_date_string = datetime.strptime(question_date, question_date_format).replace(tzinfo=timezone.utc)
    
    # å¢å¼ºå‹æœç´¢æ¨¡å¼ï¼šè°ƒå¤§topk
    if enhanced_search:
        # è°ƒå¤§åˆå§‹æ£€ç´¢æ•°é‡ï¼Œä¾‹å¦‚ä¹˜ä»¥2
        enhanced_top_k = retrieve_limit * 2
        print(f"   ğŸš€ å¯ç”¨å¢å¼ºå‹æœç´¢æ¨¡å¼ï¼Œåˆå§‹æ£€ç´¢æ•°é‡: {enhanced_top_k}")
    else:
        enhanced_top_k = retrieve_limit
    
    # æœç´¢è®°å¿†ï¼Œä¼ é€’user_idã€thresholdå’Œenhanced_searchå‚æ•°
    retrieved_memories = pipeline.search_memories(question, top_k=enhanced_top_k, user_id=user_id, threshold=threshold, enhanced_search=enhanced_search)
    
    # ç¡®ä¿retrieved_memoriesä¸æ˜¯None
    retrieved_memories = retrieved_memories or []
    
    # æ„å»ºä¸Šä¸‹æ–‡ï¼ŒåŒ…å«è®°å¿†å’Œå…³è”çš„äº‹å®
    memories_with_facts = []
    
    for mem in retrieved_memories:
        # æ ¹æ®ç±»å‹åŒºåˆ†æ˜¾ç¤º
        m_type = mem.get("type", "memory").upper()
        ts_str = datetime.fromtimestamp(mem['created_at'], timezone.utc).isoformat()
        
        # æ·»åŠ å†…å®¹
        item_line = f"- [{ts_str}] [{m_type}] {mem['content']}"
        memories_with_facts.append(item_line)
        
        # æ·»åŠ ç»†èŠ‚ï¼ˆé’ˆå¯¹ Factï¼‰
        details = mem.get("details", [])
        if details and m_type == "FACT":
            details_str = "; ".join(details)
            if len(details_str) > 150:
                details_str = details_str[:150] + "..."
            memories_with_facts.append(f"  â””â”€â”€ ç»†èŠ‚: {details_str}")
    
    memories_str = "\n".join(memories_with_facts)
    
    # ç”Ÿæˆå“åº”
    response = pipeline.generate_response(question, question_date_string, memories_str)
    answer = response.choices[0].message.content
    
    return retrieved_memories, answer

def process_and_evaluate_user(line, user_index, infer=True, retrieve_limit: int = 3, extract_mode: str = "whole", vector_db_type="milvus", dataset_name="", max_history_turns: int = 5):
    """
    å°è£…å•ä¸ªç”¨æˆ·çš„æ‰€æœ‰å¤„ç†æ­¥éª¤ï¼Œä»¥ä¾¿å¹¶è¡Œæ‰§è¡Œã€‚
    è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸ã€‚
    """
    try:
        # ä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆå”¯ä¸€çš„user_idï¼Œç¡®ä¿è®°å¿†éš”ç¦»
        user_id = f"user_{user_index}"
        
        # ä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºç‹¬ç«‹çš„pipelineå®ä¾‹ï¼Œé¿å…å¤šçº¿ç¨‹ç«äº‰
        # æ³¨æ„ï¼šæ¯ä¸ªç”¨æˆ·çš„pipelineå®ä¾‹ä¸åº”è¯¥æ¸…ç©ºæ•°æ®åº“ï¼Œclear_dbå›ºå®šä¸ºFalse
        pipeline = MemoryPipeline(vector_db_type=vector_db_type, clear_db=False, dataset_name=dataset_name)
        
        # å¤„ç†ç”¨æˆ·è®°å¿†ä¼šè¯ï¼Œä¼ é€’user_idã€extract_modeå’Œmax_history_turns
        memory_counts = pipeline.process_user_memory_infer(line, retrieve_limit=retrieve_limit, extract_mode=extract_mode, user_id=user_id, max_history_turns=max_history_turns)
        
        # ç”Ÿæˆé—®é¢˜å“åº”ï¼Œä¼ é€’user_id
        retrieved_memories, answer = response_user(line, pipeline, retrieve_limit, user_id=user_id)
        
        # ç¡®ä¿retrieved_memoriesä¸æ˜¯None
        retrieved_memories = retrieved_memories or []
        
        # æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²ç”¨äºåç»­å¤„ç†
        memories_with_facts = []
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼Œç”¨äºè®¡ç®—äº‹å®ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§
        query_vec = get_embedding(line.get("question", ""))
        
        for mem in retrieved_memories:
            # æ·»åŠ è®°å¿†å†…å®¹
            memory_line = f"- [{datetime.fromtimestamp(mem['created_at'], timezone.utc).isoformat()}] {mem['content']}"
            memories_with_facts.append(memory_line)

            # print("#"*50)
            # print("mem:\n", mem)
            # print("#"*50)
            
            # æ·»åŠ å…³è”çš„äº‹å®ï¼ˆå¦‚æœæœ‰ï¼‰
            related_facts = mem.get("related_facts", [])
            max_facts_per_memory = 3  # æ¯ä¸ªè®°å¿†çš„äº‹å®æ•°é‡é™åˆ¶
            if related_facts:
                # è®¡ç®—æ¯ä¸ªäº‹å®ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§åˆ†æ•°
                fact_with_scores = []
                for fact in related_facts:
                    try:
                        fact_vec = get_embedding(fact["text"])
                        # ä½¿ç”¨å‘é‡ç‚¹ç§¯ä½œä¸ºç›¸å…³æ€§åˆ†æ•°
                        dot_product = sum(a * b for a, b in zip(query_vec, fact_vec))
                        fact_with_scores.append((fact, dot_product))
                    except Exception as e:
                        print(f"è®¡ç®—äº‹å®ç›¸å…³æ€§å¤±è´¥: {e}")
                        fact_with_scores.append((fact, 0))
                
                # æ ¹æ®ç›¸å…³æ€§åˆ†æ•°å¯¹äº‹å®è¿›è¡Œæ’åº
                # fact_with_scores.sort(key=lambda x: x[1], reverse=True)
                
                
                # æ·»åŠ æ’åºåçš„äº‹å®ï¼Œé™åˆ¶æ•°é‡
                for i, (fact, score) in enumerate(fact_with_scores[:max_facts_per_memory]):
                    # ä¼˜åŒ–äº‹å®è¾“å‡ºæ ¼å¼
                    # fact_text = fact['text']
                    # details = fact['details']
                    
                    # # æ ¼å¼åŒ–ç»†èŠ‚
                    # if details:
                    #     # å°†ç»†èŠ‚åˆ—è¡¨è½¬æ¢ä¸ºæ›´æ˜“è¯»çš„æ ¼å¼
                    #     details_str = "; ".join(details)
                    #     # å¦‚æœç»†èŠ‚å¤ªé•¿ï¼Œæˆªæ–­
                    #     if len(details_str) > 100:
                    #         details_str = details_str[:97] + "..."
                    #     fact_line = f"  â”œâ”€â”€ [{i+1}] äº‹å®: {fact_text}\n  â”‚     ç»†èŠ‚: {details_str}"
                    # else:
                    #     fact_line = f"  â”œâ”€â”€ [{i+1}] äº‹å®: {fact_text}"
                    
                    # memories_with_facts.append(fact_line)

                        
                    fact_text = fact['text']
                    details = fact['details']
                    # è·å–å¹¶æ ¼å¼åŒ–äº‹å®çš„timestamp
                    fact_timestamp = fact.get('timestamp')
                    timestamp_str = f"[{datetime.fromtimestamp(fact_timestamp, timezone.utc).isoformat()}] " if fact_timestamp else ""
                    
                    # æ ¼å¼åŒ–ç»†èŠ‚
                    if details:
                        # å°†ç»†èŠ‚åˆ—è¡¨è½¬æ¢ä¸ºæ›´æ˜“è¯»çš„æ ¼å¼
                        details_str = "; ".join(details)
                        # å¦‚æœç»†èŠ‚å¤ªé•¿ï¼Œæˆªæ–­
                        if len(details_str) > 150:
                            details_str = details_str[:150] + "..."
                        fact_line = f"  â”œâ”€â”€ [{i+1}] {timestamp_str}äº‹å®: {fact_text}\n  â”‚     ç»†èŠ‚: {details_str}"
                    else:
                        fact_line = f"  â”œâ”€â”€ [{i+1}] {timestamp_str}äº‹å®: {fact_text}"
                    
                    memories_with_facts.append(fact_line)

                    
        memories_str = "\n".join(memories_with_facts)
        
        # è·å–æ ‡å‡†ç­”æ¡ˆå’Œé—®é¢˜ç±»å‹
        golden_answer = line.get("answer")
        question = line.get("question")
        question_type = line.get("question_type", "unknown")
        
        # è¯„ä¼°ç­”æ¡ˆæ­£ç¡®æ€§
        is_correct = lme_grader(llm_client, question, golden_answer, answer)
        
        return {
            "index": user_index,
            "is_correct": is_correct,
            "counts": memory_counts,
            "question": question,
            "question_type": question_type,
            "answer": answer,
            "golden_answer": golden_answer,
            "retrieved_memories": retrieved_memories,
            "context": memories_str,
        }
    except Exception as e:
        print(f"å¤„ç†ç”¨æˆ· {user_index} å‡ºé”™ ({line.get('question', 'Unknown')[:20]}...): {e}")
        return {
            "index": user_index,
            "is_correct": False,
            "counts": {"ADD": 0, "UPDATE": 0, "DELETE": 0, "INFER": 0, "NOOP": 0},
            "question": line.get("question", "N/A"),
            "question_type": line.get("question_type", "unknown"),
            "context": "N/A",
            "answer": "N/A",
            "golden_answer": line.get("answer", "N/A"),
            "retrieved_memories": []
        }

# ==========================================
# Main Test & Evaluation
# ==========================================
if __name__ == "__main__":
    import argparse
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="Memory Pipeline with longmemeval Evaluation")
    parser.add_argument("--eval", action="store_true", help="æ˜¯å¦è¿›è¡Œè¯„ä¼°")
    parser.add_argument("--infer", action="store_true", default=True, help="æ˜¯å¦ä½¿ç”¨æ¨ç†åŠŸèƒ½")
    parser.add_argument("--num_users", type=int, default=50, help="è¯„ä¼°ç”¨æˆ·æ•°é‡")
    parser.add_argument("--max_workers", type=int, default=10, help="å¹¶è¡Œå¤„ç†çš„å·¥ä½œçº¿ç¨‹æ•°")
    parser.add_argument("--retrieve_limit", type=int, default=3, help="æ£€ç´¢æ—¶è¿”å›çš„è®°å¿†æ•°é‡")
    parser.add_argument("--threshold", type=float, default=0.7, help="è®°å¿†ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºè¯¥é˜ˆå€¼çš„è®°å¿†å°†è¢«è¿‡æ»¤æ‰")
    parser.add_argument("--extract-mode", type=str, default="whole", choices=["whole", "turn"], help="æå–æ¨¡å¼ï¼šwhole-å¯¹æ•´ä¸ªchunkè¿›è¡Œæå–ï¼Œturn-æŒ‰è½®æ¬¡æå–ï¼ŒåŒ…å«chat history")
    parser.add_argument("--max-history-turns", type=int, default=5, help="å½“extract-modeä¸ºturnæ—¶ï¼Œä½¿ç”¨çš„èŠå¤©å†å²è½®æ•°")
    parser.add_argument("--vector-db-type", type=str, default="milvus", choices=["milvus", "qdrant"], help="æŒ‡å®šä½¿ç”¨çš„å‘é‡æ•°æ®åº“ç±»å‹")
    parser.add_argument("--clear-db", action="store_true", help="è¿è¡Œå‰æ¸…ç©ºæ•°æ®åº“")
    parser.add_argument("--data-path", type=str, help="æŒ‡å®šæ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--dataset-type", type=str, default="longmemeval", choices=["longmemeval", "hotpotqa"], help="æŒ‡å®šæ•°æ®é›†ç±»å‹")
    args = parser.parse_args()
    
    # åˆå§‹åŒ–å†…å­˜ç®¡é“
    pipeline = MemoryPipeline(vector_db_type=args.vector_db_type, clear_db=args.clear_db, mode='eval' if args.eval else 'test', dataset_name=args.dataset_type)
    
    if args.eval:
        # è¯„ä¼°æ¨¡å¼
        try:
            # æ ¹æ®æ•°æ®é›†ç±»å‹è®¾ç½®é»˜è®¤æ•°æ®è·¯å¾„
            if args.dataset_type == "hotpotqa":
                data_path = args.data_path or "./data/hotpotqa-val.jsonl"
            else:  # longmemeval
                data_path = args.data_path or "./data/longmemeval_s_cleaned.json"
                
            print(f"è°ƒè¯•ä¿¡æ¯ï¼š")
            print(f"  æ•°æ®é›†ç±»å‹ï¼š{args.dataset_type}")
            print(f"  æŒ‡å®šçš„æ•°æ®è·¯å¾„ï¼š{args.data_path}")
            print(f"  å®é™…ä½¿ç”¨çš„æ•°æ®è·¯å¾„ï¼š{data_path}")
            print(f"  æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼š{os.path.exists(data_path)}")
            
            if not os.path.exists(data_path):
                print(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
                exit()
            
            # åˆ¤æ–­æ–‡ä»¶ç±»å‹å¹¶åŠ è½½æ•°æ®
            lines = []
            print(f"  æ–‡ä»¶æ ¼å¼ï¼š{'JSONL' if data_path.endswith('.jsonl') else 'JSON'}")
            if data_path.endswith(".jsonl"):
                # å¤„ç†JSONLæ ¼å¼æ–‡ä»¶
                print(f"  å¼€å§‹åŠ è½½JSONLæ–‡ä»¶...")
                with open(data_path, "r") as f:
                    for i, line in enumerate(f):
                        lines.append(json.loads(line.strip()))
                        if i < 2:  # æ‰“å°å‰2æ¡æ•°æ®çš„å…³é”®å­—æ®µ
                            loaded_item = lines[-1]
                            print(f"    ç¬¬{i+1}æ¡æ•°æ®å…³é”®å­—æ®µï¼š")
                            print(f"      æ˜¯å¦åŒ…å«contextï¼š{'context' in loaded_item}")
                            print(f"      æ˜¯å¦åŒ…å«haystack_datesï¼š{'haystack_dates' in loaded_item}")
                            print(f"      æ•°æ®IDï¼š{loaded_item.get('id', 'æœªçŸ¥')}")
            else:
                # å¤„ç†JSONæ ¼å¼æ–‡ä»¶
                print(f"  å¼€å§‹åŠ è½½JSONæ–‡ä»¶...")
                with open(data_path, "r") as f:
                    lines = json.load(f)
                    if lines and len(lines) > 0:
                        print(f"    å…±åŠ è½½ {len(lines)} æ¡æ•°æ®")
                        if len(lines) > 0:
                            loaded_item = lines[0]
                            print(f"    ç¬¬1æ¡æ•°æ®å…³é”®å­—æ®µï¼š")
                            print(f"      æ˜¯å¦åŒ…å«contextï¼š{'context' in loaded_item}")
                            print(f"      æ˜¯å¦åŒ…å«haystack_datesï¼š{'haystack_dates' in loaded_item}")
                            print(f"      æ•°æ®IDï¼š{loaded_item.get('id', 'æœªçŸ¥')}")
                    
            # å¦‚æœnum_usersä¸º-1ï¼ŒåŠ è½½æ‰€æœ‰æ•°æ®ï¼›å¦åˆ™åŠ è½½æŒ‡å®šæ•°é‡
            if args.num_users != -1:
                lines = lines[:args.num_users]
            
            # æ•°æ®é›†æ ¼å¼åˆ¤æ–­å’Œè½¬æ¢
            def is_valid_format(line):
                """åˆ¤æ–­æ•°æ®æ¡ç›®æ˜¯å¦ç¬¦åˆlongmemevalæ ¼å¼è¦æ±‚"""
                return "haystack_dates" in line and "haystack_sessions" in line
            
            def convert_hotpotqa_to_expected_format(hotpotqa_item):
                """å°†HotpotQAæ¡ç›®è½¬æ¢ä¸ºé¢„æœŸæ ¼å¼"""
                # ç”Ÿæˆå›ºå®šæ ¼å¼çš„æ—¥æœŸ
                date = "2023/01/01 (Sun) 12:00"
                
                # æ„å»ºç³»ç»Ÿæ¶ˆæ¯ï¼ŒåŒ…å«æ‰€æœ‰èƒŒæ™¯çŸ¥è¯†
                context = hotpotqa_item["context"]
                system_content = "ä»¥ä¸‹æ˜¯èƒŒæ™¯çŸ¥è¯†ï¼š\n"
                for title, sentences in zip(context["title"], context["sentences"]):
                    system_content += f"\n{title}:\n"
                    for sentence in sentences:
                        system_content += f"- {sentence}\n"
                
                # æ„å»ºç”¨æˆ·æ¶ˆæ¯ï¼ŒåŒ…å«é—®é¢˜
                user_content = hotpotqa_item["question"]
                
                # æ„å»ºä¼šè¯ç»“æ„
                session = [
                    {"role": "system", "content": system_content.strip()},
                    {"role": "user", "content": user_content}
                ]
                
                # è¿”å›è½¬æ¢åçš„æ ¼å¼ï¼ŒåŒ…å«question_typeå­—æ®µ
                return {
                    "haystack_dates": [date],
                    "haystack_sessions": [session],
                    "id": hotpotqa_item["id"],
                    "answer": hotpotqa_item["answer"],
                    "question_type": hotpotqa_item.get("type", "unknown")  # ä½¿ç”¨hotpotqaçš„typeå­—æ®µä½œä¸ºquestion_type
                }
            
            # å¦‚æœæ˜¯hotpotqaæ•°æ®é›†ï¼Œæ£€æŸ¥æ ¼å¼å¹¶è½¬æ¢
            if args.dataset_type == "hotpotqa":
                # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ¡ç›®æ˜¯å¦ç¬¦åˆæ ¼å¼è¦æ±‚
                if lines and not is_valid_format(lines[0]):
                    print(f"HotpotQAæ•°æ®é›†æ ¼å¼ä¸ç¬¦åˆè¦æ±‚ï¼Œæ­£åœ¨è½¬æ¢ {len(lines)} ä¸ªæ¡ç›®...")
                    # è½¬æ¢æ‰€æœ‰æ¡ç›®
                    converted_lines = []
                    for i, item in enumerate(lines):
                        if i % 100 == 0:  # æ¯å¤„ç†100æ¡æ‰“å°ä¸€æ¬¡è¿›åº¦
                            print(f"å·²è½¬æ¢ {i}/{len(lines)} æ¡æ•°æ®")
                        converted_lines.append(convert_hotpotqa_to_expected_format(item))
                    lines = converted_lines
                    print(f"è½¬æ¢å®Œæˆï¼Œå…±è½¬æ¢ {len(lines)} ä¸ªæ¡ç›®")
                else:
                    print("HotpotQAæ•°æ®é›†æ ¼å¼ç¬¦åˆè¦æ±‚ï¼Œæ­£åœ¨æ£€æŸ¥å¹¶ç¡®ä¿æ‰€æœ‰æ¡ç›®åŒ…å«question_typeå­—æ®µ...")
                    # ç¡®ä¿æ‰€æœ‰æ¡ç›®éƒ½åŒ…å«question_typeå­—æ®µ
                    for i, line in enumerate(lines):
                        if "question_type" not in line:
                            # å°è¯•ä»åŸå§‹æ•°æ®ä¸­è·å–typeå­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
                            lines[i]["question_type"] = line.get("type", "unknown")
                    print("æ£€æŸ¥å®Œæˆï¼Œæ‰€æœ‰æ¡ç›®éƒ½åŒ…å«question_typeå­—æ®µ")
            
            print(f"å·²åŠ è½½ {len(lines)} ä¸ªç”¨æˆ·/é—®é¢˜ã€‚")

            user_detail_results = []
            total_memory_counts = {"ADD": 0, "UPDATE": 0, "DELETE": 0, "INFER": 0, "NOOP": 0}
            
            # å¹¶è¡Œå¤„ç†ç”¨æˆ·
            with ThreadPoolExecutor(max_workers=args.max_workers) as executor:
                # æäº¤ä»»åŠ¡ - ç¡®ä¿å‚æ•°é¡ºåºæ­£ç¡®ï¼šline, idx, args.infer, args.retrieve_limit, args.extract_mode, args.vector_db_type, args.dataset_type, args.max_history_turns
                # æ³¨æ„ï¼šè¿™é‡Œclear_dbå›ºå®šä¸ºFalseï¼Œåªåœ¨ä¸»å‡½æ•°ä¸­æ‰§è¡Œä¸€æ¬¡æ¸…ç©ºæ“ä½œ
                future_to_user = {executor.submit(process_and_evaluate_user, line, idx, args.infer, args.retrieve_limit, args.extract_mode, args.vector_db_type, args.dataset_type, args.max_history_turns): (line, idx) for idx, line in enumerate(lines)}
                
                # å¤„ç†ç»“æœ
                for future in tqdm(as_completed(future_to_user), total=len(future_to_user)):
                    line, idx = future_to_user[future]
                    try:
                        result = future.result()
                        user_detail_results.append(result)
                        
                        # ç»Ÿè®¡æ€»æ“ä½œæ¬¡æ•°
                        for key, value in result["counts"].items():
                            total_memory_counts[key] += value
                    except Exception as e:
                        print(f"å¤„ç†ç”¨æˆ· {idx} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            
            # è®¡ç®—æ€»å‡†ç¡®ç‡
            correct_count = sum(1 for result in user_detail_results if result["is_correct"])
            accuracy = correct_count / len(user_detail_results) * 100 if user_detail_results else 0
            
            # æŒ‰question_typeç»Ÿè®¡æ¯ç±»é—®é¢˜çš„å‡†ç¡®ç‡
            question_type_stats = {}
            for result in user_detail_results:
                q_type = result.get("question_type", "unknown")
                if q_type not in question_type_stats:
                    question_type_stats[q_type] = {"total": 0, "correct": 0}
                question_type_stats[q_type]["total"] += 1
                if result["is_correct"]:
                    question_type_stats[q_type]["correct"] += 1
            
            # è¾“å‡ºè¯„ä¼°ç»“æœ
            print("\n" + "="*50)
            print(f"{args.dataset_type} è¯„ä¼°ç»“æœ")
            print("="*50)
            print(f"æ€»ç”¨æˆ·æ•°: {len(user_detail_results)}")
            print(f"æ­£ç¡®å›ç­”æ•°: {correct_count}")
            print(f"æ€»å‡†ç¡®ç‡: {accuracy:.2f}%")
            print(f"è®°å¿†æ“ä½œæ€»æ•°:")
            for op, count in total_memory_counts.items():
                print(f"  {op}: {count}")
            
            # è¾“å‡ºæŒ‰question_typeåˆ†ç±»çš„å‡†ç¡®ç‡
            print("\n" + "="*50)
            print("æŒ‰é—®é¢˜ç±»å‹åˆ†ç±»çš„å‡†ç¡®ç‡")
            print("="*50)
            for q_type, stats in question_type_stats.items():
                type_accuracy = stats["correct"] / stats["total"] * 100 if stats["total"] > 0 else 0
                print(f"{q_type}: {stats['correct']}/{stats['total']} ({type_accuracy:.2f}%)")
            
            print("="*50)
            
            # è¾“å‡ºè¯¦ç»†ç»“æœ
            print("\nè¯¦ç»†ç»“æœ:")
            for result in user_detail_results:  
                print(f"ç”¨æˆ· {result['index']}: {'âœ“' if result['is_correct'] else 'âœ—'}")
                print(f"  é—®é¢˜: {result['question']}")
                print(f"  é—®é¢˜ç±»å‹: {result.get('question_type', 'unknown')}")
                print(f"  ä¸Šä¸‹æ–‡: {result['context']}")
                print(f"  å›ç­”: {result['answer']}...")
                print(f"  æ ‡å‡†ç­”æ¡ˆ: {result['golden_answer']}...")
                print(f"  è®°å¿†æ“ä½œ: {result['counts']}")
                print()
                
        except Exception as e:
            print(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
    else:
        try:
            # æµ‹è¯•æ¨¡å¼
            # æ ¹æ®æ•°æ®é›†ç±»å‹è®¾ç½®é»˜è®¤æ•°æ®è·¯å¾„
            if args.dataset_type == "hotpotqa":
                data_path = args.data_path or "./data/hotpotqa-val.jsonl"
            else:  # longmemeval
                data_path = args.data_path or "./data/longmemeval_s_cleaned_test.json"
                
            if not os.path.exists(data_path):
                print(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
                exit()
            
            # åˆ¤æ–­æ–‡ä»¶ç±»å‹å¹¶åŠ è½½æ•°æ®
            lines = []
            if data_path.endswith(".jsonl"):
                # å¤„ç†JSONLæ ¼å¼æ–‡ä»¶
                with open(data_path, "r") as f:
                    for line in f:
                        lines.append(json.loads(line.strip()))
            else:
                # å¤„ç†JSONæ ¼å¼æ–‡ä»¶
                with open(data_path, "r") as f:
                    lines = json.load(f)
                    
            if args.num_users != -1:
                lines = lines[:args.num_users]
            
            print(f"å·²åŠ è½½ {len(lines)} ä¸ªç”¨æˆ·/é—®é¢˜ã€‚")

            user_detail_results = []
            total_memory_counts = {"ADD": 0, "UPDATE": 0, "DELETE": 0, "INFER": 0, "NOOP": 0}
            
            # å¹¶è¡Œå¤„ç†ç”¨æˆ·
            with ThreadPoolExecutor(max_workers=args.max_workers) as executor:
                # æäº¤ä»»åŠ¡ï¼ŒåŒ…å«extract_modeå’Œdataset_typeå‚æ•°
                future_to_user = {executor.submit(process_and_evaluate_user, line, idx, args.infer, args.retrieve_limit, args.extract_mode, args.vector_db_type, args.dataset_type): (line, idx) for idx, line in enumerate(lines)}
                
                # å¤„ç†ç»“æœ
                for future in tqdm(as_completed(future_to_user), total=len(future_to_user)):
                    line, idx = future_to_user[future]
                    try:
                        result = future.result()
                        user_detail_results.append(result)
                        
                        # ç»Ÿè®¡æ€»æ“ä½œæ¬¡æ•°
                        for key, value in result["counts"].items():
                            total_memory_counts[key] += value
                    except Exception as e:
                        print(f"å¤„ç†ç”¨æˆ· {idx} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            
            # è®¡ç®—æ€»å‡†ç¡®ç‡
            correct_count = sum(1 for result in user_detail_results if result["is_correct"])
            accuracy = correct_count / len(user_detail_results) * 100 if user_detail_results else 0
            
            # æŒ‰question_typeç»Ÿè®¡æ¯ç±»é—®é¢˜çš„å‡†ç¡®ç‡
            question_type_stats = {}
            for result in user_detail_results:
                q_type = result.get("question_type", "unknown")
                if q_type not in question_type_stats:
                    question_type_stats[q_type] = {"total": 0, "correct": 0}
                question_type_stats[q_type]["total"] += 1
                if result["is_correct"]:
                    question_type_stats[q_type]["correct"] += 1
            
            # è¾“å‡ºè¯„ä¼°ç»“æœ
            print("\n" + "="*50)
            print("LongMemEval è¯„ä¼°ç»“æœ")
            print("="*50)
            print(f"æ€»ç”¨æˆ·æ•°: {len(user_detail_results)}")
            print(f"æ­£ç¡®å›ç­”æ•°: {correct_count}")
            print(f"æ€»å‡†ç¡®ç‡: {accuracy:.2f}%")
            print(f"è®°å¿†æ“ä½œæ€»æ•°:")
            for op, count in total_memory_counts.items():
                print(f"  {op}: {count}")
            
            # è¾“å‡ºæŒ‰question_typeåˆ†ç±»çš„å‡†ç¡®ç‡
            print("\n" + "="*50)
            print("æŒ‰é—®é¢˜ç±»å‹åˆ†ç±»çš„å‡†ç¡®ç‡")
            print("="*50)
            for q_type, stats in question_type_stats.items():
                type_accuracy = stats["correct"] / stats["total"] * 100 if stats["total"] > 0 else 0
                print(f"{q_type}: {stats['correct']}/{stats['total']} ({type_accuracy:.2f}%)")
            
            print("="*50)
            
            # è¾“å‡ºè¯¦ç»†ç»“æœ
            print("\nè¯¦ç»†ç»“æœ:")
            for result in user_detail_results:  
                print(f"ç”¨æˆ· {result['index']}: {'âœ“' if result['is_correct'] else 'âœ—'}")
                print(f"  é—®é¢˜ç±»å‹: {result.get('question_type', 'unknown')}")
                print(f"  é—®é¢˜: {result['question']}")
                print(f"  ä¸Šä¸‹æ–‡: {result['context']}")
                print(f"  å›ç­”: {result['answer']}...")
                print(f"  æ ‡å‡†ç­”æ¡ˆ: {result['golden_answer']}...")
                print(f"  è®°å¿†æ“ä½œ: {result['counts']}")
                print()
                
        except Exception as e:
            print(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()