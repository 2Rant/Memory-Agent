# from copy import deepcopy
# import uuid
# from qdrant_client import QdrantClient
# from qdrant_client.models import Distance, VectorParams
# from qdrant_client.models import PointStruct, PointIdsList
# from utils import (
#     get_embedding, parse_messages, FACT_RETRIEVAL_PROMPT, 
#     remove_code_blocks, extract_json, get_update_memory_messages, 
#     LME_JUDGE_MODEL_TEMPLATE, LME_ANSWER_PROMPT 
# )
# from lme_eval import lme_grader 
# from dotenv import load_dotenv
# import os
# import json
# from openai import OpenAI
# import pytz
# from datetime import datetime, timezone
# from tqdm import tqdm
# from concurrent.futures import ThreadPoolExecutor, as_completed

# load_dotenv()

# OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# BASE_URL = os.getenv("OPENAI_BASE_URL")
# MODEL_NAME = os.getenv("MODEL_NAME", "gpt-4o-mini")
# openai_client = OpenAI(api_key=OPENAI_API_KEY, base_url=BASE_URL)
# dimension=1536
# collection_name = "lme"
# # vect_store_client = QdrantClient(path="./qdrant_db")
# vect_store_client = QdrantClient(url=os.getenv("QDRANT_URL"),
#                                   api_key=os.getenv("QDRANT_API_KEY"))
# topk = 5
# system_prompt = FACT_RETRIEVAL_PROMPT


# def search(collection_name, vect_store_client, query_vector, top_k=5):
#     search_result = vect_store_client.query_points(
#         collection_name=collection_name,
#         query=query_vector,
#         with_payload=True,
#         limit=top_k
#     ).points
#     # print(search_result)
#     return search_result

# def insert(collection_name, vect_store_client, vectors, payloads=None):
#     points = [
#         PointStruct(id=idx, vector=vector, payload=payloads[idx])
#         for idx, vector in enumerate(vectors)
#     ]
#     vect_store_client.upsert(
#         collection_name=collection_name,
#         points=points
#     )

# def generate_response(llm_client, question, question_date, context):
#     prompt = LME_ANSWER_PROMPT.format(
#         question=question,
#         question_date=question_date,
#         context=context
#     )
#     response = llm_client.chat.completions.create(
#                 model=MODEL_NAME,
#                 messages=[{"role": "system", "content": prompt}],
#                 # response_format={"type": "json_object"},
#                 temperature=0,
#             )

#     return response
 
# def process_user_memory_infer(line):
#     dates = line.get("haystack_dates")
#     sessions = line.get("haystack_sessions")
    
#     operation_counts = {"ADD": 0, "UPDATE": 0, "DELETE": 0, "NONE": 0}

#     for session_id, session in enumerate(sessions):
#         date = dates[session_id] + " UTC"
#         date_format = "%Y/%m/%d (%a) %H:%M UTC"
#         date_string = datetime.strptime(date, date_format).replace(tzinfo=timezone.utc)
        
#         parsed_messages = parse_messages(session) 
#         # print("parsed_messages:", parsed_messages) 
#         user_prompt = f"Input:\n{parsed_messages}"
#         llm_response = openai_client.chat.completions.create(
#             model=MODEL_NAME,
#             messages=[
#                 {"role": "system", "content": system_prompt},
#                 {"role": "user", "content": user_prompt}
#             ],
#             response_format={"type": "json_object"},
#         )
#         response = llm_response.choices[0].message.content
#         # print(f"LLM è¿”å›çš„åŸå§‹å“åº”: {response}")
#         if response == '{"facts" : []}':
#             # print("parsed_messages:", parsed_messages)
#             pass
#         try:
#             response = remove_code_blocks(response)
#             if not response.strip():
#                 new_retrieved_facts = []
#             else:
#                 try:
#                     new_retrieved_facts = json.loads(response)["facts"]
#                 except json.JSONDecodeError:
#                     extracted_json = extract_json(response)
#                     new_retrieved_facts = json.loads(extracted_json)["facts"]
#         except Exception as e:
#             print(f"Error in new_retrieved_facts: {e}")
#             new_retrieved_facts = []
#         # print(f"æ–°æ£€ç´¢åˆ°çš„äº‹å®: {new_retrieved_facts}") 

#         if not new_retrieved_facts:
#             # print("No new facts retrieved; skipping memory update.")
#             continue
            
#         retrieved_old_facts = []
#         new_message_embeddings = {} 
#         try:
#             for fact in new_retrieved_facts:
#                 embedding_vector = get_embedding(openai_client, fact, dimension=dimension)
#                 new_message_embeddings[fact] = embedding_vector 
                
#                 existing_memories = search(collection_name, vect_store_client, embedding_vector, top_k=5)
#                 for mem in existing_memories:
#                     retrieved_old_facts.append({"id": mem.id, "text": mem.payload.get("data", "")})
#                     # print("mem:", mem) 
#         except Exception as e:
#             print(f"ç”ŸæˆåµŒå…¥æ—¶å‡ºé”™ï¼Œè¯·æ£€æŸ¥æ‚¨çš„ API Key å’Œç½‘ç»œè¿æ¥ï¼š{e}")

#         unique_data = {}
#         for item in retrieved_old_facts:
#             unique_data[item["id"]] = item
#         retrieved_old_facts = list(unique_data.values())

#         temp_uuid_mapping = {}
#         for idx, item in enumerate(retrieved_old_facts):
#             temp_uuid_mapping[str(idx)] = item["id"]
#             retrieved_old_facts[idx]["id"] = str(idx)
        
#         # print(f"ä¸´æ—¶ UUID æ˜ å°„: {temp_uuid_mapping}") 
#         print(f"ç”¨äºè®°å¿†æ›´æ–°çš„æ—§äº‹å®: {retrieved_old_facts}") 

#         if new_retrieved_facts:
#             memory_action_prompt = get_update_memory_messages(retrieved_old_facts, new_retrieved_facts)
#             # print("Memory Action Prompt:", memory_action_prompt)
#             response = openai_client.chat.completions.create(
#                 model=MODEL_NAME,
#                 messages=[{"role": "user", "content": memory_action_prompt}],
#                 response_format={"type": "json_object"},
#             )
#             update_response = response.choices[0].message.content
#             # print("update_response:", update_response)
#             try:
#                 if not update_response.strip() or not update_response:
#                     # print("Empty response for memory update.")
#                     new_memories_with_actions = {}
#                 else:
#                     response = remove_code_blocks(update_response)
#                     new_memories_with_actions = json.loads(response)
#             except Exception as e:
#                 print(f"Invalid JSON response: {e}")
#                 new_memories_with_actions = {}

#         else:
#             new_memories_with_actions = {}

#         returned_memories = []
#         # print(f"new_memories_with_actions: {new_memories_with_actions}")
#         try:
#             for resp in new_memories_with_actions.get("memory", []):
#                 try:
#                     action_text = resp.get("text")
#                     if not action_text:
#                         print("Skipping memory entry because of empty `text` field.")
#                         continue
                        
#                     event_type = resp.get("event")
#                     if event_type in operation_counts:
#                         operation_counts[event_type] += 1
                        
#                     embedding_vector = get_embedding(openai_client, action_text, dimension=dimension)
                    
#                     if event_type == "ADD":
#                         memory_id = str(uuid.uuid4())
#                         vect_store_client.upsert(
#                         collection_name=collection_name, 
#                         wait=True,
#                         points=[ PointStruct(
#                                     id=memory_id, vector=embedding_vector, 
#                                     payload={ "data": action_text, "created_at": date_string.isoformat()}
#                                 ) ],
#                         )
#                         returned_memories.append({"id": memory_id, "memory": action_text, "event": event_type}) 
                    
#                     elif event_type == "UPDATE":
#                         points_data = vect_store_client.retrieve(
#                             collection_name=collection_name,
#                             ids=[temp_uuid_mapping.get(resp.get("id"))],
#                             with_payload=True, 
#                         )
#                         result = points_data[0] if points_data else None
#                         if result:
#                             old_memory = result.payload.get("data", "")
#                         else:
#                             old_memory = ""

#                         new_updated_at = date_string.isoformat()
#                         result.payload["data"] = action_text
#                         result.payload["updated_at"] = new_updated_at
#                         vect_store_client.upsert(
#                             collection_name=collection_name,
#                             wait=True,
#                             points=[ PointStruct(
#                                         id=temp_uuid_mapping.get(resp.get("id")), 
#                                         vector=embedding_vector, 
#                                         payload=result.payload
#                                     ) ],
#                         )
#                         returned_memories.append( 
#                             {
#                                 # "id": temp_uuid_mapping.get(resp.get("id"), "update_get_id_error"),
#                                 "id": resp.get("id"),
#                                 "memory": action_text,
#                                 "event": event_type,
#                                 "previous_memory": old_memory,
#                             }
#                         )
#                     elif event_type == "DELETE":
#                         if temp_uuid_mapping.get(resp.get("id")) is None:
#                             print(f"Warning: Attempted DELETE on unknown temporary ID: {resp.get('id')}. Skipping.")
#                             continue
#                         # print(f"Deleting memory with ID: {temp_uuid_mapping.get(resp.get('id'))}")
#                         vect_store_client.delete(
#                             collection_name=collection_name,
#                             wait=True,
#                             points_selector=PointIdsList(
#                                 points=[temp_uuid_mapping.get(resp.get("id"))]
#                             ),
#                         )
#                         returned_memories.append(
#                             {
#                                 # "id": temp_uuid_mapping.get(resp.get("id"), "delete_get_id_error"),
#                                 "id": resp.get("id"),
#                                 "memory": action_text,
#                                 "event": event_type,
#                             }
#                         )
#                     elif event_type == "NONE":
#                         returned_memories.append(
#                             {
#                                 # "id": temp_uuid_mapping.get(resp.get("id"), "none_get_id_error"),
#                                 "id": resp.get("id"),
#                                 "memory": action_text,
#                                 "event": event_type,
#                             }
#                         )
#                 except Exception as e:
#                     print("==================================================")
#                     print(f"Error processing memory action {resp}: {e}")   
#                     print(f"å®Œæ•´å“åº”å†…å®¹: {new_memories_with_actions}")
#                     print(f"ä¸´æ—¶ UUID æ˜ å°„: {temp_uuid_mapping}") 
#                     print(f"æ£€ç´¢åˆ°çš„æ—§äº‹å®: {retrieved_old_facts}")
#                     print("==================================================")

#         except Exception as e:
#             print(f"Error iterating new_memories_with_actions: {e}")

#         print(f"æœ€ç»ˆè¿”å›çš„è®°å¿†æ“ä½œç»“æœ: {returned_memories}") 

#     return operation_counts


# def process_user_memory(line):
#     dates = line.get("haystack_dates")
#     sessions = line.get("haystack_sessions")
#     returned_memories = []
#     operation_counts = {"ADD": 0, "UPDATE": 0, "DELETE": 0, "NONE": 0}
#     for session_id, session in enumerate(sessions):
#         date = dates[session_id] + " UTC"
#         date_format = "%Y/%m/%d (%a) %H:%M UTC"
#         date_string = datetime.strptime(date, date_format).replace(tzinfo=timezone.utc)

#         # for message in session:
#         #     message_dict = message
#         #     if isinstance(message, str):
#         #         try:
#         #             message_dict = json.loads(message)
#         #         except json.JSONDecodeError:
#         #             print(f"æ— æ³•è§£ææ¶ˆæ¯ä¸º JSON: {message}")
#         #             continue

#         #     metadata = {
#         #         "created_at": date_string.isoformat(),
#         #     }

#         #     memory_id = str(uuid.uuid4())
#         #     if message_dict["role"] == "system":
#         #         continue
            
#         #     metadata["role"] = message_dict["role"]
#         #     metadata["data"] = message_dict["content"]

#         #     msg_content = message_dict["content"]
#         #     embedding_vector = get_embedding(openai_client, msg_content, dimension=dimension)
#         #     vect_store_client.upsert(
#         #                 collection_name=collection_name, wait=True,
#         #                 points=[ PointStruct(
#         #                             id=memory_id, vector=embedding_vector, 
#         #                             payload=metadata
#         #                         ) ],
#         #                 )
#         #     operation_counts["ADD"] += 1
#         #     returned_memories.append(
#         #         {
#         #             "id": memory_id,
#         #             "memory": msg_content,
#         #             "event": "ADD",
#         #             # "actor_id": actor_name if actor_name else None,
#         #             "role": message_dict["role"],
#         #         }
#         #     )

#         parsed_messages = parse_messages(session)
#         user_prompt = f"Input:\n{parsed_messages}"
#         llm_response = openai_client.chat.completions.create(
#             model=MODEL_NAME,
#             messages=[
#                 {"role": "system", "content": system_prompt},
#                 {"role": "user", "content": user_prompt}
#             ],
#             response_format={"type": "json_object"},
#         )
#         response = llm_response.choices[0].message.content
#         # print(f"LLM è¿”å›çš„åŸå§‹å“åº”: {response}")
#         if response == '{"facts" : []}':
#             # print("parsed_messages:", parsed_messages)
#             continue
#         try:
#             response = remove_code_blocks(response)
#             if not response.strip():
#                 new_retrieved_facts = []
#             else:
#                 try:
#                     new_retrieved_facts = json.loads(response)["facts"]
#                 except json.JSONDecodeError:
#                     extracted_json = extract_json(response)
#                     new_retrieved_facts = json.loads(extracted_json)["facts"]
#         except Exception as e:
#             print(f"Error in new_retrieved_facts: {e}")
#             new_retrieved_facts = []
#         # print(f"æ–°æ£€ç´¢åˆ°çš„äº‹å®: {new_retrieved_facts}")
#         for fact in new_retrieved_facts:
#             embedding_vector = get_embedding(openai_client, fact, dimension=dimension)
#             memory_id = str(uuid.uuid4())
#             vect_store_client.upsert(
#                 collection_name=collection_name, 
#                 wait=True,
#                 points=[ PointStruct(
#                             id=memory_id, vector=embedding_vector, 
#                             payload={ "data": fact, "created_at": date_string.isoformat()}
#                         ) ],
#                 )
#             operation_counts["ADD"] += 1
#             returned_memories.append(
#                 {
#                     "id": memory_id,
#                     "memory": fact,
#                     "event": "ADD",
#                 }
#             )
#         # print(f"æœ€ç»ˆè¿”å›çš„è®°å¿†æ“ä½œç»“æœ: {returned_memories}")
        
#     return operation_counts


# def response_user(line):
#     question = line.get("question")
#     question_date = line.get("question_date")
#     question_date = question_date + " UTC"
#     question_date_format = "%Y/%m/%d (%a) %H:%M UTC"
#     question_date_string = datetime.strptime(question_date, question_date_format).replace(tzinfo=timezone.utc)
#     question = line.get("question")
#     question_vector = get_embedding(openai_client, question, dimension=dimension)
#     retrieved_memories = search(collection_name, vect_store_client, question_vector, top_k=topk)
#     # context = "\n".join([mem.payload.get("data", "") for mem in retrieved_memories])
#     memories_str = (
#             "\n".join(
#                 f"- {mem.payload.get('created_at', '')}: {mem.payload.get('data', '')}"
#                 for mem in retrieved_memories
#             )
#         )
#     response = generate_response(openai_client, question, question_date_string, memories_str)
#     answer = response.choices[0].message.content

#     return answer

# def process_and_evaluate_user(line, user_index, client, infer):
#     """
#     å°è£…å•ä¸ªç”¨æˆ·çš„æ‰€æœ‰å¤„ç†æ­¥éª¤ï¼Œä»¥ä¾¿å¹¶è¡Œæ‰§è¡Œã€‚
#     è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸ã€‚
#     """
#     try:
#         if infer:
#             memory_counts = process_user_memory_infer(line)
#         else:
#             memory_counts = process_user_memory(line)
        
#         answer = response_user(line)
#         golden_answer = line.get("answer") 
#         question = line.get("question")
        
#         is_correct = lme_grader(client, question, golden_answer, answer)
        
#         return {
#             "index": user_index,
#             "is_correct": is_correct,
#             "counts": memory_counts,
#             "question": question,
#             "answer": answer,
#             "golden_answer": golden_answer
#         }
#     except Exception as e:
#         print(f"Error processing user {user_index} ({line.get('question', 'Unknown')[:20]}...): {e}")
#         return {
#             "index": user_index,
#             "is_correct": False,
#             "counts": {"ADD": 0, "UPDATE": 0, "DELETE": 0, "NONE": 0},
#             "question": line.get("question", "N/A")
#         }


# if __name__ == "__main__":
#     # æ¸…ç©ºå¹¶é‡æ–°åˆ›å»º Qdrant é›†åˆ
#     try:
#         if vect_store_client.collection_exists(collection_name=collection_name):
#             vect_store_client.delete_collection(collection_name=collection_name)
#         vect_store_client.create_collection(
#             collection_name=collection_name,
#             vectors_config=VectorParams(size=dimension, distance=Distance.DOT),
#         )
#     except Exception as e:
#         print(f"æ¸…ç©º Qdrant é›†åˆå¤±è´¥: {e}. è¯·æ£€æŸ¥ Qdrant å®¢æˆ·ç«¯è¿æ¥ã€‚")
#         exit()

#     with open("./data/longmemeval_s_cleaned.json", "r") as f:
#         lines = json.load(f)[:50]
    
#     print(f"å·²åŠ è½½ {len(lines)} ä¸ªç”¨æˆ·/é—®é¢˜ã€‚")

#     user_detail_results = [] 
#     total_memory_counts = {"ADD": 0, "UPDATE": 0, "DELETE": 0, "NONE": 0}
    
#     MAX_WORKERS = 10
#     infer = True

#     futures = []

#     print(f"å¼€å§‹ä½¿ç”¨ {MAX_WORKERS} ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†...")
    
#     with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
#         for idx, line in enumerate(lines):
#             future = executor.submit(process_and_evaluate_user, line, idx + 1, openai_client, infer=infer)
#             futures.append(future)
        
#         for future in tqdm(as_completed(futures), total=len(futures), desc="è¯„ä¼°è¿›åº¦"):
#             result = future.result()
#             user_detail_results.append(result)

#     user_detail_results.sort(key=lambda x: x.get("index", 0))

#     correct_count = 0
#     total_evaluated = len(user_detail_results)

#     for res in user_detail_results:
#         if res.get("is_correct"):
#             correct_count += 1
        
#         counts = res.get("counts", {})
#         for key in total_memory_counts:
#             total_memory_counts[key] += counts.get(key, 0)

#     print("\n\n==================================================")
#     print("             ğŸ¯ æœ€ç»ˆè¯„ä¼°ç»“æœ") 
#     print("==================================================")

#     if total_evaluated > 0:
#         final_accuracy = correct_count / total_evaluated
#         print(f"æ€»è¯„ä¼°é—®é¢˜æ•°: {total_evaluated}")
#         print(f"æ­£ç¡®å›ç­”æ•°: {correct_count}")
#         print(f"æœ€ç»ˆæ€»å‡†ç¡®ç‡: {final_accuracy:.4f} ({final_accuracy * 100:.2f}%)")
#     else:
#         print("æ²¡æœ‰è¯„ä¼°ä»»ä½•é—®é¢˜ã€‚")
#     print("==================================================")

#     print("\n\n==================================================")
#     print("        ğŸ“Š è¯¦ç»†è®°å¿†æ“ä½œç»Ÿè®¡ (æŒ‰ç”¨æˆ·)")
#     print("==================================================")

#     for res in user_detail_results:
#         user_index = res["index"]
#         is_correct = res["is_correct"]
#         counts = res["counts"]
#         question = res["question"]
#         answer = res.get("answer", "N/A")
#         golden_answer = res.get("golden_answer", "N/A")
#         status = "âœ… CORRECT" if is_correct else "âŒ WRONG"
        
#         print(f"\n--- ç”¨æˆ·/é—®é¢˜ {user_index} ---")
#         print(f"  é—®é¢˜: {question[:60]}...")
#         print(f"  æ¨¡å‹å›ç­”: {answer}...")
#         print(f"  æ ‡å‡†ç­”æ¡ˆ: {golden_answer}...")
#         print(f"  è¯„ä¼°ç»“æœ: {status}")
#         print(f"  è®°å¿†æ“ä½œ: ADD={counts.get('ADD', 0)}, UPDATE={counts.get('UPDATE', 0)}, DELETE={counts.get('DELETE', 0)}, NONE={counts.get('NONE', 0)}")

#     print("\n--- æ‰€æœ‰ç”¨æˆ·çš„è®°å¿†æ“ä½œæ€»è§ˆ ---")
#     print(f"  ADD (æ–°å¢):    {total_memory_counts['ADD']}")
#     print(f"  UPDATE (æ›´æ–°): {total_memory_counts['UPDATE']}")
#     print(f"  DELETE (åˆ é™¤): {total_memory_counts['DELETE']}")
#     print(f"  NONE (æ— æ“ä½œ): {total_memory_counts['NONE']}")
#     print("==================================================")




# import json
# import os
# from datetime import datetime
# from typing import List, Dict, Any
# from openai import OpenAI
# from dotenv import load_dotenv

# load_dotenv()
# # æ¨¡æ‹Ÿé…ç½®ï¼Œè¯·æ›¿æ¢ä¸ºä½ çš„çœŸå® Key
# # os.environ["OPENAI_API_KEY"] = "sk-..." 
# client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"),
#                 base_url=os.getenv("OPENAI_BASE_URL"))

# # ==============================================================================
# # 1. Prompt å®šä¹‰
# # ==============================================================================

# # æ³¨æ„ï¼šæˆ‘å¯¹ä½ çš„åŸå§‹ Prompt åšäº†å¾®è°ƒï¼Œå¢åŠ äº† {core_memory_block} æ’æ§½
# # è¿™æ · LLM æ‰èƒ½çœ‹åˆ°"æ—§è®°å¿†"ï¼Œä»è€Œå®ç°æŒ‡ä»£æ¶ˆè§£å’Œå»é‡ã€‚
# MEM_READER_PROMPT_TEMPLATE = """You are a Personal Information Organizer. Your goal is to extract RELEVANT facts from the user conversation.

# ### EXISTING MEMORY CONTEXT ###
# The following information is ALREADY known. Use this to:
# 1. Resolve pronouns (e.g., "He" -> "Tom").
# 2. Ignore facts that are already exactly stored.
# 3. Provide context for new preferences.

# [Start of Memory]
# {core_memory_block}
# [End of Memory]

# ### INSTRUCTIONS ###
# Extract new facts based on the user input.
# - If the input is just a greeting or irrelevant, return empty facts.
# - Detect the language of the user input and record facts in the same language.
# - Return format must be JSON: {{"facts": ["fact string 1", "fact string 2"]}}

# ### Types of Information to Remember ###
# 1. Personal Preferences (Food, hobbies, etc.)
# 2. Personal Details (Names, relationships)
# 3. Plans and Intentions
# 4. Professional Details
# 5. Health/Wellness

# ### CONVERSATION ###
# User: {user_input}
# """

# # Agent è´Ÿè´£æ ¹æ®æ–°æå–çš„äº‹å®å’Œæ—§è®°å¿†ï¼Œå†³å®šå…·ä½“æ“ä½œ (ADD, UPDATE, DELETE, NONE)
# AGENT_INFER_PROMPT_TEMPLATE = """You are the Memory Manager Agent.
# You have two inputs:
# 1. Existing Memories (The current state of the database)
# 2. New Extracted Facts (What the user just said, processed by MemReader)

# Your task is to determine the correct OPERATION for each new fact.
# Available Operations:
# - ADD: Information is completely new.
# - UPDATE: Information conflicts with or refines an existing memory (provide the old_memory_id).
# - DELETE: User explicitly asks to forget something.
# - NONE: Information is redundant or already exists.

# ### Existing Memories ###
# {memory_list_json}

# ### New Extracted Facts ###
# {new_facts_json}

# ### OUTPUT FORMAT ###
# Return a JSON object with a key "operations". Example:
# {{
#     "operations": [
#         {{"action": "ADD", "content": "Likes sushi"}},
#         {{"action": "UPDATE", "target_memory_id": "mem_01", "new_content": "Lives in New York"}},
#         {{"action": "NONE", "reason": "Already known"}}
#     ]
# }}
# """

# # ==============================================================================
# # 2. Pipeline ç±»å®ç°
# # ==============================================================================

# class MemoryPipeline:
#     def __init__(self):
#         self.model = "gpt-4o-mini"  # å»ºè®®ä½¿ç”¨ gpt-4o æˆ– gpt-3.5-turbo

#     def _get_completion(self, prompt: str) -> str:
#         """ç®€å•çš„ OpenAI è°ƒç”¨å°è£…"""
#         response = client.chat.completions.create(
#             model=self.model,
#             messages=[{"role": "user", "content": prompt}],
#             response_format={"type": "json_object"}, # å¼ºåˆ¶ JSON è¾“å‡º
#             temperature=0
#         )
#         return response.choices[0].message.content

#     def mem_reader(self, user_input: str, core_memory: List[Dict]) -> List[str]:
#         """
#         é˜¶æ®µ 1: MemReader
#         åŠŸèƒ½: ç»“åˆ Core Memory ä¸Šä¸‹æ–‡ï¼Œä»å¯¹è¯ä¸­æå–çº¯å‡€çš„ Facts
#         """
#         # 1. æ ¼å¼åŒ–æ—§è®°å¿†ç»™ MemReader çœ‹
#         if not core_memory:
#             memory_str = "No existing memory."
#         else:
#             # ç®€åŒ–å±•ç¤ºï¼Œåªç»™å†…å®¹
#             memory_str = "\n".join([f"- {m['content']}" for m in core_memory])

#         # 2. å¡«å…… Prompt
#         formatted_prompt = MEM_READER_PROMPT_TEMPLATE.format(
#             core_memory_block=memory_str,
#             user_input=user_input,
#             datetime=datetime.now().strftime("%Y-%m-%d")
#         )

#         # 3. è°ƒç”¨ LLM
#         print(f"   [MemReader] Processing input: '{user_input}'...")
#         response_json = self._get_completion(formatted_prompt)
        
#         try:
#             data = json.loads(response_json)
#             facts = data.get("facts", [])
#             print(f"   [MemReader] Extracted: {facts}")
#             return facts
#         except Exception as e:
#             print(f"Error parsing MemReader output: {e}")
#             return []

#     def agent_infer(self, new_facts: List[str], core_memory: List[Dict]) -> Dict:
#         """
#         é˜¶æ®µ 2: Agent Inference
#         åŠŸèƒ½: å¯¹æ¯” New Facts å’Œ Core Memoryï¼Œç”Ÿæˆæ“ä½œæŒ‡ä»¤ (Add/Update/Delete)
#         """
#         if not new_facts:
#             return {"operations": []}

#         # 1. å‡†å¤‡æ•°æ®
#         # ç»™ Agent å®Œæ•´çš„å¸¦æœ‰ ID çš„è®°å¿†ï¼Œä»¥ä¾¿å®ƒèƒ½æŒ‡å®š Update å“ªä¸€æ¡
#         memory_json = json.dumps(core_memory, indent=2, ensure_ascii=False)
#         facts_json = json.dumps(new_facts, indent=2, ensure_ascii=False)

#         # 2. å¡«å…… Prompt
#         formatted_prompt = AGENT_INFER_PROMPT_TEMPLATE.format(
#             memory_list_json=memory_json,
#             new_facts_json=facts_json
#         )

#         # 3. è°ƒç”¨ LLM
#         print(f"   [Agent] Inferring operations...")
#         response_json = self._get_completion(formatted_prompt)
        
#         try:
#             return json.loads(response_json)
#         except Exception as e:
#             print(f"Error parsing Agent output: {e}")
#             return {"operations": []}

#     def run(self, user_input: str, core_memory: List[Dict]):
#         """æ‰§è¡Œå®Œæ•´æµç¨‹"""
#         print(f"\n--- Pipeline Start ---")
        
#         # Step 1: MemReader (Conversation + Memory -> Facts)
#         extracted_facts = self.mem_reader(user_input, core_memory)
        
#         # Step 2: Agent Inference (Facts + Memory -> Operations)
#         results = self.agent_infer(extracted_facts, core_memory)
        
#         print(f"   [Result] Operations: {json.dumps(results, indent=2, ensure_ascii=False)}")
#         print(f"--- Pipeline End ---\n")
#         return results

# # ==============================================================================
# # 3. éªŒè¯å®éªŒ (A/B Test)
# # ==============================================================================

# if __name__ == "__main__":
#     pipeline = MemoryPipeline()

#     # æ¨¡æ‹Ÿå½“å‰çš„ Core Memory (å¸¦æœ‰ ID)
#     current_memory = [
#         {"id": "mem_01", "content": "User's son is named Tom."},
#         {"id": "mem_02", "content": "User lives in Shanghai."},
#         {"id": "mem_03", "content": "User works as a Python Developer."}
#     ]

#     print("=== å®éªŒ 1: éªŒè¯ Core Memory å¯¹ MemReader çš„å½±å“ (æŒ‡ä»£æ¶ˆè§£) ===")
#     user_input = "It is his birthday tomorrow."
    
#     print("\n>>> Group A: å‡å¦‚æ²¡æœ‰ Core Memory (ç©ºåˆ—è¡¨)")
#     pipeline.run(user_input, core_memory=[]) 
#     # é¢„æœŸ MemReader è¾“å‡º: "It is his birthday tomorrow" (æ¨¡ç³Šï¼ŒAgent æ— æ³•å¤„ç†)

#     print("\n>>> Group B: æ³¨å…¥ Core Memory")
#     pipeline.run(user_input, core_memory=current_memory)
#     # é¢„æœŸ MemReader è¾“å‡º: "It is Tom's birthday tomorrow" (æ¸…æ™°ï¼ŒAgent ç”Ÿæˆ ADD æ“ä½œ)

#     print("\n" + "="*50 + "\n")

#     print("=== å®éªŒ 2: éªŒè¯ Update/Conflict é€»è¾‘ ===")
#     user_input_2 = "I moved to Beijing yesterday."
    
#     # è¿™é‡Œçš„ MemReader åº”è¯¥æå–å‡º "User moved to Beijing"
#     # Agent åº”è¯¥æ£€æµ‹åˆ°ä¸ mem_02 (Shanghai) å†²çªï¼Œç”Ÿæˆ UPDATE æ“ä½œ
#     pipeline.run(user_input_2, core_memory=current_memory)


