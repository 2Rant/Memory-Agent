from copy import deepcopy
import uuid
import numpy as np
from pymilvus import (
    connections,
    FieldSchema,
    CollectionSchema,
    DataType,
    Collection,
    utility
)
from utils import (
    get_embedding, parse_messages, 
    remove_code_blocks, extract_json, get_update_memory_messages,
    LME_JUDGE_MODEL_TEMPLATE, LME_ANSWER_PROMPT, FACT_RETRIEVAL_CORE_MEMORY_TEMPLATE,
    FACT_RETRIEVAL_PROMPT, get_update_memory_messages_core_mem, CORE_MEMORY_UPDATE_PROMPT
)
from lme_eval import lme_grader 
from dotenv import load_dotenv
import os
import json
from openai import OpenAI
import pytz
from datetime import datetime, timezone
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# OpenAI å®¢æˆ·ç«¯é…ç½®
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
BASE_URL = os.getenv("OPENAI_BASE_URL")
MODEL_NAME = os.getenv("MODEL_NAME", "gpt-4o-mini")
openai_client = OpenAI(api_key=OPENAI_API_KEY, base_url=BASE_URL)

# åµŒå…¥å‘é‡ç»´åº¦
embedding_dim = 1536

# Milvus é…ç½®
MILVUS_HOST = os.getenv("MILVUS_HOST", "localhost")
MILVUS_PORT = os.getenv("MILVUS_PORT", "19530")
COLLECTION_NAME = "memory_graph_db"

# æœç´¢å‚æ•°é…ç½®
topk = 5
search_topk = 10

# å·¥ä½œè®°å¿†æœ€å¤§é•¿åº¦
MAX_WORKING_MEMORY_SIZE = 5

# åˆå§‹åŒ– Milvus è¿æ¥
def init_milvus_connection():
    try:
        connections.connect("default", host=MILVUS_HOST, port=MILVUS_PORT)
        print(f"æˆåŠŸè¿æ¥åˆ° Milvus æœåŠ¡å™¨: {MILVUS_HOST}:{MILVUS_PORT}")
        
        # å®šä¹‰é›†åˆç»“æ„
        fields = [
            FieldSchema(name="id", dtype=DataType.VARCHAR, is_primary=True, max_length=64),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=embedding_dim),
            FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=4096),
            FieldSchema(name="memory_type", dtype=DataType.VARCHAR, max_length=32),  # core, semantic, episodic
            FieldSchema(name="created_at", dtype=DataType.VARCHAR, max_length=32),
            FieldSchema(name="updated_at", dtype=DataType.VARCHAR, max_length=32),
            FieldSchema(name="details", dtype=DataType.JSON),  # å­˜å‚¨è¯¦ç»†ä¿¡æ¯ï¼Œå¦‚ Targetå•†åº—, ä»£é‡‘åˆ¸ä»·å€¼ç­‰
        ]
        
        schema = CollectionSchema(fields, "è®°å¿†å›¾æ•°æ®åº“é›†åˆ")
        
        # å¦‚æœé›†åˆå·²å­˜åœ¨ï¼Œåˆ é™¤å®ƒ
        if utility.has_collection(COLLECTION_NAME):
            print(f"åˆ é™¤å·²å­˜åœ¨çš„é›†åˆ: {COLLECTION_NAME}")
            utility.drop_collection(COLLECTION_NAME)
        
        # åˆ›å»ºæ–°é›†åˆ
        collection = Collection(COLLECTION_NAME, schema)
        print(f"æˆåŠŸåˆ›å»ºé›†åˆ: {COLLECTION_NAME}")
        
        # åˆ›å»ºç´¢å¼•
        index_params = {
            "index_type": "IVF_FLAT",
            "metric_type": "COSINE",
            "params": {"nlist": 1024}
        }
        collection.create_index(field_name="embedding", index_params=index_params)
        print("æˆåŠŸåˆ›å»ºå‘é‡ç´¢å¼•")
        
        # åŠ è½½é›†åˆ
        collection.load()
        print("é›†åˆåŠ è½½å®Œæˆ")
        
        return collection
    except Exception as e:
        print(f"Milvus åˆå§‹åŒ–å¤±è´¥: {e}")
        raise

# MemReader Agent ç±»
class MemReaderAgent:
    def __init__(self, llm_client):
        self.llm_client = llm_client
        self.working_memory = []  # å·¥ä½œè®°å¿†ï¼Œå­˜å‚¨æœ€è¿‘å¤„ç†çš„äº‹å®å’Œä¸Šä¸‹æ–‡
        self.core_memory = "No core memory yet."
    
    def maintain_working_memory(self, new_item, max_size=MAX_WORKING_MEMORY_SIZE):
        """ç»´æŠ¤å·¥ä½œè®°å¿†ï¼Œä¿æŒæœ€è¿‘çš„äº¤äº’å†…å®¹"""
        # æ·»åŠ æ–°é¡¹åˆ°å·¥ä½œè®°å¿†
        timestamp = datetime.now().isoformat()
        self.working_memory.append({
            "timestamp": timestamp,
            "content": new_item
        })
        
        # å¦‚æœå·¥ä½œè®°å¿†è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œç§»é™¤æœ€æ—©çš„é¡¹
        if len(self.working_memory) > max_size:
            self.working_memory.pop(0)
        
        return self.working_memory
    
    def extract_facts(self, dialogue, core_memory=None):
        """ä»å¯¹è¯ä¸­æå–äº‹å®ï¼Œå¹¶ç»´æŠ¤å·¥ä½œè®°å¿†"""
        # ä½¿ç”¨æä¾›çš„æ ¸å¿ƒè®°å¿†æˆ–é»˜è®¤æ ¸å¿ƒè®°å¿†
        current_core_memory = core_memory if core_memory else self.core_memory
        
        # æ„å»ºåŒ…å«å·¥ä½œè®°å¿†çš„ç³»ç»Ÿæç¤º
        working_memory_str = "\n".join([
            f"[{item['timestamp']}]: {item['content']}"
            for item in self.working_memory[-3:]
        ])
        
        system_prompt = FACT_RETRIEVAL_CORE_MEMORY_TEMPLATE.format(
            current_date=datetime.now().strftime("%Y-%m-%d"),
            core_memory=current_core_memory
        )
        
        # æ·»åŠ å·¥ä½œè®°å¿†åˆ°æç¤º
        if working_memory_str:
            system_prompt += f"\n\nRecent Working Memory:\n{working_memory_str}"
        
        user_prompt = f"Input:\n{dialogue}"
        
        # è°ƒç”¨ LLM æå–äº‹å®
        llm_response = self.llm_client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            response_format={"type": "json_object"},
        )
        
        response = llm_response.choices[0].message.content
        
        # æ›´æ–°å·¥ä½œè®°å¿†
        self.maintain_working_memory(dialogue)
        
        # è§£ææå–çš„äº‹å®
        if response == '{"facts" : []}':
            return []
        
        try:
            response = remove_code_blocks(response)
            if not response.strip():
                return []
            
            try:
                extracted_data = json.loads(response)
                facts = extracted_data.get("facts", [])
                
                # æ›´æ–°å·¥ä½œè®°å¿†ä¸­çš„äº‹å®
                for fact in facts:
                    self.maintain_working_memory(f"Extracted fact: {fact}")
                
                return facts
            except json.JSONDecodeError:
                extracted_json = extract_json(response)
                extracted_data = json.loads(extracted_json)
                facts = extracted_data.get("facts", [])
                
                # æ›´æ–°å·¥ä½œè®°å¿†ä¸­çš„äº‹å®
                for fact in facts:
                    self.maintain_working_memory(f"Extracted fact: {fact}")
                
                return facts
        except Exception as e:
            print(f"æå–äº‹å®æ—¶å‡ºé”™: {e}")
            return []
    
    def extract_fact_details(self, facts):
        """ä»æå–çš„äº‹å®ä¸­æå–æ›´è¯¦ç»†çš„ä¿¡æ¯ï¼ˆå¦‚å®ä½“ã€å±æ€§ã€å…³ç³»ç­‰ï¼‰"""
        detailed_facts = []
        
        for fact in facts:
            try:
                # æ„å»ºæç¤ºæ¥æå–äº‹å®çš„è¯¦ç»†ä¿¡æ¯
                prompt = f"""
è¯·ä»ä»¥ä¸‹äº‹å®ä¸­æå–è¯¦ç»†ä¿¡æ¯ï¼š

äº‹å®ï¼š{fact}

è¯·è¯†åˆ«ï¼š
1. ä¸»è¦å®ä½“/ä¸»é¢˜ï¼ˆtargetï¼‰
2. è¯¥å®ä½“çš„å±æ€§/ç‰¹æ€§
3. ä¸å…¶ä»–å®ä½“çš„å…³ç³»ï¼ˆå¦‚æœæœ‰ï¼‰
4. ä»»ä½•ç‰¹å®šçš„å€¼ã€æ—¥æœŸæˆ–å…³é”®ç»†èŠ‚
5. è¿™ä¸ªäº‹å®çš„æ ¸å¿ƒå«ä¹‰

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹é”®ï¼š
- target: ä¸»è¦å®ä½“/ä¸»é¢˜
- details: è¡¨ç¤ºå±æ€§å’Œå€¼çš„é”®å€¼å¯¹æ•°ç»„
"""
                
                response = self.llm_client.chat.completions.create(
                    model=MODEL_NAME,
                    messages=[{"role": "system", "content": prompt}],
                    response_format={"type": "json_object"},
                )
                
                fact_details = response.choices[0].message.content
                fact_details = json.loads(fact_details)
                
                # ç¡®ä¿æ ¼å¼æ­£ç¡®
                if "target" not in fact_details:
                    fact_details["target"] = fact
                if "details" not in fact_details:
                    fact_details["details"] = []
                
                # æ·»åŠ åŸå§‹äº‹å®
                fact_details["original_fact"] = fact
                detailed_facts.append(fact_details)
                
                # æ›´æ–°å·¥ä½œè®°å¿†
                self.maintain_working_memory(f"Detailed fact: {fact_details['target']} -> {fact_details['details']}")
                
            except Exception as e:
                print(f"æå–äº‹å®è¯¦æƒ…æ—¶å‡ºé”™: {e}")
                # å¦‚æœå‡ºé”™ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
                detailed_facts.append({
                    "target": fact,
                    "details": [],
                    "original_fact": fact
                })
        
        return detailed_facts
    
    def update_core_memory(self, dialogue, current_core_memory=None):
        """æ›´æ–°æ ¸å¿ƒè®°å¿†"""
        current_core = current_core_memory if current_core_memory else self.core_memory
        
        try:
            core_prompt = CORE_MEMORY_UPDATE_PROMPT.format(
                core_memory=current_core,
                new_dialogue=dialogue
            )
            
            core_response = self.llm_client.chat.completions.create(
                model=MODEL_NAME,
                messages=[{"role": "user", "content": core_prompt}], 
                temperature=0, 
            )
            
            updated_core = core_response.choices[0].message.content.strip()
            if updated_core and updated_core != "No core memory yet.":
                self.core_memory = updated_core
                # æ›´æ–°å·¥ä½œè®°å¿†
                self.maintain_working_memory(f"Updated core memory: {updated_core[:100]}...")
                return updated_core
        except Exception as e:
            print(f"æ›´æ–°æ ¸å¿ƒè®°å¿†æ—¶å‡ºé”™: {e}")
        
        return current_core

# Memory Manager ç±»ï¼Œè´Ÿè´£ä¸ Milvus å›¾æ•°æ®åº“äº¤äº’
class MemoryManager:
    def __init__(self, collection, llm_client):
        self.collection = collection
        self.llm_client = llm_client
    
    def get_embedding(self, text):
        """è·å–æ–‡æœ¬çš„åµŒå…¥å‘é‡"""
        try:
            # ä½¿ç”¨OpenAIè·å–åµŒå…¥å‘é‡
            response = openai_client.embeddings.create(
                model="text-embedding-3-small",
                input=text
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"ç”ŸæˆåµŒå…¥å‘é‡æ—¶å‡ºé”™: {e}")
            return None
    
    def search_memory(self, query_text, memory_type=None, top_k=5):
        """åœ¨è®°å¿†åº“ä¸­æœç´¢ç›¸å…³è®°å¿†"""
        try:
            query_embedding = self.get_embedding(query_text)
            if query_embedding is None:
                return []
            
            # æ„å»ºæœç´¢è¡¨è¾¾å¼
            expr = None
            if memory_type:
                expr = f"memory_type == '{memory_type}'"
            
            # ä¼˜åŒ–çš„æœç´¢å‚æ•°
            search_params = {"metric_type": "COSINE", "params": {"nprobe": 64}}
            results = self.collection.search(
                data=[query_embedding],
                anns_field="embedding",
                param=search_params,
                limit=top_k,
                expr=expr,
                output_fields=["id", "content", "memory_type", "created_at", "updated_at", "details"]
            )
            
            # æ ¼å¼åŒ–æœç´¢ç»“æœ
            search_results = []
            for hits in results:
                for hit in hits:
                    search_results.append({
                        "id": hit.entity.get("id"),
                        "content": hit.entity.get("content"),
                        "text": hit.entity.get("content"),  # æ·»åŠ textå­—æ®µä»¥å…¼å®¹å…¶ä»–ç»„ä»¶
                        "memory_type": hit.entity.get("memory_type"),
                        "score": hit.score,
                        "created_at": hit.entity.get("created_at"),
                        "updated_at": hit.entity.get("updated_at"),
                        "details": hit.entity.get("details", {})
                    })
            
            return search_results
        except Exception as e:
            print(f"æœç´¢è®°å¿†æ—¶å‡ºé”™: {e}")
            return []
    
    def add_memory(self, content, memory_type="semantic", details=None):
        """æ·»åŠ æ–°è®°å¿†åˆ°æ•°æ®åº“"""
        try:
            embedding_vector = self.get_embedding(content)
            if embedding_vector is None:
                return None
            
            memory_id = str(uuid.uuid4())
            timestamp = datetime.now().isoformat()
            
            # å‡†å¤‡æ’å…¥æ•°æ®
            data = [
                [memory_id],  # id
                [embedding_vector],  # embedding
                [content],  # content
                [memory_type],  # memory_type
                [timestamp],  # created_at
                [timestamp],  # updated_at
                [details if details else {}]  # details
            ]
            
            # æ’å…¥æ•°æ®
            self.collection.insert(data)
            # åˆ·æ–°é›†åˆä»¥ç¡®ä¿æ•°æ®å¯æœç´¢
            self.collection.flush()
            print(f"æˆåŠŸæ·»åŠ è®°å¿†: {memory_id} - {content[:50]}...")
            
            return {
                "id": memory_id,
                "content": content,
                "memory_type": memory_type,
                "created_at": timestamp,
                "details": details
            }
        except Exception as e:
            print(f"æ·»åŠ è®°å¿†æ—¶å‡ºé”™: {e}")
            return None
    
    def update_memory(self, memory_id, new_content, details=None):
        """æ›´æ–°ç°æœ‰è®°å¿†"""
        try:
            # è·å–ç°æœ‰è®°å¿†
            expr = f"id == '{memory_id}'"
            results = self.collection.query(expr=expr, output_fields=["id", "content", "memory_type", "created_at", "details"])
            
            if not results:
                print(f"æœªæ‰¾åˆ°è¦æ›´æ–°çš„è®°å¿†: {memory_id}")
                return None
            
            existing_memory = results[0]
            embedding_vector = self.get_embedding(new_content)
            if embedding_vector is None:
                return None
            
            # æ›´æ–°æ—¶é—´æˆ³
            updated_at = datetime.now().isoformat()
            
            # åˆå¹¶è¯¦ç»†ä¿¡æ¯
            updated_details = existing_memory.get("details", {})
            if details:
                updated_details.update(details)
            
            # å‡†å¤‡æ›´æ–°æ•°æ®
            data = [
                [memory_id],  # id
                [embedding_vector],  # embedding
                [new_content],  # content
                [existing_memory["memory_type"]],  # memory_type
                [existing_memory["created_at"]],  # created_at
                [updated_at],  # updated_at
                [updated_details]  # details
            ]
            
            # åˆ é™¤æ—§æ•°æ®
            self.collection.delete(expr=expr)
            
            # æ’å…¥æ›´æ–°åçš„æ•°æ®
            self.collection.insert(data)
            # åˆ·æ–°é›†åˆ
            self.collection.flush()
            print(f"æˆåŠŸæ›´æ–°è®°å¿†: {memory_id} - {new_content[:50]}...")
            
            return {
                "id": memory_id,
                "content": new_content,
                "memory_type": existing_memory["memory_type"],
                "created_at": existing_memory["created_at"],
                "updated_at": updated_at,
                "details": updated_details
            }
        except Exception as e:
            print(f"æ›´æ–°è®°å¿†æ—¶å‡ºé”™: {e}")
            return None
    
    def delete_memory(self, memory_id):
        """åˆ é™¤è®°å¿†"""
        try:
            expr = f"id == '{memory_id}'"
            result = self.collection.delete(expr=expr)
            # åˆ·æ–°é›†åˆ
            self.collection.flush()
            print(f"æˆåŠŸåˆ é™¤è®°å¿†: {memory_id}")
            return True
        except Exception as e:
            print(f"åˆ é™¤è®°å¿†æ—¶å‡ºé”™: {e}")
            return False
    
    def judge_memory_action(self, detailed_facts, existing_memories, core_memory):
        """åˆ¤æ–­å¯¹è®°å¿†çš„æ“ä½œç±»å‹ï¼ˆADDã€UPDATEã€DELETEï¼‰"""
        try:
            # å‡†å¤‡ç°æœ‰è®°å¿†çš„æ ¼å¼
            retrieved_old_memory_dict = []
            memory_id_mapping = {}
            
            for idx, mem in enumerate(existing_memories):
                mem_id = str(idx)
                retrieved_old_memory_dict.append({
                    "id": mem_id,
                    "text": mem["content"]
                })
                memory_id_mapping[mem_id] = mem["id"]
            
            # æ„å»ºä¼˜åŒ–çš„æç¤ºä»¥åˆ¤æ–­è®°å¿†æ“ä½œ
            prompt = f"""
            ä½ æ˜¯ä¸€ä¸ªè®°å¿†ç®¡ç†ä¸“å®¶ï¼Œéœ€è¦åˆ†ææ–°æå–çš„äº‹å®å’Œå·²æœ‰çš„è®°å¿†ï¼Œå†³å®šå¯¹æ¯ä¸ªæ–°äº‹å®åº”è¯¥æ‰§è¡Œä»€ä¹ˆæ“ä½œã€‚
            
            Core Memory: {core_memory}
            
            å·²æœ‰çš„ç›¸å…³è®°å¿†ï¼š
            {json.dumps(retrieved_old_memory_dict, ensure_ascii=False, indent=2)}
            
            æ–°æå–çš„è¯¦ç»†äº‹å®ï¼š
            {json.dumps(detailed_facts, ensure_ascii=False, indent=2)}
            
            å¯¹äºæ¯ä¸ªæ–°æå–çš„äº‹å®ï¼Œè¯·å†³å®šæ‰§è¡Œä»¥ä¸‹æ“ä½œä¹‹ä¸€ï¼š
            1. ADD: å¦‚æœè¿™æ˜¯ä¸€ä¸ªå…¨æ–°çš„äº‹å®ï¼Œä¸ç°æœ‰è®°å¿†æ²¡æœ‰æ˜¾è‘—é‡å 
            2. UPDATE: å¦‚æœè¿™ä¸ªäº‹å®ä¸æŸä¸ªç°æœ‰è®°å¿†é«˜åº¦ç›¸å…³ï¼Œä½†éœ€è¦æ›´æ–°æˆ–åˆå¹¶
            3. DELETE: å¦‚æœç°æœ‰è®°å¿†ä¸å‡†ç¡®æˆ–è¿‡æ—¶ï¼Œéœ€è¦è¢«è¿™ä¸ªæ–°äº‹å®æ›¿æ¢
            4. NONE: å¦‚æœè¿™ä¸ªäº‹å®å·²ç»å®Œå…¨åŒ…å«åœ¨ç°æœ‰è®°å¿†ä¸­ï¼Œä¸éœ€è¦ä»»ä½•æ“ä½œ
            
            è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹ç»“æ„ï¼š
            {
              "memory": [
                {
                  "id": "ç°æœ‰è®°å¿†çš„IDï¼ˆå¦‚æœæ“ä½œæ˜¯UPDATEæˆ–DELETEï¼‰ï¼Œå¦åˆ™ä¸ºnull",
                  "text": "æ–°äº‹å®çš„å†…å®¹",
                  "event": "ADD/UPDATE/DELETE/NONE",
                  "old_memory": "å¦‚æœæ˜¯UPDATEæˆ–DELETEï¼Œå¯¹åº”çš„æ—§è®°å¿†å†…å®¹",
                  "details": "äº‹å®çš„è¯¦ç»†ä¿¡æ¯"
                }
              ],
              "core_memory": "æ›´æ–°åçš„æ ¸å¿ƒè®°å¿†ï¼ˆå¦‚æœéœ€è¦ï¼‰"
            }
            """
            
            # è°ƒç”¨LLMè¿›è¡Œåˆ¤æ–­
            response = self.llm_client.chat.completions.create(
                model=MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
            )
            
            # è§£æå“åº”
            update_response = response.choices[0].message.content
            response_str = remove_code_blocks(update_response)
            response_json = json.loads(response_str)
            
            # æå–æ ¸å¿ƒè®°å¿†æ›´æ–°
            updated_core_memory = response_json.get("core_memory", core_memory)
            
            # æå–è®°å¿†æ“ä½œ
            memory_actions = response_json.get("memory", [])
            
            # æ˜ å°„å›å®é™…çš„è®°å¿†ID
            actions_with_actual_ids = []
            for action in memory_actions:
                mem_id = action.get("id")
                if mem_id in memory_id_mapping:
                    action["actual_id"] = memory_id_mapping[mem_id]
                actions_with_actual_ids.append(action)
            
            return updated_core_memory, actions_with_actual_ids
        
        except Exception as e:
            print(f"åˆ¤æ–­è®°å¿†æ“ä½œæ—¶å‡ºé”™: {e}")
            return core_memory, []

# åˆå§‹åŒ– Milvus é›†åˆ
try:
    milvus_collection = init_milvus_connection()
except Exception as e:
    print(f"åˆå§‹åŒ– Milvus å¤±è´¥ï¼Œç¨‹åºé€€å‡º: {e}")
    exit(1)

# Qdrantç›¸å…³å‡½æ•°å·²ç§»é™¤ï¼Œæ”¹ç”¨MemoryManagerç±»ä¸­çš„Milvusæ“ä½œæ–¹æ³•

def generate_response(llm_client, question, question_date, context, core_memory=""):
    full_context = f"{context}\nUser Profile (Core Memory):\n{core_memory}"
    
    prompt = LME_ANSWER_PROMPT.format(
        question=question,
        question_date=question_date,
        context=full_context
    )
    response = llm_client.chat.completions.create(
                model=MODEL_NAME,
                messages=[{"role": "system", "content": prompt}],
                temperature=0,
            )
    return response, full_context


 
def process_user_memory_infer(line):
    dates = line.get("haystack_dates")
    sessions = line.get("haystack_sessions")
    
    operation_counts = {"ADD": 0, "UPDATE": 0, "DELETE": 0, "NONE": 0}
    current_core_memory = "No core memory yet."
    
    # åˆå§‹åŒ–MemReaderAgentå’ŒMemoryManager
    mem_reader_agent = MemReaderAgent(llm_client=openai_client)
    memory_manager = MemoryManager(collection=milvus_collection, llm_client=openai_client)

    for session_id, session in enumerate(sessions):
        date = dates[session_id] + " UTC"
        date_format = "%Y/%m/%d (%a) %H:%M UTC"
        date_string = datetime.strptime(date, date_format).replace(tzinfo=timezone.utc)
        
        for turn_id in range(0, len(session), 2):
            # è§£æå¯¹è¯
            parsed_messages = parse_messages(session[turn_id:turn_id+2])
            print("="*40)
            print("parsed_messages:", parsed_messages)
            print("="*40)
            
            # Step 1: ä½¿ç”¨MemReaderAgentæå–äº‹å®å¹¶ç»´æŠ¤å·¥ä½œè®°å¿†
            # 1.1 æå–äº‹å®
            new_retrieved_facts = mem_reader_agent.extract_facts(parsed_messages, core_memory=current_core_memory)
            print(f"æ–°æ£€ç´¢åˆ°çš„äº‹å®: {new_retrieved_facts}")
            
            # 1.2 æ›´æ–°å·¥ä½œè®°å¿†
            mem_reader_agent.maintain_working_memory(new_retrieved_facts)
            
            # 1.3 æå–äº‹å®è¯¦æƒ…
            detailed_facts = mem_reader_agent.extract_fact_details(new_retrieved_facts)
            print(f"æå–çš„äº‹å®è¯¦æƒ…: {detailed_facts}")
            
            # 1.4 æ›´æ–°æ ¸å¿ƒè®°å¿†
            updated_core_memory = mem_reader_agent.update_core_memory(parsed_messages, current_core_memory=current_core_memory)
            if updated_core_memory and updated_core_memory != "No core memory yet.":
                current_core_memory = updated_core_memory
                print(f"æ›´æ–°åçš„æ ¸å¿ƒè®°å¿†: {current_core_memory}")
            
            # å¦‚æœæ²¡æœ‰æå–åˆ°äº‹å®ï¼Œè·³è¿‡åç»­å¤„ç†
            if not new_retrieved_facts or not detailed_facts:
                continue
            
            # Step 2: ä½¿ç”¨MemoryManageræœç´¢ç›¸å…³è®°å¿†
            # 2.1 æœç´¢ä¸æå–äº‹å®ç›¸å…³çš„ç°æœ‰è®°å¿†
            retrieved_old_facts = []
            # å¯¹æ¯ä¸ªè¯¦ç»†äº‹å®è¿›è¡Œæœç´¢
            for detailed_fact in detailed_facts:
                search_text = detailed_fact.get("target", detailed_fact.get("original_fact", ""))
                if search_text:
                    related_memories = memory_manager.search_memory(search_text, top_k=3)
                    for mem in related_memories:
                        # é¿å…é‡å¤æ·»åŠ 
                        if not any(existing_mem.get("id") == mem.get("id") for existing_mem in retrieved_old_facts):
                            retrieved_old_facts.append({
                                "id": mem.get("id"),
                                "text": mem.get("text", mem.get("content", "")),
                                "memory_type": mem.get("memory_type", "semantic"),
                                "details": mem.get("details", {})
                            })
            
            print(f"æ£€ç´¢åˆ°çš„ç°æœ‰è®°å¿†: {retrieved_old_facts}")
            
            # Step 3: ä½¿ç”¨MemoryManageråˆ¤æ–­è®°å¿†æ“ä½œ
            # 3.1 åˆ¤æ–­åº”è¯¥æ‰§è¡Œçš„è®°å¿†æ“ä½œ
            memory_action_result = memory_manager.judge_memory_action(
                detailed_facts=detailed_facts,
                existing_memories=retrieved_old_facts,
                core_memory=current_core_memory
            )
            
            # 3.2 æ›´æ–°æ ¸å¿ƒè®°å¿†ï¼ˆå¦‚æœè¿”å›äº†æ–°çš„æ ¸å¿ƒè®°å¿†ï¼‰
            if "core_memory" in memory_action_result and memory_action_result["core_memory"]:
                current_core_memory = memory_action_result["core_memory"]
            
            # Step 4: æ‰§è¡Œè®°å¿†æ“ä½œ
            memory_operations = memory_action_result.get("memory", [])
            for mem_op in memory_operations:
                event_type = mem_op.get("event")
                action_text = mem_op.get("text")
                memory_id = mem_op.get("id")
                details = mem_op.get("details", {})
                
                if event_type in operation_counts:
                    operation_counts[event_type] += 1
                
                # æ‰§è¡Œç›¸åº”çš„è®°å¿†æ“ä½œ
                if event_type == "ADD" and action_text:
                    # æ·»åŠ æ–°è®°å¿†åˆ°Milvus
                    new_memory_id = memory_manager.add_memory(
                        content=action_text,
                        memory_type="semantic",
                        details=details
                    )
                    if new_memory_id:
                        print(f"æˆåŠŸæ·»åŠ è®°å¿†: {new_memory_id} - {action_text[:50]}...")
                
                elif event_type == "UPDATE" and memory_id and action_text:
                    # æ›´æ–°ç°æœ‰è®°å¿†
                    success = memory_manager.update_memory(
                        memory_id=memory_id,
                        new_content=action_text,
                        details=details
                    )
                    if success:
                        print(f"æˆåŠŸæ›´æ–°è®°å¿†: {memory_id} - {action_text[:50]}...")
                
                elif event_type == "DELETE" and memory_id:
                    # åˆ é™¤è®°å¿†
                    success = memory_manager.delete_memory(memory_id)
                    if success:
                        print(f"æˆåŠŸåˆ é™¤è®°å¿†: {memory_id}")
                
                elif event_type == "NONE":
                    print(f"æ— éœ€æ“ä½œ: {action_text}")
            
            # è®°å½•å·¥ä½œè®°å¿†çŠ¶æ€
            print(f"å½“å‰å·¥ä½œè®°å¿†: {mem_reader_agent.working_memory}")
    
    # è¿”å›æ“ä½œç»Ÿè®¡å’Œæœ€æ–°çš„æ ¸å¿ƒè®°å¿†
    return operation_counts, current_core_memory


def process_user_memory(line):
    dates = line.get("haystack_dates")
    sessions = line.get("haystack_sessions")
    
    operation_counts = {"ADD": 0, "UPDATE": 0, "DELETE": 0, "NONE": 0}
    current_core_memory = "No core memory yet."
    
    # åˆå§‹åŒ–MemReaderAgentå’ŒMemoryManager
    mem_reader_agent = MemReaderAgent(llm_client=openai_client)
    memory_manager = MemoryManager(collection=milvus_collection, llm_client=openai_client)

    for session_id, session in enumerate(sessions):
        date = dates[session_id] + " UTC"
        date_format = "%Y/%m/%d (%a) %H:%M UTC"
        date_string = datetime.strptime(date, date_format).replace(tzinfo=timezone.utc)
        
        for turn_id in range(0, len(session), 2):
            # è§£æå¯¹è¯
            parsed_messages = parse_messages(session[turn_id:turn_id+2])
            print("="*40)
            print("parsed_messages:", parsed_messages)
            print("="*40)
            
            # Step 1: ä½¿ç”¨MemReaderAgentæå–äº‹å®å¹¶ç»´æŠ¤å·¥ä½œè®°å¿†
            # 1.1 æå–äº‹å®
            new_retrieved_facts = mem_reader_agent.extract_facts(parsed_messages, core_memory=current_core_memory)
            print(f"æ–°æ£€ç´¢åˆ°çš„äº‹å®: {new_retrieved_facts}")
            
            # 1.2 æ›´æ–°å·¥ä½œè®°å¿†
            mem_reader_agent.maintain_working_memory(new_retrieved_facts)
            
            # 1.3 æå–äº‹å®è¯¦æƒ…ï¼ˆä¿æŒç®€å•æ¨¡å¼çš„å®ç°ï¼‰
            detailed_facts = mem_reader_agent.extract_fact_details(new_retrieved_facts)
            print(f"æå–çš„äº‹å®è¯¦æƒ…: {detailed_facts}")
            
            # 1.4 æ›´æ–°æ ¸å¿ƒè®°å¿†
            updated_core_memory = mem_reader_agent.update_core_memory(parsed_messages, current_core_memory=current_core_memory)
            if updated_core_memory and updated_core_memory != "No core memory yet.":
                current_core_memory = updated_core_memory
                print(f"æ›´æ–°åçš„æ ¸å¿ƒè®°å¿†: {current_core_memory}")
            
            # å¦‚æœæ²¡æœ‰æå–åˆ°äº‹å®ï¼Œè·³è¿‡åç»­å¤„ç†
            if not new_retrieved_facts or not detailed_facts:
                continue
            
            # Step 2: ç®€åŒ–æ¨¡å¼ - ç›´æ¥å°†æ‰€æœ‰äº‹å®æ·»åŠ åˆ°Milvusï¼ˆä¸è¿›è¡ŒUPDATE/DELETEåˆ¤æ–­ï¼‰
            # è¿™ä¿æŒäº†åŸå‡½æ•°çš„ç®€å•é€»è¾‘ï¼Œåªè¿›è¡ŒADDæ“ä½œ
            for fact in new_retrieved_facts:
                try:
                    # ä¸ºæ¯ä¸ªäº‹å®åˆ›å»ºè¯¦ç»†ä¿¡æ¯
                    fact_detail = next((d for d in detailed_facts if d.get("original_fact") == fact), None)
                    details = fact_detail.get("details", {}) if fact_detail else {}
                    
                    # æ·»åŠ è®°å¿†åˆ°Milvus
                    memory_manager.add_memory(
                        content=fact,
                        memory_type="semantic",
                        details={
                            "original_fact": fact,
                            "extracted_details": details,
                            "created_at": date_string.isoformat()
                        }
                    )
                    operation_counts["ADD"] += 1
                    print(f"æˆåŠŸæ·»åŠ äº‹å®åˆ°Milvus: {fact[:50]}...")
                except Exception as e:
                    print(f"æ·»åŠ äº‹å®æ—¶å‡ºé”™: {e}")
            
            # è®°å½•å·¥ä½œè®°å¿†çŠ¶æ€
            print(f"å½“å‰å·¥ä½œè®°å¿†: {mem_reader_agent.working_memory}")
    
    # è¿”å›æ“ä½œç»Ÿè®¡å’Œæœ€æ–°çš„æ ¸å¿ƒè®°å¿†
    return operation_counts, current_core_memory

def response_user(line, core_memory):
    question = line.get("question")
    question_date = line.get("question_date")
    question_date = question_date + " UTC"
    question_date_format = "%Y/%m/%d (%a) %H:%M UTC"
    question_date_string = datetime.strptime(question_date, question_date_format).replace(tzinfo=timezone.utc)
    
    # åˆå§‹åŒ–MemoryManagerç”¨äºæ£€ç´¢è®°å¿†
    memory_manager = MemoryManager(collection=milvus_collection, llm_client=openai_client)
    
    # ä»Milvusæ£€ç´¢ç›¸å…³è®°å¿†
    retrieved_memories = memory_manager.search_memory(query_text=question, top_k=search_topk)
    
    # æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„è®°å¿†ä¸ºå­—ç¬¦ä¸²
    memories_str = ""
    if retrieved_memories:
        memories_str = "\n".join([
            f"- {mem.get('created_at', 'Unknown time')}: {mem.get('content', '')}"
            for mem in retrieved_memories
        ])
    else:
        memories_str = "æ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³è®°å¿†"
    
    # ä¼ å…¥Core Memoryç”Ÿæˆå›ç­”
    response, full_context = generate_response(openai_client, question, question_date_string, memories_str, core_memory)
    answer = response.choices[0].message.content

    return full_context, answer

def process_and_evaluate_user(line, user_index, client, infer):
    try:
        if infer:
            # è°ƒç”¨æ›´æ–°åçš„ process_user_memory_infer å‡½æ•°
            result = process_user_memory_infer(line)
            memory_counts = result.get('memory_count', {"ADD": 0, "UPDATE": 0, "DELETE": 0, "NONE": 0})
            core_memory = result.get('core_memory', {})
        else:
            # è°ƒç”¨æ›´æ–°åçš„ process_user_memory å‡½æ•°
            result = process_user_memory(line)
            memory_counts = result.get('memory_count', {"ADD": 0, "UPDATE": 0, "DELETE": 0, "NONE": 0})
            core_memory = result.get('core_memory', {})
        
        # å°† Core Memory ä¼ å…¥ response_user
        full_context, answer = response_user(line, core_memory)
        
        golden_answer = line.get("answer") 
        question = line.get("question")
        
        # ä½¿ç”¨ lme_grader è¯„ä¼°å›ç­”æ­£ç¡®æ€§
        is_correct = False
        try:
            is_correct = lme_grader(client, question, golden_answer, answer)
        except Exception as grader_error:
            print(f"ä½¿ç”¨ lme_grader è¯„ä¼°æ—¶å‡ºé”™: {str(grader_error)}")
        
        # è¿”å›å¤„ç†ç»“æœ
        return {
            "index": user_index,
            "is_correct": is_correct,
            "counts": memory_counts,
            "core_memory": core_memory, # è®°å½•
            "retrieved_memories": full_context,
            "question": question,
            "answer": answer,
            "golden_answer": golden_answer
        }
    except Exception as e:
        print(f"Error processing user {user_index}: {e}")
        # è¿”å›é”™è¯¯æƒ…å†µä¸‹çš„åŸºæœ¬ä¿¡æ¯
        return {
            "index": user_index,
            "is_correct": False,
            "counts": {"ADD": 0, "UPDATE": 0, "DELETE": 0, "NONE": 0},
            "core_memory": "No core memory yet.",
            "question": line.get("question", "N/A")
        }

if __name__ == "__main__":
    # ä¸»ç¨‹åºå…¥å£ - ä½¿ç”¨ Milvus ä½œä¸ºå›¾æ•°æ®åº“
    # Milvus å·²åœ¨æ–‡ä»¶å¼€å¤´é€šè¿‡ init_milvus_connection() åˆå§‹åŒ–
    print(f"å·²æˆåŠŸè¿æ¥åˆ° Milvus æœåŠ¡å™¨: {MILVUS_HOST}:{MILVUS_PORT}")
    print(f"ä½¿ç”¨é›†åˆ: {COLLECTION_NAME}")
    
    # å¯é€‰ï¼šæ¸…ç©º Milvus é›†åˆä¸­çš„æ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
    try:
        # ç¡®ä¿é›†åˆå·²åŠ è½½
        milvus_collection.load()
        # åˆ é™¤æ‰€æœ‰æ•°æ®
        expr = ""
        milvus_collection.delete(expr)
        print("å·²æ¸…ç©º Milvus é›†åˆä¸­çš„æ‰€æœ‰æ•°æ®")
    except Exception as e:
        print(f"æ¸…ç©º Milvus æ•°æ®æ—¶å‘ç”Ÿå¼‚å¸¸ï¼ˆç»§ç»­æ‰§è¡Œï¼‰: {e}")

    with open("./data/longmemeval_s_cleaned.json", "r") as f:
        lines = json.load(f)[:50]
    
    print(f"å·²åŠ è½½ {len(lines)} ä¸ªç”¨æˆ·/é—®é¢˜ã€‚")

    user_detail_results = [] 
    total_memory_counts = {"ADD": 0, "UPDATE": 0, "DELETE": 0, "NONE": 0}
    
    MAX_WORKERS = 10
    infer = False

    futures = []

    print(f"å¼€å§‹ä½¿ç”¨ {MAX_WORKERS} ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†...")
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        for idx, line in enumerate(lines):
            future = executor.submit(process_and_evaluate_user, line, idx + 1, openai_client, infer=infer)
            futures.append(future)
        
        for future in tqdm(as_completed(futures), total=len(futures), desc="è¯„ä¼°è¿›åº¦"):
            result = future.result()
            user_detail_results.append(result)

    user_detail_results.sort(key=lambda x: x.get("index", 0))

    correct_count = 0
    total_evaluated = len(user_detail_results)

    for res in user_detail_results:
        if res.get("is_correct"):
            correct_count += 1
        
        counts = res.get("counts", {})
        for key in total_memory_counts:
            total_memory_counts[key] += counts.get(key, 0)

    print("\n\n==================================================")
    print("              ğŸ¯ æœ€ç»ˆè¯„ä¼°ç»“æœ") 
    print("==================================================")

    if total_evaluated > 0:
        final_accuracy = correct_count / total_evaluated
        print(f"æ€»è¯„ä¼°é—®é¢˜æ•°: {total_evaluated}")
        print(f"æ­£ç¡®å›ç­”æ•°: {correct_count}")
        print(f"æœ€ç»ˆæ€»å‡†ç¡®ç‡: {final_accuracy:.4f} ({final_accuracy * 100:.2f}%)")
    else:
        print("æ²¡æœ‰è¯„ä¼°ä»»ä½•é—®é¢˜ã€‚")
    print("==================================================")

    print("\n\n==================================================")
    print("        ğŸ“Š è¯¦ç»†è®°å¿†æ“ä½œç»Ÿè®¡ (æŒ‰ç”¨æˆ·)")
    print("==================================================")

    for res in user_detail_results:
        user_index = res["index"]
        is_correct = res["is_correct"]
        counts = res["counts"]
        question = res["question"]
        answer = res.get("answer", "N/A")
        golden_answer = res.get("golden_answer", "N/A")
        core_mem = res.get("core_memory", "N/A")
        status = "âœ… CORRECT" if is_correct else "âŒ WRONG"
        
        print(f"\n--- ç”¨æˆ·/é—®é¢˜ {user_index} ---")
        print(f"  é—®é¢˜: {question}...")
        # print(f"  Core Memory: {core_mem}") 
        print(f"  æ£€ç´¢è®°å¿†: {res.get('retrieved_memories', '')}...")
        print(f"  æ¨¡å‹å›ç­”: {answer}...")
        print(f"  æ ‡å‡†ç­”æ¡ˆ: {golden_answer}...")
        print(f"  è¯„ä¼°ç»“æœ: {status}")
        print(f"  è®°å¿†æ“ä½œ: ADD={counts.get('ADD', 0)}, UPDATE={counts.get('UPDATE', 0)}, DELETE={counts.get('DELETE', 0)}, NONE={counts.get('NONE', 0)}")

    print("\n--- æ‰€æœ‰ç”¨æˆ·çš„è®°å¿†æ“ä½œæ€»è§ˆ ---")
    print(f"  ADD (æ–°å¢):    {total_memory_counts['ADD']}")
    print(f"  UPDATE (æ›´æ–°): {total_memory_counts['UPDATE']}")
    print(f"  DELETE (åˆ é™¤): {total_memory_counts['DELETE']}")
    print(f"  NONE (æ— æ“ä½œ): {total_memory_counts['NONE']}")
    print("==================================================")