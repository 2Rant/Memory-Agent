import os
import time
import uuid
import json
import numpy as np
from typing import List, Dict, Optional, Any, Union
from dataclasses import dataclass
from dotenv import load_dotenv
from openai import OpenAI
from utils import (MEMREADER_PROMPT, 
                   get_embedding, parse_messages, LME_JUDGE_MODEL_TEMPLATE, 
                   LME_ANSWER_PROMPT, remove_code_blocks, extract_json)
from lme_eval import lme_grader
from datetime import datetime, timezone
import pytz
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from vector_db import VectorDBConfig, VectorDBFactory
# ==========================================
# 0. Setup & Prompts
# ==========================================
load_dotenv()

# âš ï¸ è¯·ç¡®ä¿ç¯å¢ƒå˜é‡ä¸­æœ‰ OPENAI_API_KEY å’Œ MILVUS_URI
# å¦‚æœæ˜¯æœ¬åœ°æµ‹è¯•ï¼Œç¡®ä¿ Docker ä¸­ Milvus å·²å¯åŠ¨

llm_client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"), 
    base_url=os.getenv("OPENAI_BASE_URL")
)

MEMORY_MANAGER_PROMPT = """You are a specialized Memory Manager Agent.
Your role is to maintain the consistency and growth of a memory graph using the provided tools.

[INPUTS]
You will receive:
1. "New Facts": A list of atomic facts extracted from the latest user input.
2. "Existing Memories": A list of retrieved memory items, each with a simplified Integer ID (e.g., "0", "1", "2").
   - These memories include those directly related to the new facts, as well as other related facts connected with these memories.
   - They form a connected graph of information relevant to the new facts.
3. "Existing Facts": A list of retrieved fact items, each with a simplified Integer ID (e.g., "F0", "F1", "F2").
   - These facts include those directly related to the new facts, with their text and details.
   - They provide additional context and historical information about the new facts.

[MANDATORY OUTPUT FORMAT]
For every new fact you process, you MUST:
1. First generate a detailed thinking process
2. Then call the appropriate tool

[THINKING PROCESS REQUIREMENTS]
Your thinking process MUST include:
- The specific new fact you're analyzing
- Which existing memories are relevant (with their IDs)
- Which existing facts are relevant (with their IDs)
- How memories and facts are connected
- Your comparison and reasoning
- Which operation you've decided to perform and why

[OPERATIONS & GUIDELINES]
Compare New Facts with Existing Memories and Existing Facts, then perform the following operations using the available tools. 
DO NOT output raw JSON text. You MUST use the provided function tools.

1. **ADD MEMORY (create_memory)**
   - **Condition**: If a fact contains completely NEW information not present in Existing Memories or Existing Facts.
   - **Action**: Call `create_memory` with a concise summary of the facts, not just a simple concatenation.
   - **Important**: Memory content should be a meaningful and concise summary.
  
2. **UPDATE MEMORY (update_memory)**
   - **Condition**: If a fact adds detail, corrects, or updates a specific Existing Memory.
   - **Constraint**: You MUST use the Integer ID (e.g., "0") provided in the input as the `target_memory_id`.
   - **Logic**: Merge the old content and new fact into a comprehensive statement, not just a simple concatenation.
   - **Example**:
     - Old (ID="0"): "User likes generic pizza."
     - New Fact: "User loves pepperoni pizza."
     - Action: `update_memory(target_memory_id="0", new_content="User loves pepperoni pizza", ...)`

3. **DELETE MEMORY (delete_memory)**
   - **Condition**: If a fact explicitly contradicts an Existing Memory (and the new fact is trusted), or if the memory is no longer valid.
   - **Constraint**: Use the Integer ID (e.g., "1") as `target_memory_id`.

4. **INFER MEMORY (infer_memory)**
   - **Condition**: Look for higher-level insights. If combining "Memory A" and "Memory B" reveals a hidden connection or causality.
   - **Action**: Call `infer_memory`.
   - **Example**:
     - Memory A (ID="2"): "User moved to Singapore."
     - Memory B (ID="3"): "User bought a Type G power adapter."
     - Inference: "User is preparing electronics for Singapore power standards."
     - Action: `infer_memory(source_memory_ids=["2", "3"], inference_content="...")`

5. **ADD FACT (fact_add)**
   - **Condition**: If a new fact is completely unrelated to all Existing Facts.
   - **Action**: Call `fact_add` to save the new fact to the database.
   - **Example**:
     - New Fact: "User just bought a new laptop."
     - Details: ["brand: Apple", "model: MacBook Pro", "purchase_date: 2023-10-15"]
     - No related Existing Facts found.
     - Action: `fact_add(content="User just bought a new laptop", details=["brand: Apple", "model: MacBook Pro", "purchase_date: 2023-10-15"])`

6. **UPDATE FACT (fact_trajectorize)**
   - **Condition**: If a new fact is related to an Existing Fact (e.g., updates, corrects, or expands it).
   - **Action**: Call `fact_trajectorize` to update the Existing Fact to the new content while recording the change trajectory.
   - **Constraint**: You MUST use the Integer ID (e.g., "F0") provided in the input as the `target_fact_id`.
   - **Example**:
     - Old Fact (ID="F0"): "User lives in Beijing."
     - Old Details: ["city: Beijing", "duration: 5 years"]
     - New Fact: "User moved to Shanghai."
     - New Details: ["city: Shanghai", "duration: 0 months"]
     - Action: `fact_trajectorize(target_fact_id="F0", new_content="User moved to Shanghai", diff="Changed residence from Beijing to Shanghai", details=["city: Shanghai", "duration: 0 months"])`

7. **NOOP (no_operation)**
   - **Condition**: If the fact is redundant (already exactly covered by memory or fact) or trivial.
"""

# --- TOOLS ---
MEMORY_TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "create_memory",
            "description": "Create a NEW independent memory node with a concise summary of the facts.",
            "parameters": {
                "type": "object",
                "properties": {
                    "content": {"type": "string", "description": "The concise summary content of the new memory, not just a list of facts."},
                    "evidence_facts": {"type": "array", "items": {"type": "string"}, "description": "Facts supporting this memory."}
                },
                "required": ["content", "evidence_facts"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "update_memory",
            "description": "Update an existing memory by merging the old content and new fact into a comprehensive, concise statement.",
            "parameters": {
                "type": "object",
                "properties": {
                    "target_memory_id": {"type": "string", "description": "The simplified Integer ID (e.g., '0') of the memory to update, found in the [EXISTING MEMORIES] list."},
                    "new_content": {"type": "string", "description": "The merged/updated comprehensive statement."},
                    "evidence_facts": {"type": "array", "items": {"type": "string"}, "description": "Facts supporting this update."}
                },
                "required": ["target_memory_id", "new_content", "evidence_facts"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "infer_memory",
            "description": "Look for higher-level insights. If combining multiple existing memories reveals a hidden connection or causality, create an inferred memory.",
            "parameters": {
                "type": "object",
                "properties": {
                    "source_memory_ids": {"type": "array", "items": {"type": "string"}, "description": "List of simplified Integer IDs (e.g., ['0', '1']) acting as premises, found in the [EXISTING MEMORIES] list."},
                    "inference_content": {"type": "string", "description": "The higher-level insight or inference derived from combining the source memories."},
                    "evidence_facts": {"type": "array", "items": {"type": "string"}, "description": "Facts supporting this inference."}
                },
                "required": ["source_memory_ids", "inference_content", "evidence_facts"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "delete_memory",
            "description": "Archive/Soft-delete a memory if it explicitly contradicts a new fact (and the new fact is trusted), or if the memory is no longer valid.",
            "parameters": {
                "type": "object",
                "properties": {
                    "target_memory_id": {"type": "string", "description": "The simplified Integer ID (e.g., '1') of the memory to delete, found in the [EXISTING MEMORIES] list."},
                    "evidence_facts": {"type": "array", "items": {"type": "string"}, "description": "Facts supporting this deletion."}
                },
                "required": ["target_memory_id", "evidence_facts"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "fact_add",
            "description": "Add a new fact to the database when it's completely unrelated to all existing facts.",
            "parameters": {
                "type": "object",
                "properties": {
                    "content": {"type": "string", "description": "The content of the new fact."},
                    "details": {"type": "array", "items": {"type": "string"}, "description": "Additional details about the fact, as strings in format 'key: value'."}
                },
                "required": ["content", "details"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "fact_trajectorize",
            "description": "Update an existing fact with new content while recording the change trajectory.",
            "parameters": {
                "type": "object",
                "properties": {
                    "target_fact_id": {"type": "string", "description": "The simplified Integer ID (e.g., 'F0') of the fact to update, found in the [EXISTING FACTS] list."},
                    "new_content": {"type": "string", "description": "The new content for the fact."},
                    "diff": {"type": "string", "description": "Description of the difference between old and new content."},
                    "details": {"type": "array", "items": {"type": "string"}, "description": "Additional details about the fact, as strings in format 'key: value'."}
                },
                "required": ["target_fact_id", "new_content", "diff", "details"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "no_operation",
            "description": "No action needed if the fact is redundant (already exactly covered by memory or fact) or trivial.",
            "parameters": {
                "type": "object",
                "properties": {"reason": {"type": "string", "description": "The reason for no operation."}},
                "required": ["reason"]
            }
        }
    }
]

# --- UTILS ---
def get_embedding(text: str) -> List[float]:
    text = text.replace("\n", " ")
    return llm_client.embeddings.create(input=[text], model="text-embedding-3-small").data[0].embedding

@dataclass
class MilvusConfig:
    """Milvusé…ç½®ç±»ï¼ˆå…¼å®¹æ—§ä»£ç ï¼‰"""
    uri: str = os.getenv("MILVUS_URI")
    user_name: str = os.getenv("MILVUS_USER_NAME")
    # password: str = os.getenv("MILVUS_PASSWORD")
    db_name: str = os.getenv("MILVUS_DB_NAME", "default")
    dimension: int = 1536
    
    def to_vector_db_config(self, vector_db_type: str = "milvus") -> VectorDBConfig:
        """è½¬æ¢ä¸ºVectorDBConfig"""
        # ç¡®ä¿vector_db_typeæ˜¯å­—ç¬¦ä¸²ç±»å‹
        if not isinstance(vector_db_type, str):
            vector_db_type = "milvus"  # é»˜è®¤ä½¿ç”¨milvus
        
        # æ ¹æ®vector_db_typeé€‰æ‹©ä¸åŒçš„URL
        if vector_db_type == "qdrant":
            uri = os.getenv("QDRANT_URL")
            api_key = os.getenv("QDRANT_API_KEY")
            user_name = ""
            password = ""
        else:
            uri = self.uri
            api_key = ""
            user_name = self.user_name
            password = os.getenv("MILVUS_PASSWORD")
        
        return VectorDBConfig(
            uri=uri,
            user_name=user_name,
            password=password,
            api_key=api_key,
            db_name=self.db_name,
            dimension=self.dimension,
            vector_db_type=vector_db_type
        )

# ==========================================
# 1. Pipeline Class
# ==========================================
class MemoryPipeline:
    def __init__(self, config=None, vector_db_type="milvus", clear_db=False, mode='eval', dataset_name=""):
        """
        åˆå§‹åŒ–MemoryPipeline
        
        Args:
            config: MilvusConfigæˆ–VectorDBConfigå®ä¾‹ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
            vector_db_type: æŒ‡å®šä½¿ç”¨çš„å‘é‡æ•°æ®åº“ç±»å‹ï¼Œæ”¯æŒ"milvus"æˆ–"qdrant"
            clear_db: æ˜¯å¦æ¸…ç©ºæ•°æ®åº“ï¼Œé»˜è®¤ä¸ºFalse
            dataset_name: æ•°æ®é›†åç§°ï¼Œç”¨äºé›†åˆåç§°åç¼€ï¼Œé»˜è®¤ä¸ºç©º
        """
        # å¦‚æœæ²¡æœ‰æä¾›é…ç½®ï¼Œåˆ›å»ºé»˜è®¤é…ç½®
        if config is None:
            config = MilvusConfig()
        
        self.config = config
        
        # è½¬æ¢ä¸ºVectorDBConfig
        if hasattr(config, 'to_vector_db_config'):
            vector_db_config = config.to_vector_db_config(vector_db_type=vector_db_type)
        else:
            # å¦‚æœå·²ç»æ˜¯VectorDBConfigå®ä¾‹ï¼Œç›´æ¥ä½¿ç”¨
            vector_db_config = config
        
        # ä½¿ç”¨å·¥å‚ç±»åˆ›å»ºå‘é‡æ•°æ®åº“å®¢æˆ·ç«¯
        self.client = VectorDBFactory.create_db(vector_db_config)
        
        # æ ¹æ®æ¨¡å¼å’Œæ•°æ®é›†åç§°è®¾ç½®é›†åˆåç§°
        base_suffix = "_test" if mode == 'test' else ""
        dataset_suffix = f"_{dataset_name}" if dataset_name else ""
        full_suffix = f"{base_suffix}{dataset_suffix}"
        
        self.mem_col = f"memories{full_suffix}"
        self.fact_col = f"facts{full_suffix}"
        self.chunk_col = f"chunks{full_suffix}"
        
        self.dim = vector_db_config.dimension  # Save dimension as instance variable
        # åˆå§‹åŒ–æ“ä½œæ¬¡æ•°è®¡æ•°å™¨
        self.operation_counts = {"ADD": 0, "UPDATE": 0, "DELETE": 0, "INFER": 0, "NOOP": 0}
        self._init_collections(clear_db=clear_db)

    def _init_collections(self, clear_db=False):
        dim = self.config.dimension
        
        # å¦‚æœéœ€è¦æ¸…ç©ºæ•°æ®åº“ï¼Œå…ˆåˆ é™¤æ‰€æœ‰é›†åˆ
        if clear_db:
            print("æ­£åœ¨æ¸…ç©ºæ•°æ®åº“...")
            # ç›´æ¥åˆ é™¤é›†åˆï¼Œä¸æ£€æŸ¥å­˜åœ¨æ€§
            self.client.drop_collection(self.mem_col)
            self.client.drop_collection(self.fact_col)
            self.client.drop_collection(self.chunk_col)
            print("æ•°æ®åº“æ¸…ç©ºå®Œæˆ.")
        
        # ç›´æ¥åˆ›å»ºæˆ–è·å–é›†åˆï¼Œä¸è¿›è¡Œå­˜åœ¨æ€§æ£€æŸ¥
        # åˆ›å»ºé›†åˆçš„é€»è¾‘å·²ç»åŒ…å«äº†å¦‚æœé›†åˆå­˜åœ¨åˆ™è·³è¿‡çš„å¤„ç†
        
        # å¤„ç† memories é›†åˆ
        if hasattr(self.client, 'DataType'):
            # è¿™æ˜¯ Milvus å®¢æˆ·ç«¯ï¼Œåˆ›å»ºå®Œæ•´çš„schema
            s = self.client.create_schema(auto_id=False, enable_dynamic_field=True)
            s.add_field("memory_id", self.client.DataType.VARCHAR, max_length=64, is_primary=True)
            s.add_field("embedding", self.client.DataType.FLOAT_VECTOR, dim=dim)
            s.add_field("content", self.client.DataType.VARCHAR, max_length=65535)
            s.add_field("user_id", self.client.DataType.VARCHAR, max_length=64)
            s.add_field("status", self.client.DataType.VARCHAR, max_length=16)
            s.add_field("created_at", self.client.DataType.INT64)
            s.add_field("updated_at", self.client.DataType.INT64)
            s.add_field("relations", self.client.DataType.JSON) 
            
            # åˆ›å»ºé›†åˆ
            self.client.create_collection(self.mem_col, schema=s)
            print(f"Collection '{self.mem_col}' created or exists.")
            
            # ç›´æ¥åˆ›å»ºç´¢å¼•ï¼Œä¸æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            # Milvusçš„create_indexæ–¹æ³•ä¼šåœ¨ç´¢å¼•å·²å­˜åœ¨æ—¶è‡ªåŠ¨è·³è¿‡æˆ–è¿”å›æˆåŠŸ
            try:
                print(f"ä¸ºé›†åˆ '{self.mem_col}' åˆ›å»ºç´¢å¼•...")
                idx_params = self.client.prepare_index_params()
                idx_params.add_index(field_name="embedding", index_type="IVF_FLAT", metric_type="COSINE", params={"nlist": 128})
                self.client.create_index(self.mem_col, index_params=idx_params)
                print(f"é›†åˆ '{self.mem_col}' çš„ç´¢å¼•åˆ›å»ºæˆåŠŸæˆ–å·²å­˜åœ¨")
            except Exception as e:
                print(f"åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
        else:
            # éMilvuså®¢æˆ·ç«¯ï¼Œç›´æ¥åˆ›å»ºé›†åˆ
            self.client.create_collection(self.mem_col)
            print(f"Collection '{self.mem_col}' created or exists.")
        
        # å¤„ç† facts é›†åˆ
        if hasattr(self.client, 'DataType'):
            s = self.client.create_schema(auto_id=False, enable_dynamic_field=True)
            s.add_field("fact_id", self.client.DataType.VARCHAR, max_length=64, is_primary=True)
            s.add_field("linked_memory_ids", self.client.DataType.JSON)
            s.add_field("linked_chunk_id", self.client.DataType.VARCHAR, max_length=64)
            s.add_field("text", self.client.DataType.VARCHAR, max_length=65535)
            s.add_field("details", self.client.DataType.JSON)  # æ·»åŠ detailså­—æ®µ
            s.add_field("timestamp", self.client.DataType.INT64)
            s.add_field("user_id", self.client.DataType.VARCHAR, max_length=64)  # æ·»åŠ user_idå­—æ®µ
            s.add_field("embedding", self.client.DataType.FLOAT_VECTOR, dim=dim)
            s.add_field("trajectory", self.client.DataType.JSON, default=[])  # æ·»åŠ trajectoryå­—æ®µï¼Œè®°å½•å˜åŒ–è½¨è¿¹
            
            # åˆ›å»ºé›†åˆ
            self.client.create_collection(self.fact_col, schema=s)
            print(f"Collection '{self.fact_col}' created or exists.")
            
            # ç›´æ¥åˆ›å»ºç´¢å¼•ï¼Œä¸æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            # Milvusçš„create_indexæ–¹æ³•ä¼šåœ¨ç´¢å¼•å·²å­˜åœ¨æ—¶è‡ªåŠ¨è·³è¿‡æˆ–è¿”å›æˆåŠŸ
            try:
                print(f"ä¸ºé›†åˆ '{self.fact_col}' åˆ›å»ºç´¢å¼•...")
                idx_params = self.client.prepare_index_params()
                idx_params.add_index(field_name="embedding", index_type="IVF_FLAT", metric_type="COSINE", params={"nlist": 128})
                self.client.create_index(self.fact_col, index_params=idx_params)
                print(f"é›†åˆ '{self.fact_col}' çš„ç´¢å¼•åˆ›å»ºæˆåŠŸæˆ–å·²å­˜åœ¨")
            except Exception as e:
                print(f"åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
        else:
            # éMilvuså®¢æˆ·ç«¯ï¼Œç›´æ¥åˆ›å»ºé›†åˆ
            self.client.create_collection(self.fact_col)
            print(f"Collection '{self.fact_col}' created or exists.")
        
        # å¤„ç† chunks é›†åˆ
        if hasattr(self.client, 'DataType'):
            s = self.client.create_schema(auto_id=False, enable_dynamic_field=True)
            s.add_field("chunk_id", self.client.DataType.VARCHAR, max_length=64, is_primary=True)
            s.add_field("text", self.client.DataType.VARCHAR, max_length=65535)
            s.add_field("timestamp", self.client.DataType.INT64)
            s.add_field("embedding", self.client.DataType.FLOAT_VECTOR, dim=dim)
            
            # åˆ›å»ºé›†åˆ
            self.client.create_collection(self.chunk_col, schema=s)
            print(f"Collection '{self.chunk_col}' created or exists.")
            
            # ç›´æ¥åˆ›å»ºç´¢å¼•ï¼Œä¸æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            # Milvusçš„create_indexæ–¹æ³•ä¼šåœ¨ç´¢å¼•å·²å­˜åœ¨æ—¶è‡ªåŠ¨è·³è¿‡æˆ–è¿”å›æˆåŠŸ
            try:
                print(f"ä¸ºé›†åˆ '{self.chunk_col}' åˆ›å»ºç´¢å¼•...")
                idx_params = self.client.prepare_index_params()
                idx_params.add_index(field_name="embedding", index_type="IVF_FLAT", metric_type="COSINE", params={"nlist": 128})
                self.client.create_index(self.chunk_col, index_params=idx_params)
                print(f"é›†åˆ '{self.chunk_col}' çš„ç´¢å¼•åˆ›å»ºæˆåŠŸæˆ–å·²å­˜åœ¨")
            except Exception as e:
                print(f"åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
        else:
            # éMilvuså®¢æˆ·ç«¯ï¼Œç›´æ¥åˆ›å»ºé›†åˆ
            self.client.create_collection(self.chunk_col)
            print(f"Collection '{self.chunk_col}' created or exists.")
        
        # ç›´æ¥åŠ è½½æ‰€æœ‰é›†åˆï¼Œä¸è¿›è¡Œå¤æ‚çš„é”™è¯¯å¤„ç†
        print("Loading collections into memory...")
        
        # åŠ è½½é›†åˆï¼ˆQdrant ä¸éœ€è¦æ˜¾å¼åŠ è½½ï¼‰
        if hasattr(self.client, 'load_collection'):
            # ä¸ºæ¯ä¸ªé›†åˆåˆ›å»ºç´¢å¼•åç›´æ¥åŠ è½½
            print(f"åŠ è½½é›†åˆ '{self.mem_col}'...")
            self.client.load_collection(self.mem_col)
            
            print(f"åŠ è½½é›†åˆ '{self.fact_col}'...")
            self.client.load_collection(self.fact_col)
            
            print(f"åŠ è½½é›†åˆ '{self.chunk_col}'...")
            self.client.load_collection(self.chunk_col)
            
            print("All collections loaded successfully.")

    # --- Step 1: Extract ---
    def step_extract(self, chunk_text: str, extract_mode: str = "whole") -> Dict:
        """
        ä»å¯¹è¯ä¸­æå–äº‹å®
        
        Args:
            chunk_text: å¯¹è¯æ–‡æœ¬
            extract_mode: æå–æ¨¡å¼ï¼Œå¯é€‰å€¼ï¼š
                - "whole": å¯¹æ•´ä¸ªchunkè¿›è¡Œæå–
                - "turn": æŒ‰è½®æ¬¡æå–ï¼Œæ¯è½®user-assistantå¯¹è¯å•ç‹¬æå–
        
        Returns:
            åŒ…å«æå–äº‹å®çš„å­—å…¸
        """
        # print(f"\nğŸ‘€ [1. Extract] Processing: '{chunk_text}'")
        
        # å¦‚æœæ˜¯æŒ‰è½®æ¬¡æå–ï¼Œå…ˆè§£æå¯¹è¯è½®æ¬¡
        if extract_mode == "turn":
            # å°è¯•è§£æå¯¹è¯è½®æ¬¡
            try:
                # ç®€å•çš„è½®æ¬¡æ£€æµ‹ï¼šæŸ¥æ‰¾user:å’Œassistant:çš„ç»„åˆ
                import re
                # åŒ¹é…user: ... assistant: ... çš„æ¨¡å¼
                turn_pattern = r'(user: .*?)(?=assistant: |$)' 
                turns = re.findall(turn_pattern, chunk_text, re.DOTALL)
                
                # å¦‚æœæ‰¾åˆ°è½®æ¬¡ï¼Œå•ç‹¬å¤„ç†æ¯è½®
                if turns:
                    all_facts = []
                    for turn in turns:
                        # ç¡®ä¿æ¯è½®éƒ½æœ‰å®Œæ•´çš„user-assistantå¯¹è¯
                        turn_text = turn.strip()
                        if turn_text:
                            # å¯¹å•è½®å¯¹è¯æå–äº‹å®
                            turn_facts = self._extract_single_turn(turn_text)
                            all_facts.extend(turn_facts)
                    
                    return {"chunk_id": str(uuid.uuid4()), "chunk_text": chunk_text, "new_facts": all_facts, "timestamp": int(time.time())}
            except Exception as e:
                print(f"è§£æå¯¹è¯è½®æ¬¡å¤±è´¥ï¼Œå›é€€åˆ°wholeæ¨¡å¼: {e}")
        
        # é»˜è®¤æ¨¡å¼ï¼šå¯¹æ•´ä¸ªchunkè¿›è¡Œæå–
        facts = self._extract_single_turn(chunk_text)
        return {"chunk_id": str(uuid.uuid4()), "chunk_text": chunk_text, "new_facts": facts, "timestamp": int(time.time())}
    
    def _extract_single_turn(self, text: str) -> List[Dict]:
        """
        å¯¹å•ä¸ªæ–‡æœ¬ç‰‡æ®µæå–äº‹å®
        
        Args:
            text: è¦æå–äº‹å®çš„æ–‡æœ¬
            
        Returns:
            æå–åˆ°çš„äº‹å®åˆ—è¡¨
        """
        try:
            response = llm_client.chat.completions.create(
                model="gpt-4.1",
                messages=[
                        {"role": "system", "content": MEMREADER_PROMPT}, 
                        {"role": "user", "content": text}],
                response_format={"type": "json_object"}, temperature=0
            )
            fact_objects = json.loads(response.choices[0].message.content).get("facts", [])
            # ä¿ç•™å®Œæ•´çš„factå¯¹è±¡ï¼ŒåŒ…æ‹¬detailsä¿¡æ¯
            facts = []
            for fact_obj in fact_objects:
                if fact_obj.get("fact"):
                    facts.append({
                        "text": fact_obj.get("fact", ""),
                        "details": fact_obj.get("details", [])
                    })
        except Exception as e: 
            print(f"Extraction failed: {e}")
            facts = [{"text": text, "details": []}]
        return facts

    # --- Step 2: Retrieve ---    
    def step_retrieve(self, extract_result: Dict, limit: int = 3, user_id: str = 'default', similarity_threshold: float = None) -> List[Dict]:
        new_facts = extract_result['new_facts']
        if not new_facts: return []
        
        print(f"ğŸ” [2. Retrieve] Searching Memories and Facts for {len(new_facts)} facts...")
        context_bundles = []

        for fact in new_facts:
            query_vec = get_embedding(fact['text'])
            
            # 1. æœç´¢ç›¸å…³çš„memories
            print(f"   Searching memories for fact: {fact['text'][:50]}...")
            mem_res = self.client.search(
                self.mem_col, [query_vec], filter=f"status == 'active' and user_id == '{user_id}'", limit=limit,
                output_fields=["content", "memory_id", "created_at"],
                similarity_threshold=similarity_threshold
            )
            
            candidates = []
            if mem_res and mem_res[0]:
                for hit in mem_res[0]:
                    candidates.append(hit['entity'])
            
            # 2. æœç´¢ç›¸å…³çš„facts
            print(f"   Searching facts for fact: {fact['text'][:50]}...")
            fact_res = self.client.search(
                self.fact_col, [query_vec], filter=f"user_id == '{user_id}'", limit=limit * 2,
                output_fields=["fact_id", "text", "details", "timestamp", "linked_memory_ids", "trajectory"],
                similarity_threshold=similarity_threshold
            )
            
            related_facts = []
            if fact_res and fact_res[0]:
                for hit in fact_res[0]:
                    related_facts.append(hit['entity'])
            
            # 3. æ£€ç´¢è®°å¿†å…³è”çš„äº‹å®
            if candidates:
                memory_ids = [mem['memory_id'] for mem in candidates]
                expr_parts = [f'array_contains(linked_memory_ids, "{mem_id}")' for mem_id in memory_ids]
                filter_expr = " || ".join(expr_parts)
                
                try:
                    mem_related_facts = self.client.query(
                        collection_name=self.fact_col,
                        filter=filter_expr,
                        output_fields=["fact_id", "linked_memory_ids", "text", "linked_chunk_id", "timestamp", "details", "trajectory"]
                    )
                    # åˆå¹¶ç›¸å…³äº‹å®ï¼Œå»é‡
                    seen_fact_ids = set(f['fact_id'] for f in related_facts)
                    for f in mem_related_facts:
                        if f['fact_id'] not in seen_fact_ids:
                            seen_fact_ids.add(f['fact_id'])
                            related_facts.append(f)
                except Exception as e:
                    print(f"   âš ï¸ Error retrieving memory-related facts: {e}")
            
            # 4. æ£€ç´¢è¿™äº›äº‹å®å…³è”çš„å…¶ä»–è®°å¿†
            if related_facts:
                all_related_memory_ids = set()
                for f in related_facts:
                    linked_mem_ids = f.get("linked_memory_ids", [])
                    all_related_memory_ids.update(linked_mem_ids)
                
                existing_memory_ids = set([mem['memory_id'] for mem in candidates])
                new_memory_ids = all_related_memory_ids - existing_memory_ids
                
                if new_memory_ids:
                    quoted_new_ids = [f'"{mem_id}"' for mem_id in new_memory_ids]
                    mem_filter = f"status == 'active' and user_id == '{user_id}' and memory_id in [{','.join(quoted_new_ids)}]"
                    
                    try:
                        additional_memories = self.client.query(
                            collection_name=self.mem_col,
                            filter=mem_filter,
                            output_fields=["content", "memory_id", "created_at"]
                        )
                        candidates.extend(additional_memories)
                    except Exception as e:
                        print(f"   âš ï¸ Error retrieving additional memories: {e}")
            
            # 5. å¯¹å€™é€‰è®°å¿†è¿›è¡Œå»é‡
            unique_candidates = []
            seen_memory_ids = set()
            for mem in candidates:
                mem_id = mem['memory_id']
                if mem_id not in seen_memory_ids:
                    seen_memory_ids.add(mem_id)
                    unique_candidates.append(mem)
            
            # 6. å¯¹ç›¸å…³äº‹å®è¿›è¡Œå»é‡
            unique_related_facts = []
            seen_fact_ids = set()
            for f in related_facts:
                fact_id = f['fact_id']
                if fact_id not in seen_fact_ids:
                    seen_fact_ids.add(fact_id)
                    unique_related_facts.append(f)
            
            context_bundles.append({
                "new_fact": fact,
                "candidates": unique_candidates,
                "related_facts": unique_related_facts
            })
            
        return context_bundles

    # --- Step 3: Decide (With ID Mapping) ---
    def step_decide(self, extract_result: Dict, context_bundles: List[Dict], user_id: str = 'default', training_mode: bool = False) -> List[Dict]:
        all_new_facts = extract_result['new_facts']
        
        # 1. åˆå¹¶å»é‡ Candidates (Memories)
        temp_mem_storage = {}
        for bundle in context_bundles:
            for mem in bundle['candidates']:
                temp_mem_storage[mem['memory_id']] = mem
        
        unique_memories_list = list(temp_mem_storage.values())
        
        # 2. åˆå¹¶å»é‡ Related Facts
        temp_fact_storage = {}
        for bundle in context_bundles:
            if 'related_facts' in bundle:
                for fact in bundle['related_facts']:
                    temp_fact_storage[fact['fact_id']] = fact
        
        unique_facts_list = list(temp_fact_storage.values())
        
        if not training_mode:
            print(f"ğŸ§  [3. Manager] Global Decide: {len(all_new_facts)} facts vs {len(unique_memories_list)} memories vs {len(unique_facts_list)} facts.")

        # ğŸŒŸ 3. æ„é€  ID æ˜ å°„ (Mapping Logic)
        uuid_mapping = {}  # { "0": "real-uuid", "1": "real-uuid" }
        fact_id_mapping = {}  # { "F0": "real-fact-id", "F1": "real-fact-id" }
        
        candidates_str = ""
        facts_str = ""

        # æ„é€ Memorieséƒ¨åˆ†
        if not unique_memories_list:
            candidates_str = "(No relevant memories found. Treat as new topic.)"
        else:
            for idx, mem in enumerate(unique_memories_list):
                simple_id = str(idx)
                real_uuid = mem['memory_id']
                uuid_mapping[simple_id] = real_uuid
                candidates_str += f"[Memory Item ID: {simple_id}]\n- Content: {mem['content']}\n\n"
        
        # æ„é€ Factséƒ¨åˆ†
        if not unique_facts_list:
            facts_str = "(No relevant facts found.)"
        else:
            for idx, fact in enumerate(unique_facts_list):
                simple_id = f"F{idx}"
                real_fact_id = fact['fact_id']
                fact_id_mapping[simple_id] = real_fact_id
                facts_str += f"[Fact Item ID: {simple_id}]\n- Text: {fact['text']}\n- Details: {json.dumps(fact.get('details', []), ensure_ascii=False)}\n\n"

        # æ„é€ æœ€ç»ˆ Prompt
        system_msg = MEMORY_MANAGER_PROMPT

        # æ„é€ åŒ…å«textã€detailså’Œfact_indexçš„factå­—ç¬¦ä¸²
        fact_objects = []
        for idx, fact in enumerate(all_new_facts):
            fact_obj = {
                "text": fact['text'], 
                "details": fact.get('details', []),
                "fact_index": idx  # æ·»åŠ fact_indexï¼Œç”¨äºæ ‡è¯†åŒä¸€äº‹å®
            }
            fact_objects.append(fact_obj)
        
        user_content = f"""
        [New Facts Stream]
        {json.dumps(fact_objects, ensure_ascii=False, indent=2)}
        
        [EXISTING MEMORIES]
        {candidates_str}
        
        [EXISTING FACTS]
        {facts_str}
        """

        all_decisions = []
        try:
            # ä½¿ç”¨streamingæ¨¡å¼æ¥è·å–å®Œæ•´çš„å“åº”ï¼ŒåŒ…æ‹¬æ€ç»´è¿‡ç¨‹
            response = llm_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": user_content}
                ],
                tools=MEMORY_TOOLS,
                tool_choice="required",
                temperature=0,
                stream=True
            )
            
            # æ”¶é›†å®Œæ•´çš„å“åº”
            collected_messages = []
            for chunk in response:
                try:
                    if hasattr(chunk, 'choices') and chunk.choices:
                        choice = chunk.choices[0]
                        if hasattr(choice, 'delta') and hasattr(choice.delta, 'content') and choice.delta.content is not None:
                            collected_messages.append(choice.delta.content)
                except IndexError:
                    continue
            
            # æ‹¼æ¥å®Œæ•´çš„æ€è€ƒè¿‡ç¨‹
            thinking_process = ''.join(collected_messages)
            if thinking_process and not training_mode:
                print(f"\n   ğŸ§  LLMæ€è€ƒè¿‡ç¨‹:")
                print(f"   {thinking_process}")
            
            # é‡æ–°åˆ›å»ºéæµå¼å“åº”ä»¥è·å–å·¥å…·è°ƒç”¨
            response = llm_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": user_content}
                ],
                tools=MEMORY_TOOLS,
                tool_choice="required",
                temperature=0
            )
            
            # æ£€æŸ¥å“åº”ç»“æ„æ˜¯å¦å®Œæ•´
            if not response.choices or len(response.choices) == 0:
                if not training_mode:
                    print(f"   âš ï¸ Warning: No choices in response")
                return []
            
            choice = response.choices[0]
            if not hasattr(choice, 'message') or not choice.message:
                if not training_mode:
                    print(f"   âš ï¸ Warning: No message in choice")
                return []
            
            tool_calls = choice.message.tool_calls
            if not tool_calls: return []

            # ğŸŒŸ è¾…åŠ©å‡½æ•°: è¿˜åŸ Memory ID
            def resolve_memory_id(simple_id):
                real = uuid_mapping.get(str(simple_id))
                if not real and not training_mode:
                    print(f"   âš ï¸ Warning: LLM hallucinated Memory ID '{simple_id}', ignoring.")
                return real
            
            # ğŸŒŸ è¾…åŠ©å‡½æ•°: è¿˜åŸ Fact ID
            def resolve_fact_id(simple_id):
                real = fact_id_mapping.get(str(simple_id))
                if not real and not training_mode:
                    print(f"   âš ï¸ Warning: LLM hallucinated Fact ID '{simple_id}', ignoring.")
                return real

            for tool_call in tool_calls:
                try:
                    func_name = tool_call.function.name
                    args = json.loads(tool_call.function.arguments)
                    
                    if not training_mode:
                        print(f"   ğŸ¤– Raw Action: {func_name} | Args: {args}")
                    decision = {"action": "NOOP"}

                    if func_name == "create_memory":
                        decision.update({
                            "action": "ADD", 
                            "summary": args.get("content", ""), 
                            "facts_to_link": args.get("evidence_facts", []),
                            "user_id": user_id
                        })
                    
                    elif func_name == "update_memory":
                        if "target_memory_id" in args:
                            real_tid = resolve_memory_id(args["target_memory_id"])
                            if real_tid:
                                orig_created = temp_mem_storage.get(real_tid, {}).get('created_at', int(time.time()))
                                decision.update({
                                    "action": "UPDATE", 
                                    "target_id": real_tid, 
                                    "new_content": args.get("new_content", ""), 
                                    "facts_to_link": args.get("evidence_facts", []), 
                                    "orig_created": orig_created,
                                    "user_id": user_id
                                })

                    elif func_name == "delete_memory":
                        if "target_memory_id" in args:
                            real_tid = resolve_memory_id(args["target_memory_id"])
                            if real_tid:
                                orig_created = temp_mem_storage.get(real_tid, {}).get('created_at', int(time.time()))
                                decision.update({
                                    "action": "DELETE", 
                                    "target_id": real_tid, 
                                    "facts_to_link": args.get("evidence_facts", []), 
                                    "orig_created": orig_created,
                                    "user_id": user_id
                                })

                    elif func_name == "infer_memory":
                        if "source_memory_ids" in args:
                            source_simples = args["source_memory_ids"]
                            # ç¡®ä¿source_simplesæ˜¯åˆ—è¡¨
                            if not isinstance(source_simples, list):
                                source_simples = [source_simples]
                            real_source_ids = [resolve_memory_id(sid) for sid in source_simples if resolve_memory_id(sid)]
                            if real_source_ids:
                                decision.update({
                                    "action": "INFER", 
                                    "source_ids": real_source_ids, 
                                    "summary": args.get("inference_content", ""), 
                                    "facts_to_link": args.get("evidence_facts", []),
                                    "user_id": user_id
                                })
                    
                    elif func_name == "fact_add":
                        # æŸ¥æ‰¾å¯¹åº”çš„fact_index
                        content = args.get("content", "")
                        details = args.get("details", [])
                        fact_index = -1
                        # æ ¹æ®contentå’ŒdetailsæŸ¥æ‰¾å¯¹åº”çš„æ–°äº‹å®
                        for idx, fact in enumerate(all_new_facts):
                            if fact['text'] == content and json.dumps(fact['details'], sort_keys=True) == json.dumps(details, sort_keys=True):
                                fact_index = idx
                                break
                        
                        decision.update({
                            "action": "FACT_ADD", 
                            "content": content,
                            "details": details,
                            "fact_index": fact_index,
                            "user_id": user_id
                        })
                    
                    elif func_name == "fact_trajectorize":
                        if "target_fact_id" in args:
                            real_fact_id = resolve_fact_id(args["target_fact_id"])
                            if real_fact_id:
                                new_content = args.get("new_content", "")
                                details = args.get("details", [])
                                fact_index = -1
                                # æ ¹æ®new_contentå’ŒdetailsæŸ¥æ‰¾å¯¹åº”çš„æ–°äº‹å®
                                for idx, fact in enumerate(all_new_facts):
                                    if fact['text'] == new_content and json.dumps(fact['details'], sort_keys=True) == json.dumps(details, sort_keys=True):
                                        fact_index = idx
                                        break
                                
                                decision.update({
                                    "action": "FACT_TRAJECTORIZE", 
                                    "target_fact_id": real_fact_id, 
                                    "new_content": new_content,
                                    "diff": args.get("diff", ""),
                                    "details": details,
                                    "fact_index": fact_index,
                                    "user_id": user_id
                                })

                    elif func_name == "no_operation":
                        decision.update({"reason": args.get("reason", "No reason provided"), "user_id": user_id})
                    
                    if decision["action"] != "NOOP" or "reason" in decision:
                        all_decisions.append(decision)
                except Exception as e:
                    if not training_mode:
                        print(f"   âš ï¸ Error processing tool call: {e}")
                    continue

        except Exception as e:
            if not training_mode:
                print(f"   âš ï¸ Decision Error: {e}")
        
        return all_decisions
        
    # --- Batch Processing for Training with GRPO Support ---
    def batch_process(self, batch_data: List[Dict], user_id: str = 'default', grpo_compatible: bool = True) -> List[Dict]:
        """
        Batch processing for memory management training with GRPO compatibility.
        
        Args:
            batch_data (List[Dict]): List of input data for batch processing.
            user_id (str, optional): User ID for memory operations. Defaults to 'default'.
            grpo_compatible (bool, optional): Whether to return GRPO-compatible format. Defaults to True.
            
        Returns:
            List[Dict]: List of results for each input in the batch.
        """
        results = []
        
        for data in batch_data:
            # Extract facts from input text
            extract_result = self.step_extract(data['text'], extract_mode='whole')
            
            # Retrieve relevant memories
            context_bundles = self.step_retrieve(extract_result, limit=3, user_id=user_id)
            
            # Make decisions (memory operations) in training mode
            decisions = self.step_decide(extract_result, context_bundles, user_id=user_id, training_mode=True)
            
            # Execute decisions
            self.step_execute(decisions, extract_result, user_id=user_id)
            
            if grpo_compatible:
                # Format result for GRPO training
                result = {
                    'input': data['text'],
                    'extract_result': extract_result,
                    'decisions': decisions,
                    # Add GRPO-specific fields
                    'memory_operations': [d['action'] for d in decisions if d['action'] != 'NOOP'],
                    'memory_contents': [d.get('summary', '') for d in decisions if d['action'] != 'NOOP'],
                    # Ensure we have the expected_operation if provided in data
                    'expected_operation': data.get('expected_operation', '')
                }
            else:
                # Standard format for non-GRPO training
                result = {
                    'input': data['text'],
                    'extract_result': extract_result,
                    'decisions': decisions
                }
            
            results.append(result)
        
        return results

    # ==========================================
    # Step 4: Execute (Modified for Fact Inheritance)
    # ==========================================
    def step_execute(self, decisions: List[Dict], extract_result: Dict, user_id: str = 'default'):
        ts = int(time.time())
        chunk_id = extract_result['chunk_id']
        all_new_facts = extract_result['new_facts']
        
        # 1. ä¿å­˜åŸå§‹ Chunk
        self.client.insert(self.chunk_col, [{"chunk_id": chunk_id, "text": extract_result["chunk_text"], "timestamp": ts, "embedding": get_embedding(extract_result["chunk_text"])}])

        # 2. æ”¶é›†æ‰€æœ‰è¦é“¾æ¥çš„äº‹å®æ–‡æœ¬
        all_facts_to_link = set()
        for decision in decisions:
            action = decision.get("action")
            facts_to_link = decision.get('facts_to_link', [])
            for fact_text in facts_to_link:
                all_facts_to_link.add(fact_text)
        
        # 3. åˆ›å»ºfact_id_mapï¼Œç”¨äºè®°å½•fact_indexä¸ç”Ÿæˆçš„fact IDä¹‹é—´çš„å…³ç³»
        fact_id_map = {}
        # é¦–å…ˆä¸ºæ¯ä¸ªæ–°äº‹å®æ·»åŠ fact_index
        for idx, fact in enumerate(all_new_facts):
            fact['fact_index'] = idx
        
        # 3. å¯¹æ‰€æœ‰è¦å¤„ç†çš„äº‹å®è¿›è¡Œæœ€ç»ˆå»é‡
        # æ”¶é›†æ‰€æœ‰æ–°äº‹å®
        all_facts = []
        for fact in all_new_facts:
            # åªå¤„ç†åœ¨all_facts_to_linkä¸­çš„äº‹å®
            if fact['text'] in all_facts_to_link:
                all_facts.append(fact)
        
        # å¯¹æ‰€æœ‰äº‹å®è¿›è¡Œå»é‡
        unique_all_facts = []
        seen_fact_keys = set()
        for fact in all_facts:
            fact_key = f"{fact['text']}::{json.dumps(fact['details'], sort_keys=True)}"
            # ä¹Ÿè€ƒè™‘å»æ‰"User"å‰ç¼€çš„æƒ…å†µ
            stripped_fact_key = f"{fact['text'].lower().replace('user ', '')}::{json.dumps(fact['details'], sort_keys=True)}"
            if fact_key not in seen_fact_keys and stripped_fact_key not in seen_fact_keys:
                seen_fact_keys.add(fact_key)
                seen_fact_keys.add(stripped_fact_key)
                unique_all_facts.append(fact)
        
        if len(unique_all_facts) < len(all_facts):
            print(f"   âœ… æœ€ç»ˆå»é‡ {len(all_facts) - len(unique_all_facts)} ä¸ªé‡å¤äº‹å®")
        
        # æ›´æ–°all_facts_to_linkä¸ºå»é‡åçš„äº‹å®æ–‡æœ¬é›†åˆ
        all_facts_to_link = {fact['text'] for fact in unique_all_facts}

        # åˆå§‹åŒ–æ“ä½œæ¬¡æ•°è®¡æ•°å™¨ï¼Œæ·»åŠ æ–°çš„æ“ä½œç±»å‹
        self.operation_counts.setdefault("FACT_ADD", 0)
        self.operation_counts.setdefault("FACT_TRAJECTORIZE", 0)
        
        # 3. å¤„ç†æ¯ä¸ªå†³ç­–
        has_non_noop_action = False
        
        # æ”¶é›†æ‰€æœ‰è¦é“¾æ¥çš„äº‹å®ï¼Œç¡®ä¿å»é‡
        all_matched_facts = []
        seen_fact_keys = set()
        
        for decision in decisions:
            action = decision.get("action")
            if action == "NOOP":
                self.operation_counts["NOOP"] += 1
                print(f"   ğŸš« No operation: {decision.get('reason', 'No reason provided')}")
                continue

            has_non_noop_action = True
            target_mem_id = None
            relations = []

            # --- CASE 1: ADD MEMORY ---
            if action == "ADD":
                self.operation_counts["ADD"] += 1
                target_mem_id = str(uuid.uuid4())
                self._upsert_mem(target_mem_id, decision['summary'], ts, ts, "active", [], decision.get('user_id', 'default'))
                print(f"   âœ… Created Mem: {target_mem_id[:8]}... | Content: {decision['summary']}")

            # --- CASE 2: UPDATE MEMORY ---
            elif action == "UPDATE":
                self.operation_counts["UPDATE"] += 1
                target_mem_id = decision['target_id']
                
                # æŸ¥è¯¢æ—§çš„memoryå†…å®¹
                old_memories = self.client.query(
                    collection_name=self.mem_col,
                    filter=f"memory_id == '{target_mem_id}'",
                    output_fields=["content", "created_at"]
                )
                
                old_content = "" if not old_memories else old_memories[0].get("content", "")
                new_content = decision['new_content']
                
                # è®°å½•updateå‰åçš„å†…å®¹
                print(f"   ğŸ”„ Updating Mem: {target_mem_id[:8]}...")
                print(f"      Before: {old_content[:]}...")
                print(f"      After:  {new_content[:]}...")
                
                self._upsert_mem(target_mem_id, new_content, decision['orig_created'], ts, "active", [], decision.get('user_id', 'default'))

            # --- CASE 3: DELETE MEMORY ---
            elif action == "DELETE":
                self.operation_counts["DELETE"] += 1
                target_mem_id = decision['target_id']
                self._upsert_mem(target_mem_id, "(Archived)", decision['orig_created'], ts, "archived", [], decision.get('user_id', 'default'))
                print(f"   âŒ Deleted Mem: {target_mem_id[:8]}...")

            # --- CASE 4: INFER MEMORY (With Fact Inheritance) ---
            elif action == "INFER":
                self.operation_counts["INFER"] += 1
                target_mem_id = str(uuid.uuid4()) # è¿™æ˜¯ Memory C
                source_ids = decision.get('source_ids', []) # è¿™æ˜¯ [A, B]
                #############################################################
                # æŸ¥è¯¢source_idså¯¹åº”çš„memoryå†…å®¹ï¼Œç”¨äºæ‰“å°
                source_mems = []
                if source_ids:
                    quoted_source_ids = [f'"{sid}"' for sid in source_ids]
                    mem_filter = f"status == 'active' and memory_id in [{','.join(quoted_source_ids)}]"
                    try:
                        source_mems = self.client.query(
                            collection_name=self.mem_col,
                            filter=mem_filter,
                            output_fields=["content", "memory_id", "created_at", "user_id"]
                        )
                    except Exception as e:
                        print(f"   âš ï¸ æŸ¥è¯¢source memoryå¤±è´¥: {e}")
                #############################################################
                # 4.1 åˆ›å»ºæ–°è®°å¿† Cï¼Œå¹¶è®°å½•è¡€ç¼˜å…³ç³» (inferred_from)
                relations = [{"type": "inferred_from", "target_id": sid} for sid in source_ids]
                self._upsert_mem(target_mem_id, decision['summary'], ts, ts, "active", relations, decision.get('user_id', 'default'))
                ####################################################################################
                # å°†inferå‰åçš„memoryå†…å®¹æ‹¼åœ¨åŒä¸€ä¸ªå­—ç¬¦ä¸²é‡Œè¾“å‡º
                infer_output = f"   ğŸ’¡ Inferred Mem: {target_mem_id[:8]}... | From: {[s[:8] for s in source_ids]}\n"
                infer_output += f"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
                
                # æ‹¼æ¥inferå‰çš„memoryå†…å®¹
                if source_mems:
                    infer_output += f"   â”‚ ğŸ“‹ Infer å‰çš„ Memory ({len(source_mems)}ä¸ª):\n"
                    for mem in source_mems:
                        mem_id = mem.get("memory_id", "unknown")
                        content = mem.get("content", "")
                        infer_output += f"   â”‚      ğŸ“Œ ID: {mem_id[:8]}... | å†…å®¹: {content[:]}...\n"
                
                # æ‹¼æ¥inferç”Ÿæˆçš„memoryå†…å®¹
                infer_output += f"   â”‚ ğŸ“ Inferç”Ÿæˆçš„ Memory:\n"
                infer_output += f"   â”‚      ğŸ“Œ ID: {target_mem_id[:8]}... | å†…å®¹: {decision['summary'][:]}...\n"
                infer_output += f"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
                
                # ä¸€æ¬¡æ€§è¾“å‡ºæ•´ä¸ªå­—ç¬¦ä¸²
                print(infer_output)
                #################################################################################
                # 4.2 ğŸŒŸ æ ¸å¿ƒä¿®æ”¹ï¼šç»§æ‰¿æ—§ Facts
                # é€»è¾‘ï¼šæ‰¾å‡ºæ‰€æœ‰æ”¯æŒ A æˆ– B çš„ Factï¼ŒæŠŠ C ä¹ŸåŠ åˆ°å®ƒä»¬çš„æ”¯æŒåˆ—è¡¨é‡Œ
                if source_ids:
                    # æ„å»ºæŸ¥è¯¢è¡¨è¾¾å¼ï¼šarray_contains(linked_memory_ids, 'A') or ...
                    expr_parts = [f'array_contains(linked_memory_ids, "{sid}")' for sid in source_ids]
                    filter_expr = " || ".join(expr_parts)
                    
                    try:
                        # æŸ¥å‡ºæ—§ Facts
                        old_related_facts = self.client.query(
                            collection_name=self.fact_col,
                            filter=filter_expr,
                            output_fields=["fact_id", "linked_memory_ids", "text", "linked_chunk_id", "timestamp", "details", "embedding"]
                        )
                        
                        if old_related_facts:
                            updated_rows = []
                            for fact in old_related_facts:
                                current_links = fact.get("linked_memory_ids", [])
                                # å¦‚æœ C è¿˜æ²¡å…³è”ä¸Šï¼Œå°±åŠ è¿›å»
                                if target_mem_id not in current_links:
                                    current_links.append(target_mem_id)
                                    # ç¡®ä¿factsåŒ…å«detailså­—æ®µ
                                    if "details" not in fact:
                                        fact["details"] = []
                                    # ç¡®ä¿factsåŒ…å«embeddingå­—æ®µ
                                    if "embedding" not in fact or not isinstance(fact["embedding"], list):
                                        text = fact.get("text", "")
                                        details = fact.get("details", [])
                                        fact["embedding"] = self._generate_fact_embedding(text, details)
                                    updated_rows.append(fact)
                            
                            # å†™å›æ•°æ®åº“ (Upsert)
                            if updated_rows:
                                self.client.upsert(self.fact_col, updated_rows)
                                print(f"      â†³ ğŸ§¬ Inherited {len(updated_rows)} old facts from sources.")
                    except Exception as e:
                        print(f"      âš ï¸ Error inheriting facts: {e}")
            
            # --- CASE 5: ADD FACT ---
            elif action == "FACT_ADD":
                self.operation_counts["FACT_ADD"] += 1
                content = decision['content']
                details = decision.get('details', [])
                fact_index = decision.get('fact_index', -1)  # è·å–fact_index
                
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰è¯¥fact_indexå¯¹åº”çš„fact_id
                if fact_index >= 0 and fact_index in fact_id_map:
                    # å¦‚æœå·²æœ‰ï¼Œä½¿ç”¨å·²æœ‰çš„fact_id
                    fact_id = fact_id_map[fact_index]
                    print(f"   ğŸ”„ ä½¿ç”¨å·²æœ‰çš„ Fact ID: {fact_id[:8]}... | Content: {content}")
                else:
                    # å¦åˆ™ç”Ÿæˆæ–°çš„fact_id
                    fact_id = str(uuid.uuid4())
                    # è®°å½•fact_indexä¸fact_idçš„æ˜ å°„å…³ç³»
                    if fact_index >= 0:
                        fact_id_map[fact_index] = fact_id
                
                # ä¿å­˜æ–°äº‹å®åˆ°æ•°æ®åº“ï¼ŒåŒ…å«details
                fact = {
                    "fact_id": fact_id,
                    "linked_memory_ids": [],  # åˆå§‹ä¸å…³è”ä»»ä½•è®°å¿†
                    "linked_chunk_id": chunk_id,
                    "text": content,
                    "details": details,  # ä¿å­˜ä¼ å…¥çš„details
                    "timestamp": ts,
                    "user_id": decision.get('user_id', user_id),
                    "embedding": self._generate_fact_embedding(content, details),
                    "trajectory": []  # åˆå§‹ç©ºè½¨è¿¹
                }
                
                self.client.upsert(self.fact_col, [fact])
                print(f"   âœ… Added Fact: {fact_id[:8]}... | Content: {content}")
                if details:
                    print(f"      Details: {json.dumps(details, ensure_ascii=False, indent=2)}")
            
            # --- CASE 6: UPDATE FACT (Fact Trajectorize) ---
            elif action == "FACT_TRAJECTORIZE":
                self.operation_counts["FACT_TRAJECTORIZE"] += 1
                target_fact_id = decision['target_fact_id']
                new_content = decision['new_content']
                diff = decision['diff']
                details = decision.get('details', [])
                fact_index = decision.get('fact_index', -1)  # è·å–fact_index
                
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰è¯¥fact_indexå¯¹åº”çš„fact_id
                if fact_index >= 0 and fact_index in fact_id_map:
                    # å¦‚æœå·²æœ‰ï¼Œä½¿ç”¨å·²æœ‰çš„fact_id
                    target_fact_id = fact_id_map[fact_index]
                    print(f"   ğŸ”„ ä½¿ç”¨å·²æœ‰çš„ Fact ID: {target_fact_id[:8]}... è¿›è¡Œtrajectorizeæ“ä½œ")
                
                # æŸ¥è¯¢æ—§çš„factå†…å®¹
                old_facts = self.client.query(
                    collection_name=self.fact_col,
                    filter=f"fact_id == '{target_fact_id}'",
                    output_fields=["fact_id", "linked_memory_ids", "linked_chunk_id", "text", "details", "timestamp", "user_id", "embedding", "trajectory"]
                )
                
                if not old_facts:
                    print(f"   âš ï¸ Fact not found: {target_fact_id}")
                    continue
                
                old_fact = old_facts[0]
                old_content = old_fact.get("text", "")
                old_details = old_fact.get("details", [])
                
                # æ„å»ºæ›´æ–°åçš„fact
                updated_fact = old_fact.copy()
                updated_fact["text"] = new_content
                updated_fact["details"] = details  # æ›´æ–°details
                updated_fact["timestamp"] = ts
                
                # æ›´æ–°trajectoryå­—æ®µï¼Œæ·»åŠ æ–°çš„å˜åŒ–è®°å½•ï¼ŒåªåŒ…å«diff
                trajectory = updated_fact.get("trajectory", [])
                trajectory.append({
                    "timestamp": ts,
                    "diff": diff
                })
                updated_fact["trajectory"] = trajectory
                
                # æ›´æ–°embedding
                updated_fact["embedding"] = self._generate_fact_embedding(new_content, details)
                
                # å†™å›æ•°æ®åº“
                self.client.upsert(self.fact_col, [updated_fact])
                print(f"   ğŸ”„ Trajectorized Fact: {target_fact_id[:8]}...")
                print(f"      Diff: {diff}")
                print(f"      Before: {old_content[:]}...")
                if old_details:
                    print(f"      Old Details: {json.dumps(old_details, ensure_ascii=False, indent=2)}")
                print(f"      After:  {new_content[:]}...")
                if details:
                    print(f"      New Details: {json.dumps(details, ensure_ascii=False, indent=2)}")
                print(f"      Trajectory Length: {len(trajectory)}")
                
                # æ›´æ–°fact_id_mapï¼Œç¡®ä¿å…¶ä»–æ“ä½œä½¿ç”¨æ›´æ–°åçš„fact_id
                if fact_index >= 0:
                    fact_id_map[fact_index] = target_fact_id

            # --- Common: Link NEW Facts for this decision ---
            # æ— è®ºæ˜¯ ADD, UPDATE è¿˜æ˜¯ INFERï¼Œéƒ½ä¼šæŠŠå½“å‰å†³ç­–çš„æ–°è¯æ®å…³è”ä¸Šå»
            facts_to_link = decision.get('facts_to_link', [])
            if target_mem_id and facts_to_link:
                # æŸ¥æ‰¾ä¸å¾…é“¾æ¥äº‹å®æ–‡æœ¬åŒ¹é…çš„å®Œæ•´äº‹å®å¯¹è±¡ï¼ˆåŒ…å«detailså’Œfact_idï¼‰
                for fact_text in facts_to_link:
                    # åœ¨æ‰€æœ‰æ–°äº‹å®ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ–‡æœ¬ï¼Œä»¥è·å–å®Œæ•´çš„factå¯¹è±¡ï¼ˆåŒ…å«detailså’Œfact_idï¼‰
                    matching_fact = next((f for f in all_new_facts if f['text'] == fact_text), None)
                    if matching_fact:
                        # æ£€æŸ¥äº‹å®æ˜¯å¦å·²ç»è¢«å¤„ç†è¿‡
                        fact_key = f"{matching_fact['text']}::{json.dumps(matching_fact['details'], sort_keys=True)}"
                        # ä¹Ÿè€ƒè™‘å»æ‰"User"å‰ç¼€çš„æƒ…å†µ
                        stripped_fact_key = f"{matching_fact['text'].lower().replace('user ', '')}::{json.dumps(matching_fact['details'], sort_keys=True)}"
                        if fact_key not in seen_fact_keys and stripped_fact_key not in seen_fact_keys:
                            seen_fact_keys.add(fact_key)
                            seen_fact_keys.add(stripped_fact_key)
                            # æ·»åŠ ç›®æ ‡è®°å¿†IDåˆ°äº‹å®ä¸­
                            fact_with_target = matching_fact.copy()
                            fact_with_target['target_mem_id'] = target_mem_id
                            all_matched_facts.append(fact_with_target)
                    else:
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„å®Œæ•´äº‹å®å¯¹è±¡ï¼Œä½¿ç”¨æ–‡æœ¬åˆ›å»ºä¸€ä¸ªç®€å•çš„äº‹å®å¯¹è±¡
                        new_fact = {'text': fact_text, 'details': [], 'fact_id': str(uuid.uuid4()), 'target_mem_id': target_mem_id}
                        fact_key = f"{new_fact['text']}::{json.dumps(new_fact['details'], sort_keys=True)}"
                        stripped_fact_key = f"{new_fact['text'].lower().replace('user ', '')}::{json.dumps(new_fact['details'], sort_keys=True)}"
                        if fact_key not in seen_fact_keys and stripped_fact_key not in seen_fact_keys:
                            seen_fact_keys.add(fact_key)
                            seen_fact_keys.add(stripped_fact_key)
                            all_matched_facts.append(new_fact)
        
        # æ‰¹é‡å¤„ç†æ‰€æœ‰åŒ¹é…çš„äº‹å®ï¼Œç¡®ä¿æ¯ä¸ªäº‹å®åªè¢«å…³è”åˆ°ç›¸åº”çš„è®°å¿†
        if all_matched_facts:
            # æŒ‰äº‹å®IDåˆ†ç»„
            facts_by_id = {}
            for fact in all_matched_facts:
                fact_id = fact['fact_id']
                if fact_id not in facts_by_id:
                    facts_by_id[fact_id] = {
                        'fact': fact,
                        'target_mem_ids': set()
                    }
                facts_by_id[fact_id]['target_mem_ids'].add(fact['target_mem_id'])
            
            # æ„å»ºè¦å†™å…¥æ•°æ®åº“çš„è¡Œ
            rows = []
            for fact_info in facts_by_id.values():
                fact = fact_info['fact']
                fact_id = fact['fact_id']  # è·å–å½“å‰factçš„fact_id
                target_mem_ids = list(fact_info['target_mem_ids'])
                
                rows.append({
                    "fact_id": fact_id,
                    "linked_memory_ids": target_mem_ids, # å…³è”åˆ°æ‰€æœ‰ç›¸å…³çš„è®°å¿†
                    "linked_chunk_id": chunk_id,
                    "text": fact['text'],
                    "details": fact['details'],  # ä¿å­˜äº‹å®çš„detailsä¿¡æ¯
                    "timestamp": ts,
                    "user_id": decision.get('user_id', user_id),
                    "embedding": self._generate_fact_embedding(fact['text'], fact['details']),
                    "trajectory": []  # æ·»åŠ trajectoryå­—æ®µï¼Œè®°å½•å˜åŒ–è½¨è¿¹
                })
            
            if rows:
                self.client.upsert(self.fact_col, rows)
                print(f"   ğŸ”— æ‰¹é‡å…³è” {len(rows)} ä¸ªäº‹å®åˆ°å¯¹åº”è®°å¿†")

        # 4. å¤„ç†æœªå…³è”åˆ°ä»»ä½•è®°å¿†çš„æ–°äº‹å®ï¼ˆå½“æ‰€æœ‰å†³ç­–éƒ½æ˜¯NOOPæ—¶ï¼‰
        # æ‰¾å‡ºæ‰€æœ‰æœªè¢«å…³è”çš„æ–°äº‹å®
        unlinked_facts = []
        for fact in all_new_facts:
            if fact['text'] not in all_facts_to_link:
                unlinked_facts.append(fact)

        # å¦‚æœæœ‰æœªå…³è”çš„æ–°äº‹å®ï¼Œç›´æ¥ä¿å­˜åˆ°fact_colé›†åˆä¸­
        if unlinked_facts:
            rows = []
            for fact in unlinked_facts:
                # ä½¿ç”¨é¢„å¤„ç†æ­¥éª¤ä¸­ç”Ÿæˆçš„fact_idï¼Œè€Œä¸æ˜¯é‡æ–°ç”Ÿæˆ
                fact_id = fact.get('fact_id', str(uuid.uuid4()))
                rows.append({
                    "fact_id": fact_id,
                    "linked_memory_ids": [],  # ä¸å…³è”åˆ°ä»»ä½•è®°å¿†
                    "linked_chunk_id": chunk_id,
                    "text": fact['text'],
                    "details": fact['details'],  # ä¿å­˜äº‹å®çš„detailsä¿¡æ¯
                    "timestamp": ts,
                    "user_id": user_id,
                    "embedding": self._generate_fact_embedding(fact['text'], fact['details']),
                    "trajectory": []  # æ·»åŠ trajectoryå­—æ®µï¼Œè®°å½•å˜åŒ–è½¨è¿¹
                })
            if rows:
                self.client.upsert(self.fact_col, rows)
                print(f"   ğŸ’¾ Saved {len(rows)} unlinked facts to database...")

        # 5. å¤„ç†æ‰€æœ‰å†³ç­–éƒ½æ˜¯NOOPä½†æœ‰æ–°äº‹å®çš„æƒ…å†µ
        # å¦‚æœæ‰€æœ‰å†³ç­–éƒ½æ˜¯NOOPï¼Œä¸”æœ‰æ–°äº‹å®ï¼Œç¡®ä¿å®ƒä»¬éƒ½è¢«ä¿å­˜
        if not has_non_noop_action and all_new_facts:
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æœªä¿å­˜çš„æ–°äº‹å®
            all_fact_texts = {fact['text'] for fact in all_new_facts}
            saved_fact_texts = all_facts_to_link  # å·²ç»é€šè¿‡å†³ç­–å…³è”çš„äº‹å®
            unsaved_fact_texts = all_fact_texts - saved_fact_texts
            
            if unsaved_fact_texts:
                rows = []
                for fact in all_new_facts:
                    if fact['text'] in unsaved_fact_texts:
                        # ä½¿ç”¨é¢„å¤„ç†æ­¥éª¤ä¸­ç”Ÿæˆçš„fact_idï¼Œè€Œä¸æ˜¯é‡æ–°ç”Ÿæˆ
                        fact_id = fact.get('fact_id', str(uuid.uuid4()))
                        rows.append({
                            "fact_id": fact_id,
                            "linked_memory_ids": [],  # ä¸å…³è”åˆ°ä»»ä½•è®°å¿†
                            "linked_chunk_id": chunk_id,
                            "text": fact['text'],
                            "details": fact['details'],  # ä¿å­˜äº‹å®çš„detailsä¿¡æ¯
                            "timestamp": ts,
                            "user_id": user_id,
                            "embedding": self._generate_fact_embedding(fact['text'], fact['details']),
                            "trajectory": []  # æ·»åŠ trajectoryå­—æ®µï¼Œè®°å½•å˜åŒ–è½¨è¿¹
                        })
                if rows:
                    self.client.upsert(self.fact_col, rows)
                    print(f"   ğŸ’¾ Saved {len(rows)} unlinked facts to database (all actions were NOOP)...")
                    
    def _upsert_mem(self, mem_id, content, c_at, u_at, status, relations, user_id):
        self.client.upsert(self.mem_col, [{
            "memory_id": mem_id,
            "embedding": get_embedding(content),
            "content": content,
            "user_id": user_id,
            "status": status,
            "created_at": c_at,
            "updated_at": u_at,
            "relations": relations
        }])

    def step_preprocess_facts(self, extract_result: Dict, user_id: str = 'default') -> Dict:
        """
        é¢„å¤„ç†æå–å‡ºçš„äº‹å®ï¼Œæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨äºæ•°æ®åº“ä¸­ï¼Œç¡®ä¿ä»æºå¤´ä¸Šå»é‡
        
        Args:
            extract_result: æå–ç»“æœå­—å…¸ï¼ŒåŒ…å«new_facts
            user_id: ç”¨æˆ·æ ‡è¯†ï¼Œç¡®ä¿åªå¤„ç†å½“å‰ç”¨æˆ·çš„äº‹å®
            
        Returns:
            æ›´æ–°åçš„æå–ç»“æœå­—å…¸ï¼ŒåŒ…å«fact_idä¿¡æ¯
        """
        new_facts = extract_result['new_facts']
        processed_facts = []
        ts = extract_result['timestamp']
        chunk_id = extract_result['chunk_id']
        
        print(f"ğŸ” [Preprocess Facts] æ£€æŸ¥ {len(new_facts)} ä¸ªäº‹å®æ˜¯å¦å·²å­˜åœ¨...")
        
        # 1. å…ˆå¯¹åŒä¸€æ‰¹æ¬¡å†…çš„äº‹å®è¿›è¡Œå»é‡ï¼Œé¿å…åŒä¸€æ‰¹æ¬¡ä¸­é‡å¤çš„äº‹å®è¢«å¤„ç†
        unique_facts_in_batch = []
        seen_fact_keys = set()
        for fact in new_facts:
            # ä½¿ç”¨fact_textå’Œdetailsçš„ç»„åˆä½œä¸ºå”¯ä¸€æ ‡è¯†
            fact_key = f"{fact['text']}::{json.dumps(fact['details'], sort_keys=True)}"
            if fact_key not in seen_fact_keys:
                seen_fact_keys.add(fact_key)
                unique_facts_in_batch.append(fact)
        
        if len(unique_facts_in_batch) < len(new_facts):
            print(f"   âœ… åŒä¸€æ‰¹æ¬¡å†…å»é‡ {len(new_facts) - len(unique_facts_in_batch)} ä¸ªé‡å¤äº‹å®")
        
        for fact in unique_facts_in_batch:
            fact_text = fact['text']
            fact_details = fact['details']

            
            # 3. æŸ¥è¯¢æ•°æ®åº“ä¸­æ˜¯å¦å­˜åœ¨ç›¸åŒçš„fact
            existing_fact = None
            try:
                # å…ˆå°è¯•æœç´¢ç›¸å…³äº‹å®ï¼Œé¿å…å…¨é‡æŸ¥è¯¢
                # ä½¿ç”¨æ›´å®‰å…¨çš„æŸ¥è¯¢æ–¹å¼ï¼ŒåŸºäºtextçš„å‰ç¼€åŒ¹é…
                # åªæŸ¥è¯¢textå­—æ®µåŒ…å«fact_textå…³é”®è¯çš„äº‹å®
                search_vec = get_embedding(fact_text)
                search_results = self.client.search(
                    self.fact_col, [search_vec], 
                    output_fields=["fact_id", "details", "timestamp", "linked_memory_ids", "linked_chunk_id", "text"],
                    limit=20,  # åªæŸ¥è¯¢å‰20ä¸ªæœ€ç›¸ä¼¼çš„äº‹å®
                    similarity_threshold=0.8  # è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œåªè¿”å›ç›¸ä¼¼åº¦è¾ƒé«˜çš„äº‹å®
                )
                
                # å¤„ç†æœç´¢ç»“æœï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å®Œå…¨åŒ¹é…çš„äº‹å®
                if search_results and search_results[0]:
                    for hit in search_results[0]:
                        res = hit['entity']
                        res_text = res.get("text", "")
                        res_details = res.get("details", [])
                        # æ£€æŸ¥æ˜¯å¦æ˜¯ç›¸åŒçš„äº‹å®ï¼Œè€ƒè™‘åˆ°è¡¨è¿°å¯èƒ½ç•¥æœ‰ä¸åŒ
                        # 1. å®Œå…¨ç›¸åŒçš„æƒ…å†µ
                        if res_text == fact_text and res_details == fact_details:
                            existing_fact = res
                            break
                        # 2. æ ¸å¿ƒå†…å®¹ç›¸åŒä½†è¡¨è¿°ç•¥æœ‰ä¸åŒçš„æƒ…å†µï¼ˆå¦‚æœ‰æ— "User"å‰ç¼€ï¼‰
                        stripped_res_text = res_text.lower().replace("user ", "").strip()
                        stripped_fact_text = fact_text.lower().replace("user ", "").strip()
                        if stripped_res_text == stripped_fact_text and res_details == fact_details:
                            existing_fact = res
                            break
            
            except Exception as e:
                print(f"   âš ï¸ æŸ¥è¯¢äº‹å®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            
            if existing_fact:
                # äº‹å®å·²å­˜åœ¨ï¼Œæ›´æ–°timestamp
                fact_id = existing_fact["fact_id"]
                old_ts = existing_fact["timestamp"]
                
                # è·å–ç°æœ‰çš„linked_memory_idså’Œlinked_chunk_id
                existing_links = existing_fact.get("linked_memory_ids", [])
                existing_chunk = existing_fact.get("linked_chunk_id", "")
                
                # æ›´æ–°timestampå’Œå…³è”ä¿¡æ¯
                self.client.upsert(self.fact_col, [{
                    "fact_id": fact_id,
                    "linked_memory_ids": existing_links,
                    "linked_chunk_id": existing_chunk,
                    "text": fact_text,
                    "details": fact_details,
                    "timestamp": ts,
                    "user_id": user_id,
                    "embedding": self._generate_fact_embedding(fact_text, fact_details)
                }])
                
                # å°†ç°æœ‰äº‹å®æ·»åŠ åˆ°processed_facts
                processed_fact = {
                    "text": fact_text,
                    "details": fact_details,
                    "fact_id": fact_id
                }
                processed_facts.append(processed_fact)
                
                print(f"   ğŸ”„ äº‹å®å·²å­˜åœ¨ï¼Œæ›´æ–°timestamp: {fact_id} (æ—§: {old_ts}, æ–°: {ts})")
            else:
                # äº‹å®ä¸å­˜åœ¨ï¼Œç”Ÿæˆæ–°çš„fact_idå¹¶ä¿å­˜
                fact_id = str(uuid.uuid4())
                # print(f"   ğŸ†• æ–°äº‹å®: {fact_id}")
                
                # ä¿å­˜æ–°äº‹å®åˆ°æ•°æ®åº“
                self.client.upsert(self.fact_col, [{
                    "fact_id": fact_id,
                    "linked_memory_ids": [],
                    "linked_chunk_id": chunk_id,
                    "text": fact_text,
                    "details": fact_details,
                    "timestamp": ts,
                    "user_id": user_id,
                    "embedding": self._generate_fact_embedding(fact_text, fact_details)
                }])
                
                processed_fact = {
                    "text": fact_text,
                    "details": fact_details,
                    "fact_id": fact_id
                }
                
                processed_facts.append(processed_fact)
        
        # æ›´æ–°æå–ç»“æœ
        extract_result['new_facts'] = processed_facts
        return extract_result
    
    def process(self, text, retrieve_limit: int = 3, extract_mode: str = "whole", user_id: str = 'default', similarity_threshold: float = None):
        res = self.step_extract(text, extract_mode=extract_mode)
        if not res['new_facts']: return
        
        # é¢„å¤„ç†äº‹å®ï¼Œæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        res = self.step_preprocess_facts(res, user_id=user_id)
        
        # æ£€æŸ¥é¢„å¤„ç†åæ˜¯å¦è¿˜æœ‰æ–°äº‹å®
        if not res['new_facts']:
            print(f"   âœ… æ‰€æœ‰äº‹å®éƒ½å·²å­˜åœ¨ï¼Œæ— éœ€å¤„ç†")
            return
        
        print(f"   æ–°è¯æ®: {res['new_facts']}")
        
        # æ£€ç´¢ç›¸å…³è®°å¿†
        context_bundles = self.step_retrieve(res, limit=retrieve_limit, user_id=user_id, similarity_threshold=similarity_threshold)
        
        # ç”Ÿæˆå†³ç­–
        decisions = self.step_decide(res, context_bundles, user_id=user_id)
        
        # æ‰§è¡Œå†³ç­–
        self.step_execute(decisions, res, user_id=user_id)
        
        # è¿”å›æ“ä½œç»“æœï¼ˆå¯é€‰ï¼‰
        return decisions

    def _generate_fact_embedding(self, text: str, details: List) -> List[float]:
        """
        ä¸ºäº‹å®ç”ŸæˆåµŒå…¥å‘é‡ï¼Œè€ƒè™‘textå’Œdetails
        
        Args:
            text: äº‹å®æ–‡æœ¬
            details: äº‹å®è¯¦æƒ…åˆ—è¡¨
            
        Returns:
            åµŒå…¥å‘é‡åˆ—è¡¨
        """
        # å°†textå’Œdetailsåˆå¹¶ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²è¿›è¡ŒåµŒå…¥
        combined_text = text
        if details:
            # å°†detailsè½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œä¿ç•™å…³é”®ä¿¡æ¯
            details_str = " ".join([str(detail) for detail in details])
            combined_text = f"{text} {details_str}"
        
        return get_embedding(combined_text)

    def get_operation_counts(self):
        """è·å–æ“ä½œæ¬¡æ•°ç»Ÿè®¡"""
        return self.operation_counts

    def reset_operation_counts(self):
        """é‡ç½®æ“ä½œæ¬¡æ•°ç»Ÿè®¡"""
        self.operation_counts = {"ADD": 0, "UPDATE": 0, "DELETE": 0, "INFER": 0, "NOOP": 0}

    def get_collections(self):
        """è·å–é›†åˆåç§°"""
        return {
            "memories": self.mem_col,
            "facts": self.fact_col,
            "chunks": self.chunk_col
        }

    def drop_collections(self):
        """åˆ é™¤æ‰€æœ‰é›†åˆ"""
        self.client.drop_collection(self.mem_col)
        self.client.drop_collection(self.fact_col)
        self.client.drop_collection(self.chunk_col)
        print("All collections dropped.")

    def get_facts_by_memory_id(self, memory_id: str) -> List[Dict]:
        """
        è·å–å…³è”åˆ°æŒ‡å®šè®°å¿†çš„æ‰€æœ‰äº‹å®
        
        Args:
            memory_id: è®°å¿†ID
            
        Returns:
            äº‹å®åˆ—è¡¨
        """
        try:
            res = self.client.query(
                collection_name=self.fact_col,
                filter=f'array_contains(linked_memory_ids, "{memory_id}")',
                output_fields=["fact_id", "text", "details", "timestamp", "linked_chunk_id"]
            )
            return res
        except Exception as e:
            print(f"Error getting facts by memory id: {e}")
            return []

    def get_memories_by_fact_id(self, fact_id: str) -> List[Dict]:
        """
        è·å–å…³è”åˆ°æŒ‡å®šäº‹å®çš„æ‰€æœ‰è®°å¿†
        
        Args:
            fact_id: äº‹å®ID
            
        Returns:
            è®°å¿†åˆ—è¡¨
        """
        try:
            # å…ˆè·å–äº‹å®çš„linked_memory_ids
            fact_res = self.client.query(
                collection_name=self.fact_col,
                filter=f'fact_id == "{fact_id}"',
                output_fields=["linked_memory_ids"]
            )
            
            if not fact_res:
                return []
            
            linked_memory_ids = fact_res[0].get("linked_memory_ids", [])
            if not linked_memory_ids:
                return []
            
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            quoted_ids = [f'"{mem_id}"' for mem_id in linked_memory_ids]
            filter_expr = f"memory_id in [{','.join(quoted_ids)}] and status == 'active'"
            
            # æŸ¥è¯¢å…³è”çš„è®°å¿†
            mem_res = self.client.query(
                collection_name=self.mem_col,
                filter=filter_expr,
                output_fields=["memory_id", "content", "created_at", "updated_at", "relations"]
            )
            
            return mem_res
        except Exception as e:
            print(f"Error getting memories by fact id: {e}")
            return []
    
    def search_memories(self, query: str, top_k: int = 20, user_id: str = 'default', threshold: float = 0.0, enhanced_search: bool = False) -> List[Dict]:
        """
        æœç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„è®°å¿†ï¼Œå¹¶è¿”å›å¸¦æœ‰ç›¸å…³äº‹å®çš„ç»“æœ
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›çš„è®°å¿†æ•°é‡
            user_id: ç”¨æˆ·IDï¼Œç”¨äºè¿‡æ»¤è®°å¿†
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            enhanced_search: æ˜¯å¦å¯ç”¨å¢å¼ºå‹æœç´¢
            
        Returns:
            å¸¦æœ‰ç›¸å…³äº‹å®çš„è®°å¿†åˆ—è¡¨
        """
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_vec = get_embedding(query)
        
        # æœç´¢ç›¸å…³çš„memories
        mem_res = self.client.search(
            self.mem_col, [query_vec], filter=f"status == 'active' and user_id == '{user_id}'", limit=top_k,
            output_fields=["content", "memory_id", "created_at"],
            similarity_threshold=threshold
        )
        
        results = []
        if mem_res and mem_res[0]:
            for hit in mem_res[0]:
                mem = hit['entity']
                mem['similarity'] = hit['distance']
                
                # è·å–ç›¸å…³çš„facts
                related_facts = self.get_facts_by_memory_id(mem['memory_id'])
                for fact in related_facts:
                    fact_vec = get_embedding(fact['text'])
                    # è®¡ç®—äº‹å®ä¸æŸ¥è¯¢çš„ç›¸ä¼¼åº¦
                    similarity = np.dot(query_vec, fact_vec)
                    fact['similarity'] = similarity
                
                mem['related_facts'] = related_facts
                results.append(mem)
        
        return results
    
    def generate_response(self, question, question_date, context):
        """
        ç”Ÿæˆé—®é¢˜å“åº”
        
        Args:
            question: é—®é¢˜æ–‡æœ¬
            question_date: é—®é¢˜æ—¥æœŸ
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            LLMå“åº”å¯¹è±¡
        """
        prompt = LME_ANSWER_PROMPT.format(
            question=question,
            question_date=question_date,
            context=context
        )
        response = llm_client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "system", "content": prompt}],
                    temperature=0,
                )
        
        return response
    
    def process_user_memory_infer(self, line, retrieve_limit=3, extract_mode="whole", user_id="default"):
        """
        å¤„ç†ç”¨æˆ·è®°å¿†ä¼šè¯ï¼ŒåŒ…æ‹¬æå–ã€æ£€ç´¢ã€å†³ç­–å’Œæ‰§è¡Œæ­¥éª¤
        
        Args:
            line: åŒ…å«ç”¨æˆ·è®°å¿†ä¼šè¯çš„å­—å…¸
            retrieve_limit: æ£€ç´¢è®°å¿†çš„æ•°é‡é™åˆ¶
            extract_mode: æå–æ¨¡å¼
            user_id: ç”¨æˆ·ID
            
        Returns:
            æ“ä½œç»Ÿè®¡ä¿¡æ¯
        """
        # è·å–contextï¼ˆå³è®°å¿†ä¼šè¯æ–‡æœ¬ï¼‰
        context = line.get("context", "")
        if not context:
            # å¦‚æœæ²¡æœ‰contextï¼Œå°è¯•ä»haystack_datesä¸­è·å–ï¼ˆlongmemevalæ•°æ®é›†æ ¼å¼ï¼‰
            haystack_dates = line.get("haystack_dates", [])
            if haystack_dates and isinstance(haystack_dates, list):
                # åˆå¹¶æ‰€æœ‰haystack_datesä¸­çš„æ–‡æœ¬
                context = "\n".join([item["text"] for item in haystack_dates if isinstance(item, dict) and "text" in item])
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰contextï¼Œè·³è¿‡å¤„ç†
        if not context:
            print(f"   âš ï¸ æ²¡æœ‰æ‰¾åˆ°contextï¼Œè·³è¿‡å¤„ç†")
            return {"ADD": 0, "UPDATE": 0, "DELETE": 0, "INFER": 0, "NOOP": 0}
        
        # æå–äº‹å®
        extract_result = self.step_extract(context, extract_mode=extract_mode)
        
        # å¦‚æœæ²¡æœ‰æå–åˆ°äº‹å®ï¼Œè·³è¿‡å¤„ç†
        if not extract_result.get("new_facts"):
            print(f"   âš ï¸ æ²¡æœ‰æå–åˆ°äº‹å®ï¼Œè·³è¿‡å¤„ç†")
            return {"ADD": 0, "UPDATE": 0, "DELETE": 0, "INFER": 0, "NOOP": 0}
        
        # æ£€ç´¢ç›¸å…³è®°å¿†å’Œäº‹å®
        context_bundles = self.step_retrieve(extract_result, limit=retrieve_limit, user_id=user_id)
        
        # ç”Ÿæˆå†³ç­–
        decisions = self.step_decide(extract_result, context_bundles, user_id=user_id)
        
        # æ‰§è¡Œå†³ç­–
        self.step_execute(decisions, extract_result, user_id=user_id)
        
        # è¿”å›æ“ä½œç»Ÿè®¡ä¿¡æ¯
        return self.get_operation_counts()

# ==========================================
# è¯„ä¼°ç›¸å…³å‡½æ•°
# ==========================================
def response_user(line, pipeline, retrieve_limit=20, max_facts_per_memory=3, user_id='default', threshold: float = 0.0, enhanced_search: bool = False):
    """å¤„ç†ç”¨æˆ·é—®é¢˜ï¼Œç”Ÿæˆå“åº”
    
    Args:
        line: åŒ…å«é—®é¢˜å’Œå…¶ä»–ä¿¡æ¯çš„å­—å…¸
        pipeline: MemoryPipelineå®ä¾‹
        retrieve_limit: æ£€ç´¢è®°å¿†çš„æ•°é‡é™åˆ¶
        max_facts_per_memory: æ¯ä¸ªè®°å¿†çš„äº‹å®æ•°é‡é™åˆ¶
        user_id: ç”¨æˆ·æ ‡è¯†ï¼Œç¡®ä¿åªæ£€ç´¢å½“å‰ç”¨æˆ·çš„è®°å¿†
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºè¯¥é˜ˆå€¼çš„è®°å¿†å°†è¢«è¿‡æ»¤æ‰
        enhanced_search: æ˜¯å¦å¯ç”¨å¢å¼ºå‹æœç´¢æ¨¡å¼ï¼Œå¯ç”¨åä¼šè°ƒå¤§topkå¹¶å¢å¼ºrerank
    """
    question = line.get("question")
    question_date = line.get("question_date")
    question_date = question_date + " UTC"
    question_date_format = "%Y/%m/%d (%a) %H:%M UTC"
    question_date_string = datetime.strptime(question_date, question_date_format).replace(tzinfo=timezone.utc)
    
    # å¢å¼ºå‹æœç´¢æ¨¡å¼ï¼šè°ƒå¤§topk
    if enhanced_search:
        # è°ƒå¤§åˆå§‹æ£€ç´¢æ•°é‡ï¼Œä¾‹å¦‚ä¹˜ä»¥2
        enhanced_top_k = retrieve_limit * 2
        print(f"   ğŸš€ å¯ç”¨å¢å¼ºå‹æœç´¢æ¨¡å¼ï¼Œåˆå§‹æ£€ç´¢æ•°é‡: {enhanced_top_k}")
    else:
        enhanced_top_k = retrieve_limit
    
    # æœç´¢è®°å¿†ï¼Œä¼ é€’user_idã€thresholdå’Œenhanced_searchå‚æ•°
    retrieved_memories = pipeline.search_memories(question, top_k=enhanced_top_k, user_id=user_id, threshold=threshold, enhanced_search=enhanced_search)
    
    # ç¡®ä¿retrieved_memoriesä¸æ˜¯None
    retrieved_memories = retrieved_memories or []
    
    # æ„å»ºä¸Šä¸‹æ–‡ï¼ŒåŒ…å«è®°å¿†å’Œå…³è”çš„äº‹å®
    memories_with_facts = []
    
    for mem in retrieved_memories:
        # æ·»åŠ è®°å¿†å†…å®¹
        memory_line = f"- [{datetime.fromtimestamp(mem['created_at'], timezone.utc).isoformat()}] {mem['content']}"
        memories_with_facts.append(memory_line)
        
        # æ·»åŠ å…³è”çš„äº‹å®ï¼ˆå¦‚æœæœ‰ï¼‰
        related_facts = mem.get("related_facts", [])
        if related_facts:
            # ç›´æ¥ä½¿ç”¨search_memoriesä¸­å·²ç»è®¡ç®—å¥½çš„ç›¸ä¼¼åº¦åˆ†æ•°
            fact_with_scores = []
            for fact in related_facts:
                # è·å–å·²è®¡ç®—çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œé»˜è®¤ä¸º0
                similarity = fact.get("similarity", 0)
                fact_with_scores.append((fact, similarity))
            
            # æ ¹æ®ç›¸å…³æ€§åˆ†æ•°å¯¹äº‹å®è¿›è¡Œæ’åº
            fact_with_scores.sort(key=lambda x: x[1], reverse=True)
            
            # æ·»åŠ æ’åºåçš„äº‹å®ï¼Œé™åˆ¶æ•°é‡
            for i, (fact, score) in enumerate(fact_with_scores[:max_facts_per_memory]):
                # ä¼˜åŒ–äº‹å®è¾“å‡ºæ ¼å¼
                fact_text = fact['text']
                details = fact['details']
                
                # æ ¼å¼åŒ–ç»†èŠ‚
                if details:
                    # å°†ç»†èŠ‚åˆ—è¡¨è½¬æ¢ä¸ºæ›´æ˜“è¯»çš„æ ¼å¼
                    details_str = "; ".join(details)
                    # å¦‚æœç»†èŠ‚å¤ªé•¿ï¼Œæˆªæ–­
                    if len(details_str) > 100:
                        details_str = details_str[:97] + "..."
                    fact_line = f"  â”œâ”€â”€ [{i+1}] äº‹å®: {fact_text}\n  â”‚     ç»†èŠ‚: {details_str}"
                else:
                    fact_line = f"  â”œâ”€â”€ [{i+1}] äº‹å®: {fact_text}"
                
                memories_with_facts.append(fact_line)
    
    memories_str = "\n".join(memories_with_facts)
    
    # ç”Ÿæˆå“åº”
    response = pipeline.generate_response(question, question_date_string, memories_str)
    answer = response.choices[0].message.content
    
    return retrieved_memories, answer


def process_and_evaluate_user(line, user_index, infer=True, retrieve_limit: int = 3, extract_mode: str = "whole", vector_db_type="milvus", dataset_name=""):
    """
    å°è£…å•ä¸ªç”¨æˆ·çš„æ‰€æœ‰å¤„ç†æ­¥éª¤ï¼Œä»¥ä¾¿å¹¶è¡Œæ‰§è¡Œã€‚
    è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸ã€‚
    """
    try:
        # ä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆå”¯ä¸€çš„user_idï¼Œç¡®ä¿è®°å¿†éš”ç¦»
        user_id = f"user_{user_index}"
        
        # ä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºç‹¬ç«‹çš„pipelineå®ä¾‹ï¼Œé¿å…å¤šçº¿ç¨‹ç«äº‰
        # æ³¨æ„ï¼šæ¯ä¸ªç”¨æˆ·çš„pipelineå®ä¾‹ä¸åº”è¯¥æ¸…ç©ºæ•°æ®åº“ï¼Œclear_dbå›ºå®šä¸ºFalse
        pipeline = MemoryPipeline(vector_db_type=vector_db_type, clear_db=False, dataset_name=dataset_name)
        
        # å¤„ç†ç”¨æˆ·è®°å¿†ä¼šè¯ï¼Œä¼ é€’user_id
        memory_counts = pipeline.process_user_memory_infer(line, retrieve_limit=retrieve_limit, extract_mode=extract_mode, user_id=user_id)
        
        # ç”Ÿæˆé—®é¢˜å“åº”ï¼Œä¼ é€’user_id
        retrieved_memories, answer = response_user(line, pipeline, retrieve_limit, user_id=user_id)
        
        # ç¡®ä¿retrieved_memoriesä¸æ˜¯None
        retrieved_memories = retrieved_memories or []
        
        # æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²ç”¨äºåç»­å¤„ç†
        memories_with_facts = []
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼Œç”¨äºè®¡ç®—äº‹å®ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§
        query_vec = get_embedding(line.get("question", ""))
        
        for mem in retrieved_memories:
            # æ·»åŠ è®°å¿†å†…å®¹
            memory_line = f"- [{datetime.fromtimestamp(mem['created_at'], timezone.utc).isoformat()}] {mem['content']}"
            memories_with_facts.append(memory_line)

            # æ·»åŠ å…³è”çš„äº‹å®ï¼ˆå¦‚æœæœ‰ï¼‰
            related_facts = mem.get("related_facts", [])
            max_facts_per_memory = 3  # æ¯ä¸ªè®°å¿†çš„äº‹å®æ•°é‡é™åˆ¶
            if related_facts:
                # è®¡ç®—æ¯ä¸ªäº‹å®ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§åˆ†æ•°
                fact_with_scores = []
                for fact in related_facts:
                    try:
                        fact_vec = get_embedding(fact["text"])
                        # ä½¿ç”¨å‘é‡ç‚¹ç§¯ä½œä¸ºç›¸å…³æ€§åˆ†æ•°
                        dot_product = sum(a * b for a, b in zip(query_vec, fact_vec))
                        fact_with_scores.append((fact, dot_product))
                    except Exception as e:
                        print(f"è®¡ç®—äº‹å®ç›¸å…³æ€§å¤±è´¥: {e}")
                        fact_with_scores.append((fact, 0))
                
                # æ ¹æ®ç›¸å…³æ€§åˆ†æ•°å¯¹äº‹å®è¿›è¡Œæ’åº
                # fact_with_scores.sort(key=lambda x: x[1], reverse=True)
                
                # æ·»åŠ æ’åºåçš„äº‹å®ï¼Œé™åˆ¶æ•°é‡
                for i, (fact, score) in enumerate(fact_with_scores[:max_facts_per_memory]):
                    # ä¼˜åŒ–äº‹å®è¾“å‡ºæ ¼å¼
                    fact_text = fact['text']
                    details = fact['details']
                    
                    # æ ¼å¼åŒ–ç»†èŠ‚
                    if details:
                        # å°†ç»†èŠ‚åˆ—è¡¨è½¬æ¢ä¸ºæ›´æ˜“è¯»çš„æ ¼å¼
                        details_str = "; ".join(details)
                        # å¦‚æœç»†èŠ‚å¤ªé•¿ï¼Œæˆªæ–­
                        if len(details_str) > 100:
                            details_str = details_str[:97] + "..."
                        fact_line = f"  â”œâ”€â”€ [{i+1}] äº‹å®: {fact_text}\n  â”‚     ç»†èŠ‚: {details_str}"
                    else:
                        fact_line = f"  â”œâ”€â”€ [{i+1}] äº‹å®: {fact_text}"
                    
                    memories_with_facts.append(fact_line)
        
        memories_str = "\n".join(memories_with_facts)
        
        # è·å–æ ‡å‡†ç­”æ¡ˆå’Œé—®é¢˜ç±»å‹
        golden_answer = line.get("answer")
        question = line.get("question")
        question_type = line.get("question_type", "unknown")
        
        # è¯„ä¼°ç­”æ¡ˆæ­£ç¡®æ€§
        is_correct = lme_grader(llm_client, question, golden_answer, answer)
        
        return {
            "index": user_index,
            "is_correct": is_correct,
            "counts": memory_counts,
            "question": question,
            "question_type": question_type,
            "answer": answer,
            "golden_answer": golden_answer,
            "retrieved_memories": retrieved_memories,
            "context": memories_str,
        }
    except Exception as e:
        print(f"å¤„ç†ç”¨æˆ· {user_index} å‡ºé”™ ({line.get('question', 'Unknown')[:20]}...): {e}")
        return {
            "index": user_index,
            "is_correct": False,
            "counts": {"ADD": 0, "UPDATE": 0, "DELETE": 0, "INFER": 0, "NOOP": 0},
            "question": line.get("question", "N/A"),
            "question_type": line.get("question_type", "unknown"),
            "context": "N/A",
            "answer": "N/A",
            "golden_answer": line.get("answer", "N/A"),
            "retrieved_memories": []
        }


# ==========================================
# Main Test & Evaluation
# ==========================================
if __name__ == "__main__":
    import argparse
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="Memory Pipeline with longmemeval Evaluation")
    parser.add_argument("--eval", action="store_true", help="æ˜¯å¦è¿›è¡Œè¯„ä¼°")
    parser.add_argument("--infer", action="store_true", default=True, help="æ˜¯å¦ä½¿ç”¨æ¨ç†åŠŸèƒ½")
    parser.add_argument("--num_users", type=int, default=50, help="è¯„ä¼°ç”¨æˆ·æ•°é‡")
    parser.add_argument("--max_workers", type=int, default=10, help="å¹¶è¡Œå¤„ç†çš„å·¥ä½œçº¿ç¨‹æ•°")
    parser.add_argument("--retrieve_limit", type=int, default=3, help="æ£€ç´¢æ—¶è¿”å›çš„è®°å¿†æ•°é‡")
    parser.add_argument("--threshold", type=float, default=0.7, help="è®°å¿†ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºè¯¥é˜ˆå€¼çš„è®°å¿†å°†è¢«è¿‡æ»¤æ‰")
    parser.add_argument("--extract-mode", type=str, default="whole", choices=["whole", "turn"], help="æå–æ¨¡å¼ï¼šwhole-å¯¹æ•´ä¸ªchunkè¿›è¡Œæå–ï¼Œturn-æŒ‰è½®æ¬¡æå–")
    parser.add_argument("--vector-db-type", type=str, default="milvus", choices=["milvus", "qdrant"], help="æŒ‡å®šä½¿ç”¨çš„å‘é‡æ•°æ®åº“ç±»å‹")
    parser.add_argument("--clear-db", action="store_true", help="è¿è¡Œå‰æ¸…ç©ºæ•°æ®åº“")
    parser.add_argument("--data-path", type=str, help="æŒ‡å®šæ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--dataset-type", type=str, default="longmemeval", choices=["longmemeval", "hotpotqa"], help="æŒ‡å®šæ•°æ®é›†ç±»å‹")
    args = parser.parse_args()
    
    # åˆå§‹åŒ–å†…å­˜ç®¡é“
    pipeline = MemoryPipeline(vector_db_type=args.vector_db_type, clear_db=args.clear_db, dataset_name=args.dataset_type)
    
    if args.eval:
        # è¯„ä¼°æ¨¡å¼
        try:
            # æ ¹æ®æ•°æ®é›†ç±»å‹è®¾ç½®é»˜è®¤æ•°æ®è·¯å¾„
            if args.dataset_type == "hotpotqa":
                data_path = args.data_path or "./data/hotpotqa-val.jsonl"
            else:  # longmemeval
                data_path = args.data_path or "./data/longmemeval_s_cleaned.json"
                
            print(f"è°ƒè¯•ä¿¡æ¯ï¼š")
            print(f"  æ•°æ®é›†ç±»å‹ï¼š{args.dataset_type}")
            print(f"  æŒ‡å®šçš„æ•°æ®è·¯å¾„ï¼š{args.data_path}")
            print(f"  å®é™…ä½¿ç”¨çš„æ•°æ®è·¯å¾„ï¼š{data_path}")
            print(f"  æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼š{os.path.exists(data_path)}")
            
            if not os.path.exists(data_path):
                print(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
                exit()
            
            # åˆ¤æ–­æ–‡ä»¶ç±»å‹å¹¶åŠ è½½æ•°æ®
            lines = []
            print(f"  æ–‡ä»¶æ ¼å¼ï¼š{'JSONL' if data_path.endswith('.jsonl') else 'JSON'}")
            if data_path.endswith(".jsonl"):
                # å¤„ç†JSONLæ ¼å¼æ–‡ä»¶
                print(f"  å¼€å§‹åŠ è½½JSONLæ–‡ä»¶...")
                with open(data_path, "r") as f:
                    for i, line in enumerate(f):
                        lines.append(json.loads(line.strip()))
                        if i < 2:  # æ‰“å°å‰2æ¡æ•°æ®çš„å…³é”®å­—æ®µ
                            loaded_item = lines[-1]
                            print(f"    ç¬¬{i+1}æ¡æ•°æ®å…³é”®å­—æ®µï¼š")
                            print(f"      æ˜¯å¦åŒ…å«contextï¼š{'context' in loaded_item}")
                            print(f"      æ˜¯å¦åŒ…å«haystack_datesï¼š{'haystack_dates' in loaded_item}")
                            print(f"      æ•°æ®IDï¼š{loaded_item.get('id', 'æœªçŸ¥')}")
            else:
                # å¤„ç†JSONæ ¼å¼æ–‡ä»¶
                print(f"  å¼€å§‹åŠ è½½JSONæ–‡ä»¶...")
                with open(data_path, "r") as f:
                    data = json.load(f)
                    lines = data.get("items", []) if isinstance(data, dict) else data
                    
                    if len(lines) > 0:
                        # æ‰“å°å‰2æ¡æ•°æ®çš„å…³é”®å­—æ®µ
                        for i in range(min(2, len(lines))):
                            loaded_item = lines[i]
                            print(f"    ç¬¬{i+1}æ¡æ•°æ®å…³é”®å­—æ®µï¼š")
                            print(f"      æ˜¯å¦åŒ…å«contextï¼š{'context' in loaded_item}")
                            print(f"      æ˜¯å¦åŒ…å«haystack_datesï¼š{'haystack_dates' in loaded_item}")
                            print(f"      æ•°æ®IDï¼š{loaded_item.get('id', 'æœªçŸ¥')}")
            
            # é™åˆ¶å¤„ç†çš„ç”¨æˆ·æ•°é‡
            lines = lines[:args.num_users] if args.num_users > 0 else lines
            print(f"  åŠ è½½å®Œæˆï¼Œå…± {len(lines)} æ¡æ•°æ®ï¼Œå‡†å¤‡å¤„ç†...")
            
            # åˆå§‹åŒ–ç»Ÿè®¡æ•°æ®
            all_results = []
            total = len(lines)
            correct = 0
            
            # å¹¶è¡Œå¤„ç†ç”¨æˆ·æ•°æ®
            with ThreadPoolExecutor(max_workers=args.max_workers) as executor:
                # ä¸ºæ¯ä¸ªç”¨æˆ·æäº¤ä¸€ä¸ªä»»åŠ¡
                future_to_index = {
                    executor.submit(process_and_evaluate_user, line, i, args.infer, args.retrieve_limit, args.extract_mode, args.vector_db_type, args.dataset_type): i
                    for i, line in enumerate(lines)
                }
                
                # å¤„ç†å®Œæˆçš„ä»»åŠ¡
                for future in tqdm(as_completed(future_to_index), total=total, desc="å¤„ç†ç”¨æˆ·æ•°æ®"):
                    index = future_to_index[future]
                    try:
                        result = future.result()
                        all_results.append(result)
                        if result["is_correct"]:
                            correct += 1
                            print(f"\nâœ… ç”¨æˆ· {index} å›ç­”æ­£ç¡®")
                        else:
                            print(f"\nâŒ ç”¨æˆ· {index} å›ç­”é”™è¯¯")
                            print(f"   é—®é¢˜: {result['question']}")
                            print(f"   é¢„æµ‹: {result['answer']}")
                            print(f"   æ ‡å‡†ç­”æ¡ˆ: {result['golden_answer']}")
                    except Exception as e:
                        print(f"\nâš ï¸ ç”¨æˆ· {index} å¤„ç†å¤±è´¥: {e}")
            
            # è®¡ç®—å‡†ç¡®ç‡
            accuracy = correct / total if total > 0 else 0
            print(f"\nğŸ“Š è¯„ä¼°ç»“æœï¼š")
            print(f"   æ€»ç”¨æˆ·æ•°: {total}")
            print(f"   æ­£ç¡®æ•°: {correct}")
            print(f"   å‡†ç¡®ç‡: {accuracy:.4f}")
            
            # ä¿å­˜è¯„ä¼°ç»“æœ
            result_file = f"evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(result_file, "w") as f:
                json.dump(all_results, f, ensure_ascii=False, indent=2)
            print(f"   è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
            
        except Exception as e:
            print(f"è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    else:
        # æµ‹è¯•æ¨¡å¼ï¼Œç”¨äºå•æ¡æ•°æ®æµ‹è¯•
        print("ğŸš€ è¿›å…¥æµ‹è¯•æ¨¡å¼")
        
        # ç¤ºä¾‹æµ‹è¯•æ•°æ®
        test_data = {
            "context": "User bought a new laptop. It's a MacBook Pro with M3 chip.",
            "question": "What did the user buy?",
            "answer": "A new MacBook Pro with M3 chip."
        }
        
        # å¤„ç†æµ‹è¯•æ•°æ®
        print(f"\nğŸ“ æµ‹è¯•æ•°æ®ï¼š")
        print(f"   Context: {test_data['context']}")
        print(f"   Question: {test_data['question']}")
        print(f"   Answer: {test_data['answer']}")
        
        # æå–äº‹å®
        extract_result = pipeline.step_extract(test_data['context'], extract_mode=args.extract_mode)
        print(f"\nğŸ” æå–ç»“æœï¼š")
        print(f"   {json.dumps(extract_result, ensure_ascii=False, indent=2)}")
        
        # æ£€ç´¢ç›¸å…³è®°å¿†å’Œäº‹å®
        context_bundles = pipeline.step_retrieve(extract_result, limit=args.retrieve_limit)
        print(f"\nğŸ§  æ£€ç´¢ç»“æœï¼š")
        for i, bundle in enumerate(context_bundles):
            print(f"   äº‹å® {i+1}: {bundle['new_fact']['text']}")
            print(f"   ç›¸å…³è®°å¿†: {len(bundle['candidates'])}")
            print(f"   ç›¸å…³äº‹å®: {len(bundle['related_facts'])}")
        
        # ç”Ÿæˆå†³ç­–
        decisions = pipeline.step_decide(extract_result, context_bundles)
        print(f"\nğŸ“‹ å†³ç­–ç»“æœï¼š")
        for decision in decisions:
            print(f"   {decision}")
        
        # æ‰§è¡Œå†³ç­–
        pipeline.step_execute(decisions, extract_result)
        print(f"\nâœ… æ‰§è¡Œå®Œæˆ")
        
        # ç”Ÿæˆå“åº”
        retrieved_memories, answer = response_user(test_data, pipeline, args.retrieve_limit)
        print(f"\nğŸ’¬ ç”Ÿæˆå›ç­”ï¼š")
        print(f"   é—®é¢˜: {test_data['question']}")
        print(f"   å›ç­”: {answer}")
        print(f"   æ ‡å‡†ç­”æ¡ˆ: {test_data['answer']}")
        
    # æ‰“å°æ“ä½œç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š æ“ä½œç»Ÿè®¡ï¼š")
    print(f"   {pipeline.get_operation_counts()}")
